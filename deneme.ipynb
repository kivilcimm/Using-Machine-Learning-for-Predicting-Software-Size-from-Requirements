{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kivilcimm/Using-Machine-Learning-for-Predicting-Software-Size-from-Requirements/blob/main/deneme.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfuksYapxuz9",
        "outputId": "ce82ea27-3b37-4c49-a32b-9293cec26e71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import BertTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZfNv6FWmlTM",
        "outputId": "5325dfce-a8c0-4a3d-e000-c6ea17835fa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    data_path = \"/content/drive/My Drive/Dataset/\"\n",
        "\n",
        "except:\n",
        "    print(\"You are not working in Colab at the moment :(\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "puMmOp2ws3MK",
        "outputId": "d3fd2c10-3dd3-4b4f-9373-ae79c09c7cb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sentences = 2602\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   projectNo  usecaseNo                                           sentence  \\\n",
              "0          1        0.0  User fills in the required input fields provid...   \n",
              "1          1        NaN            He/She clicks on the ‘Register’ button.   \n",
              "2          1        NaN  System will ask the user to provide his/her ph...   \n",
              "3          1        NaN  Then the user will be directed to a page in wh...   \n",
              "4          1        NaN  System will give 2 minutes for the user to ent...   \n",
              "\n",
              "   totalCFP  \n",
              "0         1  \n",
              "1         0  \n",
              "2         3  \n",
              "3         0  \n",
              "4         1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0677e627-e8b4-46f3-8026-53d48db4e01a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>projectNo</th>\n",
              "      <th>usecaseNo</th>\n",
              "      <th>sentence</th>\n",
              "      <th>totalCFP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>User fills in the required input fields provid...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>He/She clicks on the ‘Register’ button.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>System will ask the user to provide his/her ph...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Then the user will be directed to a page in wh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>System will give 2 minutes for the user to ent...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0677e627-e8b4-46f3-8026-53d48db4e01a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0677e627-e8b4-46f3-8026-53d48db4e01a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0677e627-e8b4-46f3-8026-53d48db4e01a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "#path = data_path + \"cosmic_update.csv\"\n",
        "path = data_path + \"F_cosmic.csv\"\n",
        "trng_dataset = pd.read_csv(path , delimiter=',')\n",
        "print('Number of training sentences =', trng_dataset.shape[0])\n",
        "trng_dataset.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4Q55gff7aDB",
        "outputId": "550f8a50-e31d-4c87-bc53-917c33754a30"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2602, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ],
      "source": [
        "trng_dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTVqRjHMd-u9"
      },
      "outputs": [],
      "source": [
        "trng_sentences = trng_dataset.sentence.values\n",
        "trng_labels = trng_dataset.totalCFP.values\n",
        "trng_labels = trng_labels.astype('float32')\n",
        "\n",
        "\n",
        "trng_project_no = trng_dataset.projectNo.values#****\n",
        "trng_usecase_no = trng_dataset.usecaseNo.values#*****\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcpfqj3JeydN",
        "outputId": "5d69efdc-2707-40bb-92b8-1da012382f01"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('float32')"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ],
      "source": [
        "trng_labels.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o38SpDLweyk-",
        "outputId": "8a9a99e2-c9ca-4b1c-a883-fb2f6a6a6eb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "metadata": {
        "id": "X8-Cl7z0RcE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShWoQqPoje8C",
        "outputId": "f18f8fc9-e864-4ee9-fb84-aff0a039f11b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sentence length:  106\n"
          ]
        }
      ],
      "source": [
        "max_len = 0\n",
        "\n",
        "for sent in trng_sentences:\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "    #print(\"input_ids: \",input_ids)\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zArgjVGXqeqA"
      },
      "outputs": [],
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for sent in trng_sentences:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      \n",
        "                        add_special_tokens = True, \n",
        "                        max_length = 100,           \n",
        "                        truncation = True,\n",
        "                        padding = 'max_length',\n",
        "                        return_attention_mask = True,   \n",
        "                        return_tensors = 'pt',    \n",
        "                   )\n",
        " \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "  #  print(\"input_ids: \",input_ids)\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        " #   print(\"attention_masks: \",attention_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsiJkSi8qrtH",
        "outputId": "afc67b96-da6e-4986-ead6-bf3fec887b7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  User fills in the required input fields provided by the system with their information.\n",
            "Token IDs: tensor([  101,  5310, 17469,  1999,  1996,  3223,  7953,  4249,  3024,  2011,\n",
            "         1996,  2291,  2007,  2037,  2592,  1012,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
          ]
        }
      ],
      "source": [
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(trng_labels, dtype=torch.float) \n",
        "print('Original: ', trng_sentences[0])\n",
        "print('Token IDs:', input_ids[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeFXPjDYq3d9",
        "outputId": "2d939b0f-3e19-4732-8817-914bfcf59e81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  101,  5310, 17469,  1999,  1996,  3223,  7953,  4249,  3024,  2011,\n",
            "         1996,  2291,  2007,  2037,  2592,  1012,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "<torch.utils.data.dataset.Subset object at 0x7f22769342d0>\n",
            "2,081 training samples\n",
            "  261 validation samples\n",
            "  260 test samples\n"
          ]
        }
      ],
      "source": [
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "print(dataset[0][0])\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = int(0.1 * len(dataset))\n",
        "val_size = len(dataset) - train_size - test_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "print(train_dataset)\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))\n",
        "print('{:>5,} test samples'.format(test_size))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2CnBgebrDy6"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset, \n",
        "            shuffle = True,\n",
        "            batch_size = batch_size \n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset,\n",
        "            sampler = SequentialSampler(val_dataset), \n",
        "            batch_size = batch_size \n",
        "        )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "            test_dataset,\n",
        "          # sampler = SequentialSampler(test_dataset), \n",
        "            batch_size = batch_size \n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX6PuqldrD4j",
        "outputId": "90393fab-f514-4307-f763-8853687cc0b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",      \n",
        "    num_labels = 1,                 \n",
        "    output_attentions = False,      \n",
        "    output_hidden_states = False,   \n",
        ")\n",
        "\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4o2y6CZErdhv",
        "outputId": "320a1f44-13a1-408e-a253-a9c6f31736b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, \n",
        "                  eps = 1e-8 \n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnOyaY4VriSM"
      },
      "outputs": [],
      "source": [
        "epochs = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTJKWbqerzxf"
      },
      "outputs": [],
      "source": [
        "def get_mse(preds, labels):\n",
        "    pred_flat = preds.flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum((pred_flat - labels_flat)**2)/len(labels_flat)\n",
        "\n",
        "def get_mae(preds, labels):\n",
        "    pred_flat = preds.flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    sum = 0\n",
        "    n = len(labels_flat)\n",
        "    \n",
        "    for i in range(n):\n",
        "      \n",
        "        temp = abs(pred_flat[i] - labels_flat[i])\n",
        "        sum += temp\n",
        "\n",
        "    error = sum/n \n",
        "    print(\"Mean absolute error : \" + str(error))\n",
        "\n",
        "    return error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3g59xk2r38I",
        "outputId": "18586051-7991-4bce-d374-f9503e20774e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mGörüntülenen çıkış son 5000 satıra kısaltıldı.\u001b[0m\n",
            "        [ 0.9940]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.0260, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[0.1614]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epoch took:  30.39428\n",
            "\n",
            "Running Validation...\n",
            "b_input_ids:  tensor([[  101,  2330,  2689,  ...,     0,     0,     0],\n",
            "        [  101, 12210,  4471,  ...,     0,     0,     0],\n",
            "        [  101, 11562,  5587,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  4653,  2035,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  2291,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  2291,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [ 1.8934806e-01  8.2811630e-01  1.9996945e+00  1.8794727e+00\n",
            "  1.8416648e+00  1.9183822e+00  6.8762340e-02  2.7185196e-01\n",
            "  2.2449172e+00  9.9141218e-02  1.2627685e-01  5.0628859e-01\n",
            "  1.9996945e+00  2.0524933e+00  2.0278654e+00  1.7706442e+00\n",
            "  2.2881653e+00  1.2762250e-01 -2.3333732e-02  1.8875661e+00\n",
            "  8.3753574e-01  2.0300207e+00 -1.6951036e-02  1.4296122e-04\n",
            "  9.3486942e-02  4.6279427e-01  6.7059994e-01 -6.2845466e-03\n",
            "  8.3242196e-01  1.9079735e+00  1.3816581e+00  1.6122580e+00]\n",
            "label_ids [0. 1. 2. 2. 2. 2. 0. 4. 2. 0. 0. 0. 1. 2. 2. 1. 2. 0. 0. 2. 1. 2. 0. 0.\n",
            " 0. 1. 1. 0. 1. 2. 1. 1.]\n",
            "Mean absolute error : 0.3227275429508154\n",
            "b_input_ids:  tensor([[  101,  4607,  7514,  ...,     0,     0,     0],\n",
            "        [  101, 11562,  7929,  ...,     0,     0,     0],\n",
            "        [  101,  3116,  2038,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2291,  4618,  ...,     0,     0,     0],\n",
            "        [  101,  5310,  2064,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  8619,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [-1.8871655e-03  4.6279427e-01  1.8596880e+00  1.4145751e-01\n",
            "  2.0221133e+00  1.2284979e+00  2.1236615e+00 -5.0333541e-02\n",
            "  8.9340496e-01 -4.8832908e-02 -5.4685255e-03 -4.1335661e-02\n",
            " -1.8947560e-02  1.4418164e-01  1.9132384e+00  2.0524933e+00\n",
            "  8.2811630e-01  1.6053655e+00  1.4703606e+00  1.3686209e+00\n",
            "  6.5814245e-01  1.8757961e+00  1.8485471e+00  8.2811630e-01\n",
            "  2.0038452e+00  2.4249911e+00  2.3323245e+00  4.9988154e-02\n",
            "  2.1794491e-01  1.8578886e+00 -2.3549896e-02  2.2121501e-01]\n",
            "label_ids [0. 1. 2. 0. 2. 0. 3. 0. 0. 0. 0. 0. 0. 0. 2. 2. 1. 2. 2. 2. 1. 2. 2. 1.\n",
            " 2. 6. 2. 0. 1. 2. 0. 0.]\n",
            "Mean absolute error : 0.3730188944900874\n",
            "b_input_ids:  tensor([[  101, 11562,  3972,  ...,     0,     0,     0],\n",
            "        [  101, 12210,  4471,  ...,     0,     0,     0],\n",
            "        [  101,  3967,  2052,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101, 11562,  7929,  ...,     0,     0,     0],\n",
            "        [  101,  2492,  3942,  ...,     0,     0,     0],\n",
            "        [  101,  2291,  4618,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [ 0.71449333  0.8281163   0.37775433  2.0906749   1.4640133   1.3525486\n",
            "  0.03065572  0.73253703  0.8281163   0.02256046  2.0760052  -0.0235499\n",
            "  0.24054553 -0.00239261 -0.02061956  0.05029016  1.9117438   1.9224294\n",
            "  2.1266737   0.0240723   1.7730932   0.2032287   0.7005626   0.4764204\n",
            "  0.10128404  2.0530295   1.8108605   0.00580809  1.4069458   0.46279427\n",
            "  2.0704212   1.8613105 ]\n",
            "label_ids [1. 1. 0. 2. 0. 0. 0. 1. 0. 0. 2. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 2. 0. 0.\n",
            " 0. 2. 2. 0. 2. 1. 2. 2.]\n",
            "Mean absolute error : 0.37676975002977997\n",
            "b_input_ids:  tensor([[  101,  8013,  4978,  ...,     0,     0,     0],\n",
            "        [  101,  8619, 11562,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  2291,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101, 11562,  3828,  ...,     0,     0,     0],\n",
            "        [  101,  2291, 11204,  ...,     0,     0,     0],\n",
            "        [  101,  8619,  2064,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [ 0.1429201   1.83078     2.0073729   0.30138394  0.2881123   1.9311361\n",
            "  2.0524933   1.5934844   1.9996945   1.9322257   1.1852303   0.8281163\n",
            "  0.2737114   0.38290182  0.48968607  0.48086002  1.6242664   0.07551159\n",
            "  2.2599313   0.17161839  2.1516426   1.8937962   0.07386231 -0.02850499\n",
            " -0.03250802  2.130549    1.8387434  -0.03831431  0.50557506  2.0524933\n",
            "  1.316559    0.00981756]\n",
            "label_ids [1. 2. 3. 0. 0. 2. 2. 2. 2. 2. 2. 1. 0. 0. 1. 1. 2. 2. 2. 0. 2. 2. 0. 0.\n",
            " 0. 2. 1. 0. 0. 2. 1. 0.]\n",
            "Mean absolute error : 0.3404262977128383\n",
            "b_input_ids:  tensor([[  101,  2291, 19818,  ...,     0,     0,     0],\n",
            "        [  101,  2048, 12020,  ...,     0,     0,     0],\n",
            "        [  101, 19413,  2497,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  5310,  2064,  ...,     0,     0,     0],\n",
            "        [  101,  2291,  4604,  ...,     0,     0,     0],\n",
            "        [  101,  5310, 29225,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [ 2.1957889   0.4666899   0.13650443  2.522814    0.01153121 -0.02386161\n",
            "  0.7275763   0.2213857   0.4867518   1.9853705   2.032171    0.879529\n",
            "  2.0524933   1.9886527   2.2807913   0.46470195 -0.02333373  0.73729205\n",
            "  1.9925582   0.51157033 -0.0450117   0.22274362  2.2469397  -0.04612783\n",
            "  1.9789387   1.891155    1.8910766   0.34490013  2.0524933  -0.01814849\n",
            "  2.255903    0.63344073]\n",
            "label_ids [2. 2. 2. 3. 0. 0. 2. 0. 1. 2. 2. 0. 2. 2. 3. 0. 0. 1. 2. 0. 0. 0. 2. 2.\n",
            " 2. 2. 2. 0. 2. 0. 4. 2.]\n",
            "Mean absolute error : 0.4817880066984799\n",
            "b_input_ids:  tensor([[  101,  5310,  2064,  ...,     0,     0,     0],\n",
            "        [  101,  5310, 29225,  ...,     0,     0,     0],\n",
            "        [  101,  2291, 19818,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101, 11562,  5587,  ...,     0,     0,     0],\n",
            "        [  101,  4607,  8094,  ...,     0,     0,     0],\n",
            "        [  101,  5310,  2064,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [ 0.03133971  0.36951905  2.2634068  -0.03831431  0.6572025  -0.00815968\n",
            "  0.9679472   1.6448615   1.1530547   0.590544    2.193913   -0.03256734\n",
            " -0.02376616  1.0732259   0.5055929   1.9257779   1.8933736   0.38471475\n",
            "  1.9126737   0.11129333  0.00931835  2.0257945   1.8529121   0.46279427\n",
            "  0.0311975   1.0632317   2.099897   -0.00337597  2.0524933   1.9996945\n",
            " -0.02669564 -0.04400591]\n",
            "label_ids [0. 0. 2. 0. 1. 0. 1. 2. 1. 1. 2. 0. 0. 0. 0. 2. 2. 0. 2. 0. 0. 2. 2. 1.\n",
            " 0. 0. 2. 0. 3. 2. 1. 0.]\n",
            "Mean absolute error : 0.26662842080986593\n",
            "b_input_ids:  tensor([[  101,  4653,  2035,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  2291,  ...,     0,     0,     0],\n",
            "        [  101,  5310, 29225,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  8013,  8039,  ...,     0,     0,     0],\n",
            "        [  101,  2291, 19818,  ...,     0,     0,     0],\n",
            "        [  101,  2291, 15210,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [1.9965459  1.8197541  0.6047459  0.4682455  0.27185196 0.04399583\n",
            " 1.374157   1.4023275  0.17808679 0.46279427 0.03627959 1.8688686\n",
            " 0.54887855 1.9008024  1.5548584  0.6853868  2.114749   0.03488931\n",
            " 1.9766588  0.6657115  0.00732259 1.8202907  0.46279427 0.04271251\n",
            " 0.5278201  1.1716323  0.27185196 1.8836812  2.2683797  0.00578663\n",
            " 2.2722316  2.099897  ]\n",
            "label_ids [2. 2. 1. 0. 0. 0. 0. 1. 0. 1. 1. 2. 0. 2. 0. 1. 2. 0. 2. 0. 0. 2. 1. 0.\n",
            " 0. 0. 0. 2. 2. 0. 2. 2.]\n",
            "Mean absolute error : 0.3689619711076375\n",
            "b_input_ids:  tensor([[  101,  5310, 29225,  ...,     0,     0,     0],\n",
            "        [  101,  8619,  6039,  ...,     0,     0,     0],\n",
            "        [  101, 11562,  3828,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2291, 15210,  ...,     0,     0,     0],\n",
            "        [  101,  5310, 12453,  ...,     0,     0,     0],\n",
            "        [  101,  5402,  2038,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [ 9.23493028e-01  6.61569014e-02  2.05249333e+00  2.05249333e+00\n",
            "  1.76926062e-01 -6.75008865e-03  2.59486772e-02  1.24792314e+00\n",
            " -1.60444202e-03  2.02486205e+00  6.57220423e-01 -3.62746529e-02\n",
            "  1.92301381e+00  1.65851867e+00  5.76593637e-01  1.17237282e+00\n",
            "  7.84819350e-02  2.42951773e-02  1.17903805e+00  1.16275795e-01\n",
            "  3.87207597e-01  9.95634258e-01  2.62147450e+00  5.45047045e-01\n",
            "  2.43587184e+00  6.34856939e-01  6.12491310e-01  1.08000651e-01\n",
            "  8.17889750e-01  2.09989691e+00  8.13954413e-01  1.99236429e+00]\n",
            "label_ids [0. 0. 2. 2. 0. 0. 0. 3. 0. 2. 0. 0. 3. 3. 0. 0. 0. 0. 1. 1. 0. 1. 3. 1.\n",
            " 3. 0. 0. 0. 0. 2. 0. 2.]\n",
            "Mean absolute error : 0.4372868388018105\n",
            "b_input_ids:  tensor([[  101,  8619,  6039,  1996,  4746,  2224,  2005,  3784,  2492,  2005,\n",
            "          1996,  9459,  3639,  1012,   102,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  1996,  5310, 15867,  1996,  1000, 12210,  1000,  5724,  1012,\n",
            "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101, 11562, 10086,  6462,  2005,  3563, 11412,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101, 11562,  7929,  6462,   102,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  2291,  4618, 18584,  3076,  2592,  2013,  7809,   102,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
            "       device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]], device='cuda:0')\n",
            "logits:  [0.06448828 0.10348902 0.6188802  0.46279398 1.8910766 ]\n",
            "label_ids [0. 1. 0. 1. 2.]\n",
            "Mean absolute error : 0.4452017888426781\n",
            "  Accuracy: 0.38\n",
            "  Validation Loss: 0.00\n",
            "  Validation took:  1.393612\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.4550, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.3626],\n",
            "        [ 1.4188],\n",
            "        [ 0.3626],\n",
            "        [ 0.9210],\n",
            "        [ 1.0888],\n",
            "        [ 1.8568],\n",
            "        [ 1.6224],\n",
            "        [ 0.2096],\n",
            "        [-0.0360],\n",
            "        [ 1.7658],\n",
            "        [ 0.0569],\n",
            "        [ 0.0862],\n",
            "        [ 1.8512],\n",
            "        [ 0.2983],\n",
            "        [ 0.3641],\n",
            "        [ 0.3362],\n",
            "        [-0.2861],\n",
            "        [ 1.9769],\n",
            "        [ 0.3962],\n",
            "        [ 0.9103],\n",
            "        [ 1.0517],\n",
            "        [ 1.1764],\n",
            "        [-0.0632],\n",
            "        [ 0.5448],\n",
            "        [ 1.7840],\n",
            "        [ 2.2389],\n",
            "        [ 2.1726],\n",
            "        [ 1.4356],\n",
            "        [ 2.0396],\n",
            "        [ 0.1717],\n",
            "        [ 0.7349],\n",
            "        [ 0.1596]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2272, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.7107],\n",
            "        [ 1.5602],\n",
            "        [ 2.4762],\n",
            "        [ 1.7582],\n",
            "        [ 0.9415],\n",
            "        [ 0.2993],\n",
            "        [ 0.1164],\n",
            "        [ 1.0464],\n",
            "        [-0.0977],\n",
            "        [ 1.8626],\n",
            "        [ 2.0021],\n",
            "        [ 1.4052],\n",
            "        [ 1.8754],\n",
            "        [ 0.1596],\n",
            "        [ 1.9124],\n",
            "        [-0.0425],\n",
            "        [ 1.8396],\n",
            "        [-0.1471],\n",
            "        [ 0.7255],\n",
            "        [-0.0600],\n",
            "        [ 2.0378],\n",
            "        [ 0.7646],\n",
            "        [ 0.6214],\n",
            "        [ 0.2224],\n",
            "        [ 2.1612],\n",
            "        [ 2.2176],\n",
            "        [ 2.1703],\n",
            "        [ 0.9812],\n",
            "        [ 0.0723],\n",
            "        [ 0.1892],\n",
            "        [ 2.0633],\n",
            "        [ 1.6726]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2005, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.8744],\n",
            "        [ 2.2018],\n",
            "        [ 0.1325],\n",
            "        [ 0.5333],\n",
            "        [ 0.4370],\n",
            "        [ 0.8481],\n",
            "        [ 0.2171],\n",
            "        [ 1.8314],\n",
            "        [ 1.0416],\n",
            "        [ 1.9447],\n",
            "        [-0.0859],\n",
            "        [ 1.0249],\n",
            "        [-0.0577],\n",
            "        [-0.0483],\n",
            "        [ 0.3972],\n",
            "        [-0.0697],\n",
            "        [ 0.0803],\n",
            "        [-0.1776],\n",
            "        [ 1.9885],\n",
            "        [ 1.3483],\n",
            "        [ 0.2901],\n",
            "        [ 0.6222],\n",
            "        [ 0.3394],\n",
            "        [ 1.1940],\n",
            "        [ 1.6905],\n",
            "        [ 0.0999],\n",
            "        [ 0.2674],\n",
            "        [ 0.2188],\n",
            "        [ 0.6570],\n",
            "        [ 1.2606],\n",
            "        [-0.1310],\n",
            "        [ 2.3070]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3673, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.2996e+00],\n",
            "        [ 2.2032e+00],\n",
            "        [ 2.2466e-01],\n",
            "        [ 2.0547e+00],\n",
            "        [ 2.1582e-01],\n",
            "        [ 7.7316e-02],\n",
            "        [ 2.6087e-01],\n",
            "        [ 1.8992e+00],\n",
            "        [ 1.9625e+00],\n",
            "        [ 2.0224e+00],\n",
            "        [ 1.8784e+00],\n",
            "        [ 3.1486e-01],\n",
            "        [ 5.2427e-01],\n",
            "        [ 1.7895e+00],\n",
            "        [ 3.2311e-01],\n",
            "        [ 3.9240e-04],\n",
            "        [ 1.7544e+00],\n",
            "        [ 7.3020e-02],\n",
            "        [ 3.6092e-01],\n",
            "        [ 8.6678e-01],\n",
            "        [ 2.0276e+00],\n",
            "        [ 1.8387e+00],\n",
            "        [ 1.5193e+00],\n",
            "        [ 9.7565e-01],\n",
            "        [ 1.8545e+00],\n",
            "        [ 9.7680e-01],\n",
            "        [-1.5672e-01],\n",
            "        [ 1.2301e-01],\n",
            "        [ 3.3020e-02],\n",
            "        [ 5.6264e-01],\n",
            "        [ 2.2141e-01],\n",
            "        [ 2.4149e+00]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2217, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.0275],\n",
            "        [ 0.2320],\n",
            "        [ 0.1665],\n",
            "        [-0.3712],\n",
            "        [ 1.2176],\n",
            "        [ 0.4247],\n",
            "        [-0.0283],\n",
            "        [ 1.0800],\n",
            "        [ 0.3692],\n",
            "        [ 1.8856],\n",
            "        [ 1.7600],\n",
            "        [-0.1425],\n",
            "        [ 2.0491],\n",
            "        [ 2.0002],\n",
            "        [ 0.0551],\n",
            "        [ 0.6980],\n",
            "        [-0.1611],\n",
            "        [ 0.3116],\n",
            "        [ 0.7263],\n",
            "        [ 0.6285],\n",
            "        [ 2.0201],\n",
            "        [ 0.7168],\n",
            "        [ 1.9254],\n",
            "        [ 1.4326],\n",
            "        [ 0.0291],\n",
            "        [ 2.0330],\n",
            "        [-0.0506],\n",
            "        [ 1.7959],\n",
            "        [ 2.1334],\n",
            "        [ 2.4316],\n",
            "        [ 0.3859],\n",
            "        [-0.0181]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1514, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.9411],\n",
            "        [-0.2834],\n",
            "        [-0.1337],\n",
            "        [ 0.2701],\n",
            "        [ 0.7900],\n",
            "        [ 0.0591],\n",
            "        [ 0.8607],\n",
            "        [ 1.1920],\n",
            "        [ 1.6507],\n",
            "        [ 1.8722],\n",
            "        [ 0.7161],\n",
            "        [-0.0188],\n",
            "        [ 1.9276],\n",
            "        [ 0.2814],\n",
            "        [ 1.3680],\n",
            "        [ 1.7631],\n",
            "        [ 1.9000],\n",
            "        [-0.2994],\n",
            "        [ 1.7193],\n",
            "        [ 2.2277],\n",
            "        [ 2.2338],\n",
            "        [ 0.1954],\n",
            "        [ 1.2899],\n",
            "        [ 1.9640],\n",
            "        [ 0.5446],\n",
            "        [ 0.6218],\n",
            "        [ 2.0958],\n",
            "        [-0.0098],\n",
            "        [-0.1295],\n",
            "        [ 2.2980],\n",
            "        [-0.0224],\n",
            "        [ 2.5785]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.0420, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 2.2022],\n",
            "        [ 1.8876],\n",
            "        [ 0.1499],\n",
            "        [ 1.9311],\n",
            "        [ 2.0531],\n",
            "        [ 1.7549],\n",
            "        [ 0.1722],\n",
            "        [ 2.2796],\n",
            "        [ 0.0117],\n",
            "        [ 2.1057],\n",
            "        [ 0.4785],\n",
            "        [ 1.1030],\n",
            "        [ 1.9971],\n",
            "        [ 2.1088],\n",
            "        [ 0.0917],\n",
            "        [ 0.8667],\n",
            "        [-0.0114],\n",
            "        [ 2.4823],\n",
            "        [ 1.9617],\n",
            "        [ 0.9698],\n",
            "        [ 0.1317],\n",
            "        [ 1.9245],\n",
            "        [ 2.1478],\n",
            "        [ 0.3646],\n",
            "        [ 2.0486],\n",
            "        [ 0.7071],\n",
            "        [ 1.5234],\n",
            "        [ 1.9695],\n",
            "        [-0.0705],\n",
            "        [-0.0884],\n",
            "        [-0.1017],\n",
            "        [ 2.0710]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3588, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 2.1456],\n",
            "        [ 2.1965],\n",
            "        [ 1.2323],\n",
            "        [ 1.6964],\n",
            "        [ 0.4597],\n",
            "        [ 0.7640],\n",
            "        [ 0.0814],\n",
            "        [ 0.4610],\n",
            "        [ 1.5361],\n",
            "        [ 2.0164],\n",
            "        [ 0.8920],\n",
            "        [-0.0364],\n",
            "        [ 0.0733],\n",
            "        [ 0.2364],\n",
            "        [ 2.7969],\n",
            "        [ 2.6663],\n",
            "        [ 0.1191],\n",
            "        [ 1.0762],\n",
            "        [ 0.4193],\n",
            "        [ 2.0010],\n",
            "        [ 0.9436],\n",
            "        [-0.0585],\n",
            "        [ 2.2632],\n",
            "        [ 1.6664],\n",
            "        [ 1.7064],\n",
            "        [ 0.8116],\n",
            "        [ 2.3627],\n",
            "        [ 1.1671],\n",
            "        [-0.1315],\n",
            "        [ 1.0040],\n",
            "        [ 2.2997],\n",
            "        [ 0.2774]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3406, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.0032],\n",
            "        [ 1.4100],\n",
            "        [ 2.1020],\n",
            "        [ 1.0816],\n",
            "        [-0.0068],\n",
            "        [ 0.8447],\n",
            "        [ 0.6852],\n",
            "        [ 0.7268],\n",
            "        [ 2.2196],\n",
            "        [ 2.1521],\n",
            "        [ 2.2912],\n",
            "        [ 1.8394],\n",
            "        [ 0.5596],\n",
            "        [ 2.3751],\n",
            "        [ 0.7956],\n",
            "        [ 0.0672],\n",
            "        [ 2.1178],\n",
            "        [ 0.3451],\n",
            "        [-0.0604],\n",
            "        [ 0.0596],\n",
            "        [ 0.0514],\n",
            "        [-0.2243],\n",
            "        [ 1.8983],\n",
            "        [ 2.0080],\n",
            "        [-0.0672],\n",
            "        [ 2.1701],\n",
            "        [ 1.9915],\n",
            "        [ 0.4037],\n",
            "        [ 0.2299],\n",
            "        [ 1.9244],\n",
            "        [ 1.9931],\n",
            "        [ 2.1427]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1278, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 2.0327],\n",
            "        [ 1.9335],\n",
            "        [ 0.0598],\n",
            "        [ 0.0127],\n",
            "        [ 1.6257],\n",
            "        [ 2.4935],\n",
            "        [ 1.9037],\n",
            "        [ 2.2095],\n",
            "        [ 2.3240],\n",
            "        [ 0.2458],\n",
            "        [ 0.2631],\n",
            "        [ 1.7908],\n",
            "        [ 0.0096],\n",
            "        [ 2.4563],\n",
            "        [ 1.7808],\n",
            "        [ 0.1525],\n",
            "        [ 1.6705],\n",
            "        [ 2.2044],\n",
            "        [ 2.1208],\n",
            "        [ 2.4624],\n",
            "        [ 0.7766],\n",
            "        [ 2.2168],\n",
            "        [ 2.5295],\n",
            "        [ 2.0694],\n",
            "        [-0.0121],\n",
            "        [ 2.3774],\n",
            "        [ 1.8296],\n",
            "        [ 1.7158],\n",
            "        [ 1.8976],\n",
            "        [ 0.3005],\n",
            "        [ 0.0298],\n",
            "        [ 2.0301]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1540, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.4158],\n",
            "        [-0.1022],\n",
            "        [ 1.4768],\n",
            "        [ 0.6193],\n",
            "        [ 0.3991],\n",
            "        [ 2.4234],\n",
            "        [ 1.7501],\n",
            "        [-0.1549],\n",
            "        [ 0.5572],\n",
            "        [ 2.0162],\n",
            "        [ 0.1875],\n",
            "        [ 1.8904],\n",
            "        [ 1.9376],\n",
            "        [ 1.8930],\n",
            "        [ 1.7981],\n",
            "        [ 0.7529],\n",
            "        [ 2.1534],\n",
            "        [-0.1530],\n",
            "        [ 0.2036],\n",
            "        [ 2.0543],\n",
            "        [ 2.3435],\n",
            "        [ 0.4910],\n",
            "        [ 0.3202],\n",
            "        [-0.0480],\n",
            "        [ 2.2030],\n",
            "        [ 1.7170],\n",
            "        [ 1.7418],\n",
            "        [ 0.0298],\n",
            "        [ 0.1812],\n",
            "        [-0.3874],\n",
            "        [ 1.6582],\n",
            "        [-0.3220]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.4137, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 2.0755],\n",
            "        [ 0.1693],\n",
            "        [ 0.5877],\n",
            "        [ 2.2700],\n",
            "        [ 1.9996],\n",
            "        [ 1.0253],\n",
            "        [ 2.0536],\n",
            "        [ 2.0508],\n",
            "        [ 0.1140],\n",
            "        [ 0.5457],\n",
            "        [ 0.5766],\n",
            "        [-0.1006],\n",
            "        [ 2.2270],\n",
            "        [ 0.8511],\n",
            "        [ 0.0504],\n",
            "        [ 1.2274],\n",
            "        [ 0.7594],\n",
            "        [ 0.0404],\n",
            "        [-0.0477],\n",
            "        [ 2.0203],\n",
            "        [-0.0322],\n",
            "        [ 2.6835],\n",
            "        [-0.2417],\n",
            "        [ 1.2676],\n",
            "        [ 1.4513],\n",
            "        [ 1.9962],\n",
            "        [ 0.9557],\n",
            "        [ 2.2377],\n",
            "        [ 2.3545],\n",
            "        [ 2.3863],\n",
            "        [ 0.8080],\n",
            "        [ 2.2599]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2135, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.8442],\n",
            "        [ 0.9491],\n",
            "        [ 1.8690],\n",
            "        [ 2.0561],\n",
            "        [ 1.2058],\n",
            "        [ 1.0580],\n",
            "        [ 2.1397],\n",
            "        [ 1.9341],\n",
            "        [ 2.0657],\n",
            "        [-0.0882],\n",
            "        [ 1.8966],\n",
            "        [-0.2420],\n",
            "        [-0.3135],\n",
            "        [ 0.1663],\n",
            "        [ 1.2870],\n",
            "        [ 1.9776],\n",
            "        [ 1.4411],\n",
            "        [ 2.5599],\n",
            "        [-0.2717],\n",
            "        [ 1.0440],\n",
            "        [ 0.2339],\n",
            "        [ 2.4912],\n",
            "        [-0.0284],\n",
            "        [ 2.0171],\n",
            "        [-0.0960],\n",
            "        [ 2.6911],\n",
            "        [-0.3743],\n",
            "        [ 0.7208],\n",
            "        [-0.2974],\n",
            "        [ 0.2982],\n",
            "        [ 2.0362],\n",
            "        [ 2.4250]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2224, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.9962],\n",
            "        [ 1.1771],\n",
            "        [-0.3611],\n",
            "        [ 1.3643],\n",
            "        [-0.1674],\n",
            "        [ 2.0034],\n",
            "        [-0.0681],\n",
            "        [ 1.4109],\n",
            "        [ 2.0928],\n",
            "        [ 0.6565],\n",
            "        [-0.1877],\n",
            "        [ 2.2393],\n",
            "        [-0.2282],\n",
            "        [ 2.2132],\n",
            "        [-0.1617],\n",
            "        [-0.2401],\n",
            "        [ 2.0821],\n",
            "        [ 1.3292],\n",
            "        [-0.0237],\n",
            "        [ 2.4642],\n",
            "        [ 2.0629],\n",
            "        [ 1.9512],\n",
            "        [ 0.8922],\n",
            "        [ 2.2321],\n",
            "        [ 0.2723],\n",
            "        [ 2.2110],\n",
            "        [ 0.0251],\n",
            "        [ 1.3564],\n",
            "        [ 2.0712],\n",
            "        [ 2.6987],\n",
            "        [-0.4141],\n",
            "        [-0.2752]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3321, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[-0.1938],\n",
            "        [-0.2526],\n",
            "        [ 2.2766],\n",
            "        [ 0.7574],\n",
            "        [-0.0331],\n",
            "        [ 2.5742],\n",
            "        [ 1.5159],\n",
            "        [-0.1563],\n",
            "        [ 0.9542],\n",
            "        [ 1.8190],\n",
            "        [ 0.3538],\n",
            "        [ 1.9687],\n",
            "        [ 2.1843],\n",
            "        [ 0.4255],\n",
            "        [-0.5924],\n",
            "        [ 2.0560],\n",
            "        [ 1.8573],\n",
            "        [ 1.7451],\n",
            "        [ 1.1936],\n",
            "        [ 1.8668],\n",
            "        [-0.3358],\n",
            "        [-0.1576],\n",
            "        [ 0.2333],\n",
            "        [-0.0747],\n",
            "        [ 0.4462],\n",
            "        [ 2.2146],\n",
            "        [ 2.2693],\n",
            "        [-0.0257],\n",
            "        [ 2.1613],\n",
            "        [-0.2299],\n",
            "        [ 2.6750],\n",
            "        [-0.3371]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.5223, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 6.4992e-01],\n",
            "        [ 1.9148e+00],\n",
            "        [-2.2901e-01],\n",
            "        [ 5.1293e-01],\n",
            "        [ 1.7434e+00],\n",
            "        [ 2.5500e+00],\n",
            "        [ 2.8082e+00],\n",
            "        [-3.0956e-01],\n",
            "        [ 1.9624e+00],\n",
            "        [ 3.0642e-03],\n",
            "        [-4.7671e-02],\n",
            "        [ 1.5212e+00],\n",
            "        [-1.7315e-01],\n",
            "        [ 1.5487e+00],\n",
            "        [ 1.9145e+00],\n",
            "        [ 1.9473e+00],\n",
            "        [-3.8438e-01],\n",
            "        [ 8.6268e-01],\n",
            "        [ 1.9039e+00],\n",
            "        [ 1.5165e+00],\n",
            "        [ 1.2572e+00],\n",
            "        [ 8.3379e-02],\n",
            "        [ 1.4593e+00],\n",
            "        [-1.6408e-03],\n",
            "        [-4.2202e-01],\n",
            "        [-2.6769e-01],\n",
            "        [ 1.7152e-01],\n",
            "        [ 2.0583e+00],\n",
            "        [ 2.1558e+00],\n",
            "        [ 6.7803e-02],\n",
            "        [ 7.0476e-01],\n",
            "        [ 8.0181e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.4650, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.5516],\n",
            "        [ 2.4653],\n",
            "        [-0.2530],\n",
            "        [ 2.0827],\n",
            "        [ 0.0486],\n",
            "        [ 1.9666],\n",
            "        [ 0.7867],\n",
            "        [ 1.7067],\n",
            "        [-0.3052],\n",
            "        [ 0.2070],\n",
            "        [ 1.9726],\n",
            "        [-0.2733],\n",
            "        [ 1.9613],\n",
            "        [-0.4401],\n",
            "        [ 0.0421],\n",
            "        [ 2.2525],\n",
            "        [ 1.1698],\n",
            "        [ 2.2648],\n",
            "        [ 2.2625],\n",
            "        [-0.1113],\n",
            "        [-0.2097],\n",
            "        [ 1.8812],\n",
            "        [ 1.8946],\n",
            "        [ 1.0878],\n",
            "        [ 0.0570],\n",
            "        [ 2.3834],\n",
            "        [-0.1267],\n",
            "        [ 2.1275],\n",
            "        [ 0.5537],\n",
            "        [ 2.1149],\n",
            "        [ 1.9740],\n",
            "        [-0.1656]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.0961, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.0615],\n",
            "        [ 1.8991],\n",
            "        [ 1.1064],\n",
            "        [ 2.2068],\n",
            "        [-0.1391],\n",
            "        [ 1.4115],\n",
            "        [ 2.0394],\n",
            "        [ 1.6283],\n",
            "        [ 2.1352],\n",
            "        [ 0.1005],\n",
            "        [-0.1032],\n",
            "        [ 0.2484],\n",
            "        [ 1.7757],\n",
            "        [ 0.3453],\n",
            "        [ 1.6956],\n",
            "        [ 0.6446],\n",
            "        [-0.1065],\n",
            "        [ 2.2683],\n",
            "        [ 1.4176],\n",
            "        [ 0.4924],\n",
            "        [-0.1176],\n",
            "        [ 1.9770],\n",
            "        [ 0.0581],\n",
            "        [-0.1849],\n",
            "        [ 0.2594],\n",
            "        [-0.0512],\n",
            "        [ 0.1413],\n",
            "        [ 1.8748],\n",
            "        [-0.0389],\n",
            "        [-0.2372],\n",
            "        [ 2.0994],\n",
            "        [ 0.1224]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3879, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[-0.1521],\n",
            "        [ 1.3400],\n",
            "        [ 1.5905],\n",
            "        [ 1.4378],\n",
            "        [ 1.2429],\n",
            "        [ 2.3495],\n",
            "        [ 2.0739],\n",
            "        [ 0.0263],\n",
            "        [-0.0720],\n",
            "        [-0.0616],\n",
            "        [ 0.0838],\n",
            "        [ 1.1222],\n",
            "        [ 0.5472],\n",
            "        [ 0.6648],\n",
            "        [ 2.3286],\n",
            "        [ 2.0652],\n",
            "        [ 0.1197],\n",
            "        [ 1.1032],\n",
            "        [-0.1510],\n",
            "        [-0.1787],\n",
            "        [ 2.2455],\n",
            "        [-0.0693],\n",
            "        [-0.0788],\n",
            "        [ 1.7343],\n",
            "        [ 0.9764],\n",
            "        [-0.2045],\n",
            "        [-0.4347],\n",
            "        [-0.0313],\n",
            "        [ 0.9331],\n",
            "        [ 2.3502],\n",
            "        [ 2.5863],\n",
            "        [ 2.0351]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(1.0910, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[-0.0833],\n",
            "        [ 1.1824],\n",
            "        [ 0.2189],\n",
            "        [ 2.1569],\n",
            "        [ 2.0736],\n",
            "        [ 2.0269],\n",
            "        [ 1.8667],\n",
            "        [-0.2166],\n",
            "        [-0.1698],\n",
            "        [-0.0515],\n",
            "        [ 0.1045],\n",
            "        [ 1.1849],\n",
            "        [ 0.3988],\n",
            "        [ 0.1514],\n",
            "        [ 1.1300],\n",
            "        [-0.0868],\n",
            "        [ 2.2303],\n",
            "        [ 1.8449],\n",
            "        [ 2.6051],\n",
            "        [ 2.2198],\n",
            "        [ 1.9154],\n",
            "        [ 1.9194],\n",
            "        [ 0.1473],\n",
            "        [ 2.3436],\n",
            "        [-0.0217],\n",
            "        [ 2.1711],\n",
            "        [ 1.8558],\n",
            "        [ 0.2148],\n",
            "        [ 1.3650],\n",
            "        [ 2.3382],\n",
            "        [-0.1417],\n",
            "        [-0.2030]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3792, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 2.0798],\n",
            "        [ 0.0757],\n",
            "        [ 0.0710],\n",
            "        [-0.1453],\n",
            "        [-0.0178],\n",
            "        [ 1.8607],\n",
            "        [ 0.5346],\n",
            "        [ 1.4616],\n",
            "        [ 2.2771],\n",
            "        [ 2.3090],\n",
            "        [ 1.8055],\n",
            "        [ 0.1867],\n",
            "        [ 1.4451],\n",
            "        [ 2.3247],\n",
            "        [ 1.9566],\n",
            "        [ 0.3049],\n",
            "        [ 1.4313],\n",
            "        [ 2.3851],\n",
            "        [ 0.0647],\n",
            "        [ 0.0998],\n",
            "        [-0.0715],\n",
            "        [ 2.2314],\n",
            "        [ 2.0534],\n",
            "        [ 0.2339],\n",
            "        [ 2.6790],\n",
            "        [ 1.3121],\n",
            "        [ 2.4586],\n",
            "        [ 1.9748],\n",
            "        [ 1.8848],\n",
            "        [-0.3258],\n",
            "        [ 2.2961],\n",
            "        [ 2.2099]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3958, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.7469],\n",
            "        [-0.1861],\n",
            "        [-0.1165],\n",
            "        [ 0.2303],\n",
            "        [ 1.7652],\n",
            "        [ 0.1297],\n",
            "        [ 2.0482],\n",
            "        [ 0.1822],\n",
            "        [ 0.2075],\n",
            "        [-0.0903],\n",
            "        [ 0.2323],\n",
            "        [ 2.0264],\n",
            "        [ 0.0432],\n",
            "        [ 1.9022],\n",
            "        [ 1.7951],\n",
            "        [ 1.1942],\n",
            "        [ 1.9250],\n",
            "        [-0.2009],\n",
            "        [ 2.2568],\n",
            "        [ 1.6510],\n",
            "        [ 2.5674],\n",
            "        [ 0.2955],\n",
            "        [ 0.2618],\n",
            "        [ 2.3468],\n",
            "        [ 2.3376],\n",
            "        [ 0.4212],\n",
            "        [ 1.7710],\n",
            "        [ 1.9621],\n",
            "        [ 1.0941],\n",
            "        [ 0.0905],\n",
            "        [ 0.5562],\n",
            "        [ 1.5776]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.8079, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 2.1199],\n",
            "        [ 2.3672],\n",
            "        [ 1.3502],\n",
            "        [ 2.3268],\n",
            "        [ 1.9533],\n",
            "        [ 2.5226],\n",
            "        [ 2.2956],\n",
            "        [ 0.1331],\n",
            "        [-0.1037],\n",
            "        [ 1.3974],\n",
            "        [ 0.3630],\n",
            "        [ 0.0427],\n",
            "        [ 1.3076],\n",
            "        [ 0.3067],\n",
            "        [ 1.1820],\n",
            "        [ 1.5987],\n",
            "        [ 1.2776],\n",
            "        [ 1.8154],\n",
            "        [ 1.3401],\n",
            "        [ 0.0479],\n",
            "        [ 1.6137],\n",
            "        [ 1.8715],\n",
            "        [-0.0264],\n",
            "        [ 0.6861],\n",
            "        [ 2.0571],\n",
            "        [-0.0201],\n",
            "        [ 0.1196],\n",
            "        [ 1.0158],\n",
            "        [ 1.7711],\n",
            "        [ 0.6063],\n",
            "        [ 1.3863],\n",
            "        [ 0.0155]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3390, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[0.3135],\n",
            "        [0.9008],\n",
            "        [0.0046],\n",
            "        [0.6879],\n",
            "        [0.4349],\n",
            "        [0.1614],\n",
            "        [1.9876],\n",
            "        [1.9474],\n",
            "        [0.7885],\n",
            "        [1.7902],\n",
            "        [1.2565],\n",
            "        [1.0591],\n",
            "        [1.6724],\n",
            "        [0.9852],\n",
            "        [1.3665],\n",
            "        [0.0106],\n",
            "        [1.9403],\n",
            "        [1.8213],\n",
            "        [2.0540],\n",
            "        [2.5846],\n",
            "        [0.3630],\n",
            "        [1.2870],\n",
            "        [0.1110],\n",
            "        [1.8022],\n",
            "        [1.1156],\n",
            "        [1.7664],\n",
            "        [0.0724],\n",
            "        [1.3026],\n",
            "        [1.5737],\n",
            "        [1.2078],\n",
            "        [0.3265],\n",
            "        [2.4369]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1801, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.0948],\n",
            "        [ 2.3031],\n",
            "        [ 1.9282],\n",
            "        [ 0.9040],\n",
            "        [ 0.2425],\n",
            "        [ 2.1895],\n",
            "        [ 1.4645],\n",
            "        [ 1.1472],\n",
            "        [ 2.3717],\n",
            "        [ 1.9295],\n",
            "        [ 0.7402],\n",
            "        [-0.0075],\n",
            "        [ 1.4266],\n",
            "        [ 1.4714],\n",
            "        [ 1.8766],\n",
            "        [ 0.9212],\n",
            "        [ 1.0694],\n",
            "        [ 0.2652],\n",
            "        [ 1.0534],\n",
            "        [ 0.1043],\n",
            "        [ 1.9512],\n",
            "        [ 2.0543],\n",
            "        [ 1.0885],\n",
            "        [ 2.2320],\n",
            "        [ 0.0644],\n",
            "        [ 0.0873],\n",
            "        [ 0.5123],\n",
            "        [ 0.0608],\n",
            "        [ 1.9143],\n",
            "        [ 1.5439],\n",
            "        [ 0.6952],\n",
            "        [ 2.0555]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3276, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 2.0616],\n",
            "        [ 2.0728],\n",
            "        [ 1.3672],\n",
            "        [ 0.6813],\n",
            "        [ 0.1890],\n",
            "        [ 1.8151],\n",
            "        [ 0.1016],\n",
            "        [ 2.2993],\n",
            "        [ 0.0034],\n",
            "        [ 2.1817],\n",
            "        [ 0.2828],\n",
            "        [ 0.0544],\n",
            "        [ 0.4275],\n",
            "        [ 0.1776],\n",
            "        [ 0.3732],\n",
            "        [ 0.1634],\n",
            "        [ 1.7009],\n",
            "        [ 0.2700],\n",
            "        [ 0.9363],\n",
            "        [ 1.5274],\n",
            "        [ 0.3320],\n",
            "        [ 0.9105],\n",
            "        [ 0.7360],\n",
            "        [ 2.3038],\n",
            "        [ 1.7178],\n",
            "        [ 2.2536],\n",
            "        [ 2.0175],\n",
            "        [ 1.7983],\n",
            "        [-0.0149],\n",
            "        [ 0.2455],\n",
            "        [ 1.7546],\n",
            "        [ 1.4111]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.5267, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.9078],\n",
            "        [ 2.4748],\n",
            "        [-0.2338],\n",
            "        [ 1.8354],\n",
            "        [ 1.8080],\n",
            "        [ 0.7081],\n",
            "        [ 0.3748],\n",
            "        [ 0.1211],\n",
            "        [ 0.2636],\n",
            "        [ 1.8720],\n",
            "        [ 1.5726],\n",
            "        [ 1.9294],\n",
            "        [ 2.0240],\n",
            "        [ 1.8094],\n",
            "        [ 2.1594],\n",
            "        [ 0.5111],\n",
            "        [-0.1003],\n",
            "        [ 0.8757],\n",
            "        [ 0.0972],\n",
            "        [ 0.9503],\n",
            "        [ 1.1343],\n",
            "        [ 0.8142],\n",
            "        [ 2.1616],\n",
            "        [ 1.6999],\n",
            "        [ 0.9113],\n",
            "        [ 1.0059],\n",
            "        [ 0.0259],\n",
            "        [ 2.1429],\n",
            "        [ 0.1035],\n",
            "        [ 0.7412],\n",
            "        [ 2.1287],\n",
            "        [ 1.6489]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3424, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[0.8618],\n",
            "        [1.9458],\n",
            "        [2.0588],\n",
            "        [0.3272],\n",
            "        [1.8311],\n",
            "        [0.5271],\n",
            "        [0.1964],\n",
            "        [1.8485],\n",
            "        [2.0374],\n",
            "        [0.1705],\n",
            "        [1.6845],\n",
            "        [1.8884],\n",
            "        [1.4994],\n",
            "        [2.2895],\n",
            "        [1.8675],\n",
            "        [0.8700],\n",
            "        [1.9473],\n",
            "        [1.0669],\n",
            "        [1.8069],\n",
            "        [1.5785],\n",
            "        [0.2765],\n",
            "        [1.3638],\n",
            "        [0.3966],\n",
            "        [2.2318],\n",
            "        [1.3924],\n",
            "        [1.9534],\n",
            "        [1.5328],\n",
            "        [0.2791],\n",
            "        [2.1013],\n",
            "        [0.3177],\n",
            "        [0.0922],\n",
            "        [0.0250]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2611, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.8151],\n",
            "        [ 1.9378],\n",
            "        [ 1.6310],\n",
            "        [ 2.1405],\n",
            "        [ 0.4338],\n",
            "        [-0.1100],\n",
            "        [ 0.1429],\n",
            "        [ 1.8680],\n",
            "        [ 1.0778],\n",
            "        [ 0.7322],\n",
            "        [ 0.1784],\n",
            "        [ 1.7742],\n",
            "        [ 2.1575],\n",
            "        [ 1.3796],\n",
            "        [ 1.9260],\n",
            "        [ 1.7135],\n",
            "        [ 1.7585],\n",
            "        [ 0.3040],\n",
            "        [ 1.9264],\n",
            "        [ 0.2319],\n",
            "        [ 1.8172],\n",
            "        [ 1.9720],\n",
            "        [ 2.2047],\n",
            "        [ 0.1827],\n",
            "        [ 0.9711],\n",
            "        [-0.0157],\n",
            "        [ 0.0858],\n",
            "        [ 0.0406],\n",
            "        [-0.0251],\n",
            "        [ 0.0071],\n",
            "        [ 1.7611],\n",
            "        [ 0.0365]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3043, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.3950],\n",
            "        [ 0.2486],\n",
            "        [ 2.4072],\n",
            "        [ 1.0604],\n",
            "        [ 1.7099],\n",
            "        [ 1.9963],\n",
            "        [ 0.0438],\n",
            "        [ 1.9123],\n",
            "        [ 2.2042],\n",
            "        [-0.0217],\n",
            "        [ 1.8161],\n",
            "        [ 2.0254],\n",
            "        [ 0.4827],\n",
            "        [ 1.3015],\n",
            "        [ 1.2423],\n",
            "        [-0.0926],\n",
            "        [ 2.1186],\n",
            "        [ 0.2548],\n",
            "        [ 1.1079],\n",
            "        [ 0.1446],\n",
            "        [ 0.1624],\n",
            "        [ 0.0730],\n",
            "        [-0.0304],\n",
            "        [ 0.4748],\n",
            "        [ 1.1998],\n",
            "        [ 2.0499],\n",
            "        [ 2.0911],\n",
            "        [ 0.2450],\n",
            "        [ 1.3546],\n",
            "        [ 0.3535],\n",
            "        [ 0.4042],\n",
            "        [ 2.0876]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.8056, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[0.4561],\n",
            "        [2.5331],\n",
            "        [1.1161],\n",
            "        [1.0278],\n",
            "        [1.8555],\n",
            "        [1.6009],\n",
            "        [1.4847],\n",
            "        [1.4790],\n",
            "        [1.6919],\n",
            "        [1.4379],\n",
            "        [0.5173],\n",
            "        [0.2682],\n",
            "        [0.1781],\n",
            "        [1.7554],\n",
            "        [2.9427],\n",
            "        [0.1673],\n",
            "        [2.0241],\n",
            "        [0.4578],\n",
            "        [1.9794],\n",
            "        [1.8828],\n",
            "        [1.7036],\n",
            "        [0.0890],\n",
            "        [0.0306],\n",
            "        [0.1558],\n",
            "        [2.1898],\n",
            "        [1.6720],\n",
            "        [0.7469],\n",
            "        [1.8682],\n",
            "        [2.1674],\n",
            "        [2.2000],\n",
            "        [2.3159],\n",
            "        [1.9190]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3558, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.6971],\n",
            "        [ 0.4830],\n",
            "        [ 1.8992],\n",
            "        [ 0.8958],\n",
            "        [ 2.1539],\n",
            "        [ 1.7227],\n",
            "        [ 0.2210],\n",
            "        [ 2.0789],\n",
            "        [ 2.2124],\n",
            "        [ 0.1744],\n",
            "        [ 0.3011],\n",
            "        [ 1.1002],\n",
            "        [ 2.2450],\n",
            "        [ 1.5454],\n",
            "        [ 2.0511],\n",
            "        [ 1.8022],\n",
            "        [ 2.0615],\n",
            "        [ 1.7106],\n",
            "        [ 2.1482],\n",
            "        [ 0.9584],\n",
            "        [-0.1112],\n",
            "        [ 2.1088],\n",
            "        [ 0.2183],\n",
            "        [ 0.2980],\n",
            "        [ 2.0149],\n",
            "        [ 0.3171],\n",
            "        [ 0.0991],\n",
            "        [ 1.5525],\n",
            "        [ 2.6239],\n",
            "        [ 0.6970],\n",
            "        [-0.0069],\n",
            "        [ 0.8819]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1523, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.2609],\n",
            "        [ 0.2387],\n",
            "        [ 2.5380],\n",
            "        [ 1.1451],\n",
            "        [ 2.0421],\n",
            "        [ 2.5651],\n",
            "        [ 2.2772],\n",
            "        [-0.1381],\n",
            "        [ 0.1877],\n",
            "        [ 1.9606],\n",
            "        [ 0.0668],\n",
            "        [ 1.0431],\n",
            "        [ 2.2546],\n",
            "        [ 2.5488],\n",
            "        [ 2.0104],\n",
            "        [ 2.0061],\n",
            "        [ 2.4398],\n",
            "        [ 0.1360],\n",
            "        [ 0.4241],\n",
            "        [ 1.7654],\n",
            "        [-0.1393],\n",
            "        [ 0.0801],\n",
            "        [ 1.2027],\n",
            "        [ 0.2182],\n",
            "        [ 2.2413],\n",
            "        [ 1.3658],\n",
            "        [ 1.1018],\n",
            "        [ 2.0400],\n",
            "        [ 2.1000],\n",
            "        [ 0.8769],\n",
            "        [-0.0146],\n",
            "        [ 1.9651]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2742, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.2298],\n",
            "        [ 0.2293],\n",
            "        [ 2.3100],\n",
            "        [ 2.1539],\n",
            "        [ 2.0978],\n",
            "        [ 1.9546],\n",
            "        [ 0.4110],\n",
            "        [ 2.3572],\n",
            "        [ 2.4967],\n",
            "        [ 1.8636],\n",
            "        [ 2.1566],\n",
            "        [ 0.2381],\n",
            "        [ 0.0350],\n",
            "        [ 2.2987],\n",
            "        [ 0.4001],\n",
            "        [ 0.4330],\n",
            "        [ 0.3163],\n",
            "        [ 2.0727],\n",
            "        [ 1.7670],\n",
            "        [ 0.9876],\n",
            "        [ 0.5691],\n",
            "        [ 1.9602],\n",
            "        [ 0.1564],\n",
            "        [ 1.2485],\n",
            "        [ 2.0679],\n",
            "        [ 0.4878],\n",
            "        [-0.0608],\n",
            "        [ 0.7774],\n",
            "        [ 2.4725],\n",
            "        [ 2.1609],\n",
            "        [-0.0675],\n",
            "        [ 2.1383]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(1.3585, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.0403],\n",
            "        [ 2.8766],\n",
            "        [ 0.1992],\n",
            "        [ 2.2132],\n",
            "        [ 0.0844],\n",
            "        [ 2.1741],\n",
            "        [ 0.8010],\n",
            "        [ 1.6704],\n",
            "        [ 0.1042],\n",
            "        [-0.1389],\n",
            "        [ 0.4036],\n",
            "        [ 1.2397],\n",
            "        [ 1.5631],\n",
            "        [ 2.0340],\n",
            "        [ 2.2683],\n",
            "        [ 2.2306],\n",
            "        [ 0.4361],\n",
            "        [ 0.1197],\n",
            "        [ 0.2476],\n",
            "        [ 1.2306],\n",
            "        [ 2.6106],\n",
            "        [ 2.6834],\n",
            "        [ 1.8329],\n",
            "        [ 0.7658],\n",
            "        [ 1.6694],\n",
            "        [ 0.2067],\n",
            "        [ 0.1289],\n",
            "        [ 0.1826],\n",
            "        [ 1.9553],\n",
            "        [ 1.5672],\n",
            "        [ 1.4392],\n",
            "        [ 0.1472]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1025, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[-0.0477],\n",
            "        [ 0.0306],\n",
            "        [ 0.2224],\n",
            "        [ 2.2022],\n",
            "        [ 1.8366],\n",
            "        [ 1.1321],\n",
            "        [ 0.2750],\n",
            "        [ 2.4549],\n",
            "        [ 2.3264],\n",
            "        [ 2.1524],\n",
            "        [-0.0898],\n",
            "        [ 0.3256],\n",
            "        [ 0.2164],\n",
            "        [ 1.3445],\n",
            "        [ 0.3110],\n",
            "        [ 1.8237],\n",
            "        [ 0.8178],\n",
            "        [ 2.2510],\n",
            "        [ 1.1062],\n",
            "        [ 1.3322],\n",
            "        [ 2.1796],\n",
            "        [-0.0933],\n",
            "        [ 0.0212],\n",
            "        [ 2.1350],\n",
            "        [-0.1377],\n",
            "        [ 0.1112],\n",
            "        [ 2.2150],\n",
            "        [ 0.1532],\n",
            "        [ 1.2463],\n",
            "        [ 2.1130],\n",
            "        [ 0.1993],\n",
            "        [ 0.1679]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2544, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 2.2840],\n",
            "        [-0.0828],\n",
            "        [ 2.1475],\n",
            "        [ 0.9775],\n",
            "        [ 0.1072],\n",
            "        [ 1.8036],\n",
            "        [ 1.7807],\n",
            "        [ 1.1695],\n",
            "        [ 0.2283],\n",
            "        [ 1.7695],\n",
            "        [ 0.0846],\n",
            "        [ 1.5994],\n",
            "        [ 1.9552],\n",
            "        [ 2.4498],\n",
            "        [ 1.8758],\n",
            "        [ 0.0935],\n",
            "        [ 2.4086],\n",
            "        [ 0.0924],\n",
            "        [ 1.9621],\n",
            "        [-0.0031],\n",
            "        [ 1.4618],\n",
            "        [ 1.8835],\n",
            "        [ 1.7460],\n",
            "        [ 1.8589],\n",
            "        [ 1.8550],\n",
            "        [ 1.9478],\n",
            "        [ 0.1153],\n",
            "        [ 1.3969],\n",
            "        [ 1.3177],\n",
            "        [ 1.9857],\n",
            "        [ 2.0260],\n",
            "        [ 0.5946]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1361, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.8604],\n",
            "        [-0.0151],\n",
            "        [ 1.5692],\n",
            "        [-0.2352],\n",
            "        [ 1.6542],\n",
            "        [ 0.1837],\n",
            "        [ 1.7639],\n",
            "        [ 0.0937],\n",
            "        [ 0.0702],\n",
            "        [ 1.9561],\n",
            "        [ 0.2471],\n",
            "        [ 2.1919],\n",
            "        [ 0.5248],\n",
            "        [-0.0804],\n",
            "        [ 0.6121],\n",
            "        [ 2.1601],\n",
            "        [ 0.9790],\n",
            "        [ 0.6833],\n",
            "        [ 0.0292],\n",
            "        [ 1.9335],\n",
            "        [ 2.4433],\n",
            "        [ 0.0976],\n",
            "        [ 0.3960],\n",
            "        [ 1.9920],\n",
            "        [ 2.2048],\n",
            "        [ 0.3360],\n",
            "        [ 1.3446],\n",
            "        [ 2.4384],\n",
            "        [ 1.6484],\n",
            "        [ 0.1245],\n",
            "        [ 1.9540],\n",
            "        [-0.0154]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.5657, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.1156],\n",
            "        [ 1.5340],\n",
            "        [ 0.7924],\n",
            "        [ 0.2994],\n",
            "        [-0.0260],\n",
            "        [-0.1895],\n",
            "        [ 0.1220],\n",
            "        [ 0.2637],\n",
            "        [ 2.5144],\n",
            "        [-0.1487],\n",
            "        [ 1.7148],\n",
            "        [ 1.0734],\n",
            "        [ 1.8482],\n",
            "        [ 0.3016],\n",
            "        [ 1.6798],\n",
            "        [ 0.0454],\n",
            "        [ 2.0181],\n",
            "        [-0.0476],\n",
            "        [ 1.9153],\n",
            "        [-0.1851],\n",
            "        [ 1.8053],\n",
            "        [ 0.4774],\n",
            "        [ 0.6732],\n",
            "        [ 1.7753],\n",
            "        [ 0.0245],\n",
            "        [ 0.0722],\n",
            "        [-0.1112],\n",
            "        [ 1.4210],\n",
            "        [ 0.8887],\n",
            "        [ 0.4987],\n",
            "        [ 0.3762],\n",
            "        [ 0.7254]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2884, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.0527],\n",
            "        [ 1.8682],\n",
            "        [ 0.4082],\n",
            "        [ 1.8233],\n",
            "        [ 0.3309],\n",
            "        [ 1.2261],\n",
            "        [-0.0119],\n",
            "        [ 0.2247],\n",
            "        [ 0.3709],\n",
            "        [ 0.1337],\n",
            "        [-0.1095],\n",
            "        [ 1.0418],\n",
            "        [ 0.5193],\n",
            "        [ 0.9113],\n",
            "        [ 0.7022],\n",
            "        [-0.1988],\n",
            "        [ 0.3930],\n",
            "        [ 0.1246],\n",
            "        [ 0.5956],\n",
            "        [-0.0653],\n",
            "        [ 0.0083],\n",
            "        [ 1.4571],\n",
            "        [ 1.8612],\n",
            "        [ 2.5910],\n",
            "        [ 1.5296],\n",
            "        [ 0.3810],\n",
            "        [ 0.2273],\n",
            "        [ 1.9863],\n",
            "        [ 0.4654],\n",
            "        [ 0.9568],\n",
            "        [ 2.0887],\n",
            "        [ 0.3360]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "  Batch    40  of     66.    Elapsed: 18.810888.\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1709, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.8769],\n",
            "        [ 0.1790],\n",
            "        [ 0.2622],\n",
            "        [-0.0498],\n",
            "        [ 1.8614],\n",
            "        [ 0.2985],\n",
            "        [ 0.5537],\n",
            "        [ 2.1516],\n",
            "        [-0.1848],\n",
            "        [-0.1128],\n",
            "        [ 0.3095],\n",
            "        [ 0.4053],\n",
            "        [-0.0423],\n",
            "        [ 1.7446],\n",
            "        [ 1.8125],\n",
            "        [ 1.4799],\n",
            "        [-0.0464],\n",
            "        [ 0.5429],\n",
            "        [ 0.2204],\n",
            "        [ 0.2609],\n",
            "        [ 2.1902],\n",
            "        [ 2.1050],\n",
            "        [ 0.7195],\n",
            "        [ 0.3043],\n",
            "        [ 0.4015],\n",
            "        [ 0.0475],\n",
            "        [ 0.3918],\n",
            "        [-0.0091],\n",
            "        [ 0.0443],\n",
            "        [ 1.7770],\n",
            "        [ 2.0895],\n",
            "        [-0.0706]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2481, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.6430],\n",
            "        [ 0.1076],\n",
            "        [ 0.2008],\n",
            "        [ 0.0703],\n",
            "        [ 0.1756],\n",
            "        [ 1.2392],\n",
            "        [ 1.8292],\n",
            "        [ 2.1429],\n",
            "        [ 1.3056],\n",
            "        [ 1.8920],\n",
            "        [-0.1063],\n",
            "        [ 0.4448],\n",
            "        [ 0.4736],\n",
            "        [ 0.1422],\n",
            "        [-0.0718],\n",
            "        [ 1.8641],\n",
            "        [ 0.1512],\n",
            "        [ 2.1292],\n",
            "        [ 2.0575],\n",
            "        [ 0.5917],\n",
            "        [-0.0207],\n",
            "        [-0.0099],\n",
            "        [-0.0290],\n",
            "        [ 1.7832],\n",
            "        [ 0.1472],\n",
            "        [ 0.8710],\n",
            "        [ 0.1109],\n",
            "        [ 1.7917],\n",
            "        [ 1.5520],\n",
            "        [ 1.6981],\n",
            "        [ 1.6502],\n",
            "        [ 1.6714]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.6667, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.1987],\n",
            "        [ 1.6781],\n",
            "        [ 0.0881],\n",
            "        [ 1.7874],\n",
            "        [ 0.3845],\n",
            "        [ 0.8735],\n",
            "        [-0.0131],\n",
            "        [ 0.0948],\n",
            "        [ 0.2235],\n",
            "        [ 2.0902],\n",
            "        [ 0.1262],\n",
            "        [ 0.9521],\n",
            "        [ 0.4083],\n",
            "        [ 1.0096],\n",
            "        [ 1.8521],\n",
            "        [ 1.9459],\n",
            "        [-0.0711],\n",
            "        [-0.1312],\n",
            "        [ 0.2478],\n",
            "        [ 1.0041],\n",
            "        [ 0.2030],\n",
            "        [ 1.9922],\n",
            "        [ 0.0330],\n",
            "        [-0.0345],\n",
            "        [ 1.4353],\n",
            "        [ 2.4450],\n",
            "        [ 1.7281],\n",
            "        [ 0.1705],\n",
            "        [ 0.2221],\n",
            "        [ 0.0671],\n",
            "        [ 1.7817],\n",
            "        [ 2.8590]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.4178, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 2.2594],\n",
            "        [ 0.4346],\n",
            "        [ 1.0839],\n",
            "        [-0.1067],\n",
            "        [ 0.1980],\n",
            "        [-0.1578],\n",
            "        [ 2.2237],\n",
            "        [-0.0221],\n",
            "        [ 1.9997],\n",
            "        [ 0.8522],\n",
            "        [ 2.8577],\n",
            "        [ 2.4501],\n",
            "        [ 0.5057],\n",
            "        [ 0.0626],\n",
            "        [ 1.5140],\n",
            "        [ 1.1366],\n",
            "        [ 1.8952],\n",
            "        [ 1.3940],\n",
            "        [ 0.1656],\n",
            "        [ 0.0880],\n",
            "        [ 1.5738],\n",
            "        [ 1.6247],\n",
            "        [ 0.1330],\n",
            "        [ 1.9823],\n",
            "        [ 1.1895],\n",
            "        [ 2.1856],\n",
            "        [ 2.7266],\n",
            "        [ 1.2255],\n",
            "        [ 1.0476],\n",
            "        [ 1.8921],\n",
            "        [ 0.1588],\n",
            "        [ 2.1102]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1348, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.8000],\n",
            "        [ 0.1225],\n",
            "        [ 1.7869],\n",
            "        [ 2.8329],\n",
            "        [ 1.3972],\n",
            "        [ 0.0415],\n",
            "        [ 0.1796],\n",
            "        [ 2.0124],\n",
            "        [ 1.8725],\n",
            "        [ 2.2242],\n",
            "        [ 1.8999],\n",
            "        [ 2.0397],\n",
            "        [-0.1887],\n",
            "        [ 1.9152],\n",
            "        [ 1.5251],\n",
            "        [ 0.0251],\n",
            "        [ 0.5194],\n",
            "        [ 1.9179],\n",
            "        [ 0.5685],\n",
            "        [ 0.2249],\n",
            "        [ 1.9170],\n",
            "        [ 0.2406],\n",
            "        [ 1.3464],\n",
            "        [ 1.8646],\n",
            "        [ 0.7942],\n",
            "        [ 2.6929],\n",
            "        [ 0.6417],\n",
            "        [ 1.9524],\n",
            "        [ 1.8137],\n",
            "        [ 0.2556],\n",
            "        [ 1.6922],\n",
            "        [ 0.0850]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2797, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[1.7593e-01],\n",
            "        [1.0896e+00],\n",
            "        [1.7941e+00],\n",
            "        [2.9213e+00],\n",
            "        [1.7875e-01],\n",
            "        [2.3530e+00],\n",
            "        [1.9008e+00],\n",
            "        [1.3955e-01],\n",
            "        [2.0660e+00],\n",
            "        [1.5167e-01],\n",
            "        [3.2307e-01],\n",
            "        [2.0861e+00],\n",
            "        [2.1262e+00],\n",
            "        [1.8928e+00],\n",
            "        [1.5653e-03],\n",
            "        [2.2954e-01],\n",
            "        [1.6954e+00],\n",
            "        [1.5752e-01],\n",
            "        [3.7571e-01],\n",
            "        [1.7545e+00],\n",
            "        [1.8815e+00],\n",
            "        [1.9276e+00],\n",
            "        [2.1287e+00],\n",
            "        [2.3873e-01],\n",
            "        [2.4764e+00],\n",
            "        [1.8641e+00],\n",
            "        [1.1474e+00],\n",
            "        [9.1352e-01],\n",
            "        [1.1238e+00],\n",
            "        [1.2218e+00],\n",
            "        [2.7057e-01],\n",
            "        [2.1634e+00]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1346, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 2.6668e-01],\n",
            "        [-1.2800e-01],\n",
            "        [ 6.2059e-02],\n",
            "        [ 8.8491e-01],\n",
            "        [ 1.5648e-01],\n",
            "        [ 1.1558e+00],\n",
            "        [ 5.4223e-01],\n",
            "        [ 1.1563e-03],\n",
            "        [ 2.4999e+00],\n",
            "        [ 2.0171e+00],\n",
            "        [ 1.1528e+00],\n",
            "        [ 2.7810e-01],\n",
            "        [ 1.6985e+00],\n",
            "        [ 3.8011e-01],\n",
            "        [ 2.0904e-02],\n",
            "        [ 1.6740e+00],\n",
            "        [ 3.9421e-01],\n",
            "        [ 9.2529e-01],\n",
            "        [ 6.0318e-01],\n",
            "        [ 1.0333e+00],\n",
            "        [-2.3939e-01],\n",
            "        [ 2.1851e+00],\n",
            "        [-1.6759e-01],\n",
            "        [ 2.1907e+00],\n",
            "        [ 2.4809e+00],\n",
            "        [ 4.0833e-01],\n",
            "        [ 2.1108e+00],\n",
            "        [ 8.4501e-01],\n",
            "        [-3.6468e-02],\n",
            "        [ 2.7638e-01],\n",
            "        [ 2.1276e+00],\n",
            "        [ 3.3102e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3855, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.9224],\n",
            "        [ 1.9327],\n",
            "        [ 1.2102],\n",
            "        [ 1.1064],\n",
            "        [ 1.8774],\n",
            "        [ 1.7607],\n",
            "        [ 1.1048],\n",
            "        [ 1.9193],\n",
            "        [ 0.6930],\n",
            "        [ 0.3651],\n",
            "        [ 1.5835],\n",
            "        [ 1.7924],\n",
            "        [ 2.3776],\n",
            "        [ 0.2877],\n",
            "        [ 0.0245],\n",
            "        [ 0.1782],\n",
            "        [ 0.0495],\n",
            "        [ 1.8197],\n",
            "        [ 2.0060],\n",
            "        [ 0.1171],\n",
            "        [ 0.1755],\n",
            "        [ 2.0357],\n",
            "        [ 0.8148],\n",
            "        [ 2.2676],\n",
            "        [ 2.1846],\n",
            "        [ 0.1744],\n",
            "        [-0.0148],\n",
            "        [ 0.2625],\n",
            "        [ 0.2029],\n",
            "        [ 1.9065],\n",
            "        [ 0.4821],\n",
            "        [ 0.1764]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3469, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.5288],\n",
            "        [ 0.5491],\n",
            "        [ 2.3286],\n",
            "        [-0.0351],\n",
            "        [ 2.2238],\n",
            "        [ 2.1232],\n",
            "        [ 1.9192],\n",
            "        [ 0.1502],\n",
            "        [ 1.6276],\n",
            "        [ 0.0608],\n",
            "        [ 0.3993],\n",
            "        [ 2.1784],\n",
            "        [-0.1026],\n",
            "        [ 1.4531],\n",
            "        [ 0.9192],\n",
            "        [ 0.1248],\n",
            "        [ 0.1970],\n",
            "        [ 2.1028],\n",
            "        [ 1.1662],\n",
            "        [ 0.3355],\n",
            "        [ 1.9321],\n",
            "        [ 1.3490],\n",
            "        [ 2.1282],\n",
            "        [ 0.3160],\n",
            "        [ 0.4631],\n",
            "        [ 2.0211],\n",
            "        [-0.0952],\n",
            "        [ 2.0217],\n",
            "        [ 0.3475],\n",
            "        [-0.1538],\n",
            "        [ 2.2030],\n",
            "        [ 2.1108]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.0663, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.8397],\n",
            "        [ 1.5964],\n",
            "        [ 2.0297],\n",
            "        [ 0.6301],\n",
            "        [ 1.8834],\n",
            "        [ 1.1001],\n",
            "        [ 1.1467],\n",
            "        [ 1.7260],\n",
            "        [ 0.1487],\n",
            "        [ 0.8418],\n",
            "        [ 2.0092],\n",
            "        [ 0.0709],\n",
            "        [ 0.8697],\n",
            "        [ 1.2729],\n",
            "        [ 0.1076],\n",
            "        [-0.0197],\n",
            "        [-0.2080],\n",
            "        [ 0.0698],\n",
            "        [ 2.2221],\n",
            "        [ 2.7354],\n",
            "        [-0.2158],\n",
            "        [-0.0055],\n",
            "        [ 1.8386],\n",
            "        [ 1.1792],\n",
            "        [ 0.1105],\n",
            "        [ 1.5185],\n",
            "        [-0.0059],\n",
            "        [ 0.3957],\n",
            "        [ 1.9793],\n",
            "        [ 1.1463],\n",
            "        [-0.0050],\n",
            "        [ 0.6439]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.5586, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.3856],\n",
            "        [ 0.3451],\n",
            "        [ 1.8061],\n",
            "        [ 2.1890],\n",
            "        [ 0.1490],\n",
            "        [ 0.8057],\n",
            "        [-0.2460],\n",
            "        [ 2.0146],\n",
            "        [ 0.2111],\n",
            "        [ 1.2552],\n",
            "        [ 1.8036],\n",
            "        [ 0.2197],\n",
            "        [ 1.8450],\n",
            "        [ 1.9477],\n",
            "        [ 0.5516],\n",
            "        [ 1.8553],\n",
            "        [ 0.0688],\n",
            "        [-0.2030],\n",
            "        [ 0.1294],\n",
            "        [-0.0242],\n",
            "        [ 2.0860],\n",
            "        [-0.1119],\n",
            "        [ 0.0188],\n",
            "        [ 2.0621],\n",
            "        [ 0.1696],\n",
            "        [ 0.8019],\n",
            "        [-0.1095],\n",
            "        [ 1.4915],\n",
            "        [ 0.0506],\n",
            "        [ 0.0616],\n",
            "        [ 0.9296],\n",
            "        [ 2.1247]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2045, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.0342],\n",
            "        [ 0.1151],\n",
            "        [ 0.0283],\n",
            "        [ 0.6305],\n",
            "        [ 0.2105],\n",
            "        [ 1.8463],\n",
            "        [ 0.9117],\n",
            "        [ 1.5513],\n",
            "        [ 2.1441],\n",
            "        [ 0.0732],\n",
            "        [ 0.7161],\n",
            "        [ 1.8126],\n",
            "        [ 0.6065],\n",
            "        [ 0.1166],\n",
            "        [ 0.6803],\n",
            "        [ 1.8962],\n",
            "        [ 1.1159],\n",
            "        [ 1.0847],\n",
            "        [ 0.3969],\n",
            "        [ 1.9138],\n",
            "        [ 0.2277],\n",
            "        [-0.0658],\n",
            "        [ 1.7580],\n",
            "        [ 2.5457],\n",
            "        [ 1.8754],\n",
            "        [ 0.2226],\n",
            "        [ 1.5991],\n",
            "        [ 1.7780],\n",
            "        [ 0.3902],\n",
            "        [ 1.0204],\n",
            "        [ 1.8293],\n",
            "        [ 0.9110]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1701, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.5659],\n",
            "        [ 0.3692],\n",
            "        [ 0.6590],\n",
            "        [ 0.1129],\n",
            "        [ 0.1347],\n",
            "        [ 1.5063],\n",
            "        [ 2.1279],\n",
            "        [ 2.0351],\n",
            "        [ 0.7228],\n",
            "        [ 2.1057],\n",
            "        [ 0.3309],\n",
            "        [ 1.9387],\n",
            "        [ 2.0502],\n",
            "        [ 1.7392],\n",
            "        [ 1.7235],\n",
            "        [ 0.1843],\n",
            "        [-0.2099],\n",
            "        [ 0.8159],\n",
            "        [ 0.8952],\n",
            "        [ 2.0693],\n",
            "        [ 0.0604],\n",
            "        [ 1.8846],\n",
            "        [ 0.2590],\n",
            "        [ 1.9002],\n",
            "        [ 0.1184],\n",
            "        [ 0.4638],\n",
            "        [ 0.1691],\n",
            "        [ 1.8417],\n",
            "        [ 2.0256],\n",
            "        [ 0.1402],\n",
            "        [ 0.2567],\n",
            "        [ 0.2004]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2142, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.0654],\n",
            "        [ 1.1913],\n",
            "        [ 0.5006],\n",
            "        [ 2.1389],\n",
            "        [ 0.2012],\n",
            "        [ 1.4609],\n",
            "        [ 1.6517],\n",
            "        [ 2.1084],\n",
            "        [ 1.9473],\n",
            "        [ 1.8865],\n",
            "        [ 0.6960],\n",
            "        [ 1.9520],\n",
            "        [ 0.7998],\n",
            "        [ 0.1471],\n",
            "        [ 0.1687],\n",
            "        [-0.0030],\n",
            "        [ 1.9684],\n",
            "        [-0.0077],\n",
            "        [ 0.0991],\n",
            "        [-0.0821],\n",
            "        [ 1.5032],\n",
            "        [ 2.1693],\n",
            "        [ 1.8104],\n",
            "        [ 1.7380],\n",
            "        [ 0.7533],\n",
            "        [ 0.1525],\n",
            "        [-0.0268],\n",
            "        [ 2.0098],\n",
            "        [ 2.0374],\n",
            "        [-0.1635],\n",
            "        [ 0.0264],\n",
            "        [-0.0585]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2133, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.2057],\n",
            "        [ 1.6679],\n",
            "        [ 1.7436],\n",
            "        [ 2.0996],\n",
            "        [ 1.3295],\n",
            "        [ 0.0337],\n",
            "        [-0.0963],\n",
            "        [-0.0592],\n",
            "        [ 1.8164],\n",
            "        [ 2.2918],\n",
            "        [ 0.3354],\n",
            "        [ 0.0500],\n",
            "        [ 0.2289],\n",
            "        [ 1.4085],\n",
            "        [ 0.0508],\n",
            "        [ 2.0395],\n",
            "        [ 0.1568],\n",
            "        [ 2.0451],\n",
            "        [ 1.3224],\n",
            "        [ 1.5747],\n",
            "        [ 0.8513],\n",
            "        [ 0.8524],\n",
            "        [ 0.6608],\n",
            "        [ 1.9099],\n",
            "        [ 1.7187],\n",
            "        [ 0.4766],\n",
            "        [ 0.9277],\n",
            "        [ 1.9105],\n",
            "        [ 1.2548],\n",
            "        [ 0.4906],\n",
            "        [ 1.9693],\n",
            "        [ 1.8839]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.7836, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.3427],\n",
            "        [ 2.1722],\n",
            "        [ 1.0741],\n",
            "        [ 0.0074],\n",
            "        [ 1.8576],\n",
            "        [ 0.3073],\n",
            "        [ 2.1252],\n",
            "        [ 1.8221],\n",
            "        [ 0.7593],\n",
            "        [ 0.1917],\n",
            "        [ 2.2604],\n",
            "        [ 0.4064],\n",
            "        [ 0.0322],\n",
            "        [ 0.1204],\n",
            "        [ 0.0647],\n",
            "        [ 1.0178],\n",
            "        [ 1.8595],\n",
            "        [ 0.8788],\n",
            "        [ 0.1158],\n",
            "        [ 0.2737],\n",
            "        [ 0.4014],\n",
            "        [ 2.8243],\n",
            "        [ 1.2333],\n",
            "        [ 0.9427],\n",
            "        [ 1.0940],\n",
            "        [-0.1928],\n",
            "        [ 1.8891],\n",
            "        [ 2.1166],\n",
            "        [ 1.9840],\n",
            "        [ 2.1041],\n",
            "        [ 0.1645],\n",
            "        [-0.2625]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2712, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 2.0489],\n",
            "        [ 0.2381],\n",
            "        [ 2.3384],\n",
            "        [ 1.2896],\n",
            "        [ 2.0380],\n",
            "        [ 0.7725],\n",
            "        [ 1.8581],\n",
            "        [ 1.9305],\n",
            "        [ 0.7141],\n",
            "        [ 1.0185],\n",
            "        [ 0.6393],\n",
            "        [ 2.2969],\n",
            "        [ 2.1691],\n",
            "        [-0.0488],\n",
            "        [ 0.1363],\n",
            "        [-0.0734],\n",
            "        [ 2.2497],\n",
            "        [ 0.2696],\n",
            "        [ 2.3026],\n",
            "        [ 0.9368],\n",
            "        [ 0.1373],\n",
            "        [ 1.5016],\n",
            "        [-0.0502],\n",
            "        [ 2.7282],\n",
            "        [ 2.4343],\n",
            "        [ 1.2771],\n",
            "        [ 0.9509],\n",
            "        [ 0.3331],\n",
            "        [ 0.8933],\n",
            "        [ 0.9993],\n",
            "        [ 2.5492],\n",
            "        [ 0.2654]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3000, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 2.2125],\n",
            "        [ 2.6102],\n",
            "        [ 0.1213],\n",
            "        [ 0.3216],\n",
            "        [ 2.0071],\n",
            "        [ 1.9964],\n",
            "        [ 2.4341],\n",
            "        [ 2.3693],\n",
            "        [ 1.3349],\n",
            "        [-0.0311],\n",
            "        [ 2.1476],\n",
            "        [-0.1629],\n",
            "        [ 0.0449],\n",
            "        [ 0.3470],\n",
            "        [-0.1140],\n",
            "        [ 0.6915],\n",
            "        [ 0.7483],\n",
            "        [ 0.1157],\n",
            "        [ 0.2902],\n",
            "        [ 2.1730],\n",
            "        [ 2.2119],\n",
            "        [ 0.1265],\n",
            "        [-0.0821],\n",
            "        [ 2.2815],\n",
            "        [ 1.8766],\n",
            "        [ 0.1197],\n",
            "        [ 0.1757],\n",
            "        [ 2.7632],\n",
            "        [ 0.1140],\n",
            "        [-0.0111],\n",
            "        [ 1.0655],\n",
            "        [ 2.3273]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2192, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.9378],\n",
            "        [ 2.0023],\n",
            "        [-0.0368],\n",
            "        [ 2.0507],\n",
            "        [ 1.3217],\n",
            "        [ 2.1613],\n",
            "        [ 1.4568],\n",
            "        [ 2.4979],\n",
            "        [ 0.0648],\n",
            "        [ 0.7474],\n",
            "        [ 2.5303],\n",
            "        [ 2.2868],\n",
            "        [ 0.3607],\n",
            "        [ 2.5899],\n",
            "        [ 2.4469],\n",
            "        [ 1.0934],\n",
            "        [ 0.1234],\n",
            "        [ 1.9164],\n",
            "        [ 1.8677],\n",
            "        [ 2.3161],\n",
            "        [ 2.2935],\n",
            "        [-0.0645],\n",
            "        [-0.0499],\n",
            "        [ 0.1399],\n",
            "        [ 1.4728],\n",
            "        [ 0.0712],\n",
            "        [ 0.3844],\n",
            "        [ 1.8081],\n",
            "        [ 1.4449],\n",
            "        [ 0.1574],\n",
            "        [ 0.1503],\n",
            "        [ 2.1749]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1563, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.2665],\n",
            "        [ 2.6783],\n",
            "        [ 0.2025],\n",
            "        [ 0.1800],\n",
            "        [ 1.9382],\n",
            "        [ 0.1337],\n",
            "        [ 0.2309],\n",
            "        [ 2.2075],\n",
            "        [ 0.1450],\n",
            "        [ 2.2098],\n",
            "        [ 0.0666],\n",
            "        [ 1.9316],\n",
            "        [ 1.5192],\n",
            "        [ 2.4218],\n",
            "        [ 0.7866],\n",
            "        [ 2.2493],\n",
            "        [ 0.2021],\n",
            "        [ 0.1863],\n",
            "        [ 0.8443],\n",
            "        [ 2.4714],\n",
            "        [ 0.1459],\n",
            "        [ 1.9478],\n",
            "        [ 2.4632],\n",
            "        [ 2.2083],\n",
            "        [ 2.1009],\n",
            "        [ 1.6743],\n",
            "        [-0.0101],\n",
            "        [ 0.7139],\n",
            "        [ 0.1366],\n",
            "        [ 0.1364],\n",
            "        [ 2.2686],\n",
            "        [ 2.3387]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3423, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 2.1930],\n",
            "        [ 0.0935],\n",
            "        [ 2.2019],\n",
            "        [ 0.0179],\n",
            "        [ 2.2454],\n",
            "        [ 2.1110],\n",
            "        [ 0.1226],\n",
            "        [ 2.2731],\n",
            "        [ 1.8741],\n",
            "        [ 0.8556],\n",
            "        [-0.0714],\n",
            "        [ 0.3972],\n",
            "        [ 2.5709],\n",
            "        [ 0.0177],\n",
            "        [ 2.0506],\n",
            "        [ 2.4396],\n",
            "        [ 0.1143],\n",
            "        [ 0.2609],\n",
            "        [ 1.9892],\n",
            "        [ 2.0259],\n",
            "        [-0.2091],\n",
            "        [ 2.1586],\n",
            "        [ 0.2950],\n",
            "        [ 0.9560],\n",
            "        [ 2.0021],\n",
            "        [ 2.2377],\n",
            "        [ 2.0596],\n",
            "        [ 1.8528],\n",
            "        [ 0.7018],\n",
            "        [ 1.1620],\n",
            "        [ 0.0908],\n",
            "        [ 2.3745]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2090, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.7360],\n",
            "        [-0.0780],\n",
            "        [ 2.1668],\n",
            "        [ 2.1859],\n",
            "        [ 1.3872],\n",
            "        [ 0.2729],\n",
            "        [ 2.0537],\n",
            "        [ 0.9537],\n",
            "        [ 2.0737],\n",
            "        [ 1.9971],\n",
            "        [ 1.4878],\n",
            "        [ 2.1807],\n",
            "        [ 0.9633],\n",
            "        [ 1.2444],\n",
            "        [ 2.1831],\n",
            "        [ 0.1537],\n",
            "        [ 2.4175],\n",
            "        [ 2.0844],\n",
            "        [ 2.3433],\n",
            "        [ 0.1853],\n",
            "        [ 2.0677],\n",
            "        [ 2.0874],\n",
            "        [-0.2442],\n",
            "        [ 1.8816],\n",
            "        [-0.1412],\n",
            "        [ 2.2105],\n",
            "        [ 0.3254],\n",
            "        [ 2.1484],\n",
            "        [ 0.2560],\n",
            "        [ 1.2368],\n",
            "        [-0.1855],\n",
            "        [ 2.2861]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3724, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.1108],\n",
            "        [ 0.1067],\n",
            "        [ 2.0760],\n",
            "        [ 1.3925],\n",
            "        [ 1.9496],\n",
            "        [ 1.9583],\n",
            "        [ 1.5860],\n",
            "        [ 1.6164],\n",
            "        [ 1.5183],\n",
            "        [ 1.6142],\n",
            "        [ 2.2183],\n",
            "        [ 2.8274],\n",
            "        [ 2.0287],\n",
            "        [ 0.2474],\n",
            "        [ 1.8828],\n",
            "        [ 0.1285],\n",
            "        [ 1.1322],\n",
            "        [ 0.6265],\n",
            "        [ 0.8804],\n",
            "        [ 0.6521],\n",
            "        [-0.0563],\n",
            "        [ 0.8750],\n",
            "        [ 2.1836],\n",
            "        [ 2.2974],\n",
            "        [ 2.6514],\n",
            "        [-0.1769],\n",
            "        [ 0.1067],\n",
            "        [ 0.5966],\n",
            "        [ 1.7252],\n",
            "        [ 0.8978],\n",
            "        [ 0.6731],\n",
            "        [ 2.1509]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3232, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.5996],\n",
            "        [ 0.2966],\n",
            "        [ 2.0054],\n",
            "        [ 2.1506],\n",
            "        [ 2.1505],\n",
            "        [ 1.9238],\n",
            "        [ 0.1338],\n",
            "        [ 2.1904],\n",
            "        [ 0.1705],\n",
            "        [ 1.9201],\n",
            "        [ 1.1603],\n",
            "        [ 1.7592],\n",
            "        [ 0.0552],\n",
            "        [ 1.8502],\n",
            "        [ 2.6268],\n",
            "        [ 0.0578],\n",
            "        [-0.0450],\n",
            "        [-0.0426],\n",
            "        [ 2.3106],\n",
            "        [ 0.0147],\n",
            "        [ 0.3239],\n",
            "        [ 1.9569],\n",
            "        [ 0.1673],\n",
            "        [ 0.1332],\n",
            "        [ 0.1512],\n",
            "        [ 1.7216],\n",
            "        [ 0.1179],\n",
            "        [-0.1544],\n",
            "        [ 2.2432],\n",
            "        [-0.2023],\n",
            "        [ 2.2200],\n",
            "        [ 0.9167]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3909, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.8227],\n",
            "        [ 2.0868],\n",
            "        [ 0.2052],\n",
            "        [ 1.9323],\n",
            "        [-0.0634],\n",
            "        [ 0.5118],\n",
            "        [ 1.8108],\n",
            "        [ 0.3726],\n",
            "        [ 1.8646],\n",
            "        [ 0.1901],\n",
            "        [ 0.0232],\n",
            "        [ 0.7963],\n",
            "        [ 2.0760],\n",
            "        [ 2.0157],\n",
            "        [-0.0277],\n",
            "        [ 0.2194],\n",
            "        [ 0.0452],\n",
            "        [ 0.7483],\n",
            "        [ 2.6048],\n",
            "        [ 1.3600],\n",
            "        [ 2.3580],\n",
            "        [ 1.8026],\n",
            "        [ 1.0275],\n",
            "        [ 1.7754],\n",
            "        [ 0.2881],\n",
            "        [ 2.5449],\n",
            "        [ 0.0704],\n",
            "        [ 0.0412],\n",
            "        [ 0.1847],\n",
            "        [ 0.0954],\n",
            "        [ 1.6972],\n",
            "        [ 0.8420]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.0146, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[-0.1209]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took:  30.716008\n",
            "\n",
            "Running Validation...\n",
            "b_input_ids:  tensor([[  101,  2330,  2689,  ...,     0,     0,     0],\n",
            "        [  101, 12210,  4471,  ...,     0,     0,     0],\n",
            "        [  101, 11562,  5587,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  4653,  2035,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  2291,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  2291,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [0.2830782  0.79667187 2.0943475  1.9812403  2.0108788  2.0396802\n",
            " 0.03601732 0.30100745 2.1842284  0.17417051 0.1463056  0.12219031\n",
            " 2.0943475  1.7727947  2.023845   2.1768315  2.2716746  0.10518025\n",
            " 0.03874255 2.3336625  0.96464664 2.0747619  0.13611867 0.06423549\n",
            " 0.04309062 0.7470577  1.0700482  0.08786355 1.276789   2.090151\n",
            " 0.60867906 0.73073244]\n",
            "label_ids [0. 1. 2. 2. 2. 2. 0. 4. 2. 0. 0. 0. 1. 2. 2. 1. 2. 0. 0. 2. 1. 2. 0. 0.\n",
            " 0. 1. 1. 0. 1. 2. 1. 1.]\n",
            "Mean absolute error : 0.31485653028357774\n",
            "b_input_ids:  tensor([[  101,  4607,  7514,  ...,     0,     0,     0],\n",
            "        [  101, 11562,  7929,  ...,     0,     0,     0],\n",
            "        [  101,  3116,  2038,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2291,  4618,  ...,     0,     0,     0],\n",
            "        [  101,  5310,  2064,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  8619,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [0.05507501 0.7470577  1.8845794  0.03985042 2.1611526  0.36152047\n",
            " 2.0926564  0.0124564  1.2748566  0.08087088 0.05133529 0.03396652\n",
            " 0.05556913 0.07394831 2.1521204  1.7727947  0.79667187 1.6910406\n",
            " 0.77742666 0.8130205  0.96688485 2.2909608  2.0209272  0.79667187\n",
            " 2.1635463  3.048002   2.3082333  0.03811207 0.6921002  1.9214854\n",
            " 0.10291089 0.19525969]\n",
            "label_ids [0. 1. 2. 0. 2. 0. 3. 0. 0. 0. 0. 0. 0. 0. 2. 2. 1. 2. 2. 2. 1. 2. 2. 1.\n",
            " 2. 6. 2. 0. 1. 2. 0. 0.]\n",
            "Mean absolute error : 0.35850874715833925\n",
            "b_input_ids:  tensor([[  101, 11562,  3972,  ...,     0,     0,     0],\n",
            "        [  101, 12210,  4471,  ...,     0,     0,     0],\n",
            "        [  101,  3967,  2052,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101, 11562,  7929,  ...,     0,     0,     0],\n",
            "        [  101,  2492,  3942,  ...,     0,     0,     0],\n",
            "        [  101,  2291,  4618,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [0.9984722  0.79667187 0.26011488 2.1174946  1.3226109  0.65501237\n",
            " 0.04358087 1.0702698  0.79667187 0.02661884 2.3297284  0.10291089\n",
            " 0.23598169 0.04202346 0.06753831 0.04049607 2.033295   2.0703974\n",
            " 2.0502617  0.06826974 1.0008165  0.20145904 1.030171   0.2981548\n",
            " 0.09665976 2.2969384  1.7905109  0.0224342  1.6240239  0.7470577\n",
            " 2.161866   1.9360385 ]\n",
            "label_ids [1. 1. 0. 2. 0. 0. 0. 1. 0. 0. 2. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 2. 0. 0.\n",
            " 0. 2. 2. 0. 2. 1. 2. 2.]\n",
            "Mean absolute error : 0.31706509937066585\n",
            "b_input_ids:  tensor([[  101,  8013,  4978,  ...,     0,     0,     0],\n",
            "        [  101,  8619, 11562,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  2291,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101, 11562,  3828,  ...,     0,     0,     0],\n",
            "        [  101,  2291, 11204,  ...,     0,     0,     0],\n",
            "        [  101,  8619,  2064,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [0.23817684 2.1377366  2.0011423  0.27342442 0.13175157 1.9307516\n",
            " 1.7727947  1.0501425  2.0943475  1.9263365  0.83063847 0.79667187\n",
            " 0.35285026 0.22279473 0.7142472  0.23939182 0.93231565 0.05585298\n",
            " 2.2663093  0.29194677 1.9946318  1.9760673  0.07346579 0.04922474\n",
            " 0.07410046 2.1786063  1.3502773  0.06518953 0.11831339 1.7727947\n",
            " 1.1954136  0.07237186]\n",
            "label_ids [1. 2. 3. 0. 0. 2. 2. 2. 2. 2. 2. 1. 0. 0. 1. 1. 2. 2. 2. 0. 2. 2. 0. 0.\n",
            " 0. 2. 1. 0. 0. 2. 1. 0.]\n",
            "Mean absolute error : 0.3661302455002442\n",
            "b_input_ids:  tensor([[  101,  2291, 19818,  ...,     0,     0,     0],\n",
            "        [  101,  2048, 12020,  ...,     0,     0,     0],\n",
            "        [  101, 19413,  2497,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  5310,  2064,  ...,     0,     0,     0],\n",
            "        [  101,  2291,  4604,  ...,     0,     0,     0],\n",
            "        [  101,  5310, 29225,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [2.1474087  0.32709572 0.12318164 2.87863    0.09111905 0.04740215\n",
            " 1.0411676  0.31926975 0.9589143  2.1795218  1.9240108  0.6639113\n",
            " 1.7727947  2.1519773  2.5527623  0.21617769 0.03874255 0.73960257\n",
            " 2.139496   0.68476015 0.0321888  0.22694947 2.199751   0.05043846\n",
            " 2.1876178  2.0022314  1.9152452  0.21266674 1.7727947  0.07109053\n",
            " 2.2953641  0.53158754]\n",
            "label_ids [2. 2. 2. 3. 0. 0. 2. 0. 1. 2. 2. 0. 2. 2. 3. 0. 0. 1. 2. 0. 0. 0. 2. 2.\n",
            " 2. 2. 2. 0. 2. 0. 4. 2.]\n",
            "Mean absolute error : 0.4602716435911134\n",
            "b_input_ids:  tensor([[  101,  5310,  2064,  ...,     0,     0,     0],\n",
            "        [  101,  5310, 29225,  ...,     0,     0,     0],\n",
            "        [  101,  2291, 19818,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101, 11562,  5587,  ...,     0,     0,     0],\n",
            "        [  101,  4607,  8094,  ...,     0,     0,     0],\n",
            "        [  101,  5310,  2064,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [0.5140258  0.3369889  2.1996915  0.06518953 1.0624025  0.07642546\n",
            " 1.4134153  2.0770586  1.2904086  0.99992156 2.161307   0.03879372\n",
            " 0.07809456 0.25372756 0.5083937  2.0976799  1.923535   0.11763299\n",
            " 2.1811335  0.23639087 0.13531767 2.0894344  2.0417926  0.7470577\n",
            " 0.12658884 1.15875    2.1190467  0.00438839 1.7727947  2.0943475\n",
            " 0.07149068 0.026341  ]\n",
            "label_ids [0. 0. 2. 0. 1. 0. 1. 2. 1. 1. 2. 0. 0. 0. 0. 2. 2. 0. 2. 0. 0. 2. 2. 1.\n",
            " 0. 0. 2. 0. 3. 2. 1. 0.]\n",
            "Mean absolute error : 0.2496864855347667\n",
            "b_input_ids:  tensor([[  101,  4653,  2035,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  2291,  ...,     0,     0,     0],\n",
            "        [  101,  5310, 29225,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  8013,  8039,  ...,     0,     0,     0],\n",
            "        [  101,  2291, 19818,  ...,     0,     0,     0],\n",
            "        [  101,  2291, 15210,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [2.080024   1.6981199  0.5219788  0.45651388 0.30100745 0.15043472\n",
            " 0.79320264 0.30710533 0.09444914 0.7470577  0.15993614 1.981492\n",
            " 0.72125375 2.0217924  1.1026617  1.0111893  2.1381836  0.02188285\n",
            " 2.171651   0.3066515  0.04270204 2.3420513  0.7470577  0.05809914\n",
            " 0.13819797 1.0675676  0.30100745 2.0144124  2.0014384  0.03594158\n",
            " 2.2553685  2.1190467 ]\n",
            "label_ids [2. 2. 1. 0. 0. 0. 0. 1. 0. 1. 1. 2. 0. 2. 0. 1. 2. 0. 2. 0. 0. 2. 1. 0.\n",
            " 0. 0. 0. 2. 2. 0. 2. 2.]\n",
            "Mean absolute error : 0.2994994804612361\n",
            "b_input_ids:  tensor([[  101,  5310, 29225,  ...,     0,     0,     0],\n",
            "        [  101,  8619,  6039,  ...,     0,     0,     0],\n",
            "        [  101, 11562,  3828,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2291, 15210,  ...,     0,     0,     0],\n",
            "        [  101,  5310, 12453,  ...,     0,     0,     0],\n",
            "        [  101,  5402,  2038,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [0.4415261  0.13462435 1.7727947  1.7727947  0.15735516 0.01619942\n",
            " 0.05159061 0.6698606  0.0632688  2.2331653  0.48053154 0.02018136\n",
            " 2.3492012  1.8366184  0.08420803 0.80388737 0.03059953 0.04287149\n",
            " 1.0539653  0.04658106 0.27572334 0.551211   2.549756   0.7027281\n",
            " 3.1332762  0.1903121  0.06365708 0.1594166  0.2737896  2.1190467\n",
            " 0.4414286  1.9941055 ]\n",
            "label_ids [0. 0. 2. 2. 0. 0. 0. 3. 0. 2. 0. 0. 3. 3. 0. 0. 0. 0. 1. 1. 0. 1. 3. 1.\n",
            " 3. 0. 0. 0. 0. 2. 0. 2.]\n",
            "Mean absolute error : 0.3445304125198163\n",
            "b_input_ids:  tensor([[  101,  8619,  6039,  1996,  4746,  2224,  2005,  3784,  2492,  2005,\n",
            "          1996,  9459,  3639,  1012,   102,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  1996,  5310, 15867,  1996,  1000, 12210,  1000,  5724,  1012,\n",
            "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101, 11562, 10086,  6462,  2005,  3563, 11412,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101, 11562,  7929,  6462,   102,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  2291,  4618, 18584,  3076,  2592,  2013,  7809,   102,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
            "       device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]], device='cuda:0')\n",
            "logits:  [0.07640311 0.07781745 0.94930553 0.74705756 1.9152453 ]\n",
            "label_ids [0. 1. 0. 1. 2.]\n",
            "Mean absolute error : 0.45711767077445986\n",
            "  Accuracy: 0.35\n",
            "  Validation Loss: 0.00\n",
            "  Validation took:  1.413762\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2439, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.4433],\n",
            "        [ 1.2211],\n",
            "        [ 2.6376],\n",
            "        [ 0.3701],\n",
            "        [ 1.7515],\n",
            "        [ 0.0537],\n",
            "        [ 0.3001],\n",
            "        [ 0.3255],\n",
            "        [-0.0282],\n",
            "        [ 0.5405],\n",
            "        [ 0.2643],\n",
            "        [ 0.8632],\n",
            "        [ 2.1795],\n",
            "        [ 1.8223],\n",
            "        [ 0.0428],\n",
            "        [ 2.2065],\n",
            "        [ 1.0090],\n",
            "        [ 0.2413],\n",
            "        [ 0.7383],\n",
            "        [ 2.0960],\n",
            "        [ 0.2661],\n",
            "        [ 1.3327],\n",
            "        [ 2.2652],\n",
            "        [ 2.3472],\n",
            "        [ 2.7175],\n",
            "        [ 0.6387],\n",
            "        [ 0.6200],\n",
            "        [ 1.8917],\n",
            "        [ 1.9005],\n",
            "        [ 0.0690],\n",
            "        [ 0.3917],\n",
            "        [ 2.0239]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1692, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 2.1076],\n",
            "        [ 1.7102],\n",
            "        [ 0.1413],\n",
            "        [-0.0528],\n",
            "        [ 2.0785],\n",
            "        [ 0.8547],\n",
            "        [-0.2545],\n",
            "        [ 2.0222],\n",
            "        [ 1.1978],\n",
            "        [ 0.1420],\n",
            "        [ 2.2641],\n",
            "        [ 0.2785],\n",
            "        [ 0.0799],\n",
            "        [ 0.3345],\n",
            "        [ 0.8509],\n",
            "        [ 0.7163],\n",
            "        [ 1.4903],\n",
            "        [ 0.3104],\n",
            "        [ 0.1276],\n",
            "        [ 0.4631],\n",
            "        [ 1.3605],\n",
            "        [ 2.3254],\n",
            "        [ 0.1103],\n",
            "        [ 1.7393],\n",
            "        [ 1.8218],\n",
            "        [ 3.3110],\n",
            "        [ 0.1979],\n",
            "        [ 0.3655],\n",
            "        [ 0.4893],\n",
            "        [ 1.8430],\n",
            "        [ 1.3757],\n",
            "        [ 2.1348]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.6601, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.2527],\n",
            "        [ 0.9713],\n",
            "        [ 2.1204],\n",
            "        [ 1.9523],\n",
            "        [ 2.3328],\n",
            "        [ 1.5671],\n",
            "        [ 0.0885],\n",
            "        [ 0.2338],\n",
            "        [ 2.1637],\n",
            "        [ 0.1432],\n",
            "        [ 2.4460],\n",
            "        [ 0.1748],\n",
            "        [ 2.0117],\n",
            "        [ 0.9981],\n",
            "        [ 1.2154],\n",
            "        [ 0.4327],\n",
            "        [ 2.4699],\n",
            "        [ 1.1009],\n",
            "        [ 2.7388],\n",
            "        [ 0.0689],\n",
            "        [-0.0519],\n",
            "        [ 2.1109],\n",
            "        [ 0.9215],\n",
            "        [ 1.7868],\n",
            "        [ 2.1899],\n",
            "        [ 1.8783],\n",
            "        [ 0.1005],\n",
            "        [ 0.5739],\n",
            "        [ 1.6851],\n",
            "        [ 0.7732],\n",
            "        [ 2.0636],\n",
            "        [ 0.0697]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2038, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.0407],\n",
            "        [ 1.9221],\n",
            "        [ 1.8335],\n",
            "        [ 2.4492],\n",
            "        [ 0.7301],\n",
            "        [ 0.4512],\n",
            "        [ 1.9881],\n",
            "        [ 0.9042],\n",
            "        [ 0.6038],\n",
            "        [ 3.2223],\n",
            "        [ 2.1159],\n",
            "        [ 0.4811],\n",
            "        [ 2.0969],\n",
            "        [ 2.2293],\n",
            "        [ 1.8538],\n",
            "        [ 2.1177],\n",
            "        [ 1.9850],\n",
            "        [ 2.3113],\n",
            "        [ 2.3145],\n",
            "        [ 1.5511],\n",
            "        [ 0.2862],\n",
            "        [ 2.0014],\n",
            "        [-0.2205],\n",
            "        [ 0.2093],\n",
            "        [ 0.7116],\n",
            "        [ 0.3179],\n",
            "        [ 2.0958],\n",
            "        [ 2.1617],\n",
            "        [ 2.8013],\n",
            "        [ 0.8782],\n",
            "        [ 0.3786],\n",
            "        [ 0.3424]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1395, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.2445],\n",
            "        [ 0.5553],\n",
            "        [ 2.3635],\n",
            "        [ 1.7899],\n",
            "        [ 2.0759],\n",
            "        [ 0.8741],\n",
            "        [ 1.1482],\n",
            "        [ 2.2043],\n",
            "        [ 2.2063],\n",
            "        [ 2.1031],\n",
            "        [-0.0218],\n",
            "        [ 2.7343],\n",
            "        [ 2.2563],\n",
            "        [ 0.2475],\n",
            "        [ 0.7006],\n",
            "        [ 1.8087],\n",
            "        [ 1.3731],\n",
            "        [ 0.2641],\n",
            "        [ 0.3619],\n",
            "        [ 0.4228],\n",
            "        [ 0.8038],\n",
            "        [ 2.1583],\n",
            "        [ 2.3620],\n",
            "        [ 0.7260],\n",
            "        [ 2.2478],\n",
            "        [ 0.2393],\n",
            "        [ 0.0897],\n",
            "        [ 1.3240],\n",
            "        [ 2.1757],\n",
            "        [ 1.2552],\n",
            "        [ 1.9571],\n",
            "        [ 0.2331]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2175, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[0.2644],\n",
            "        [1.3157],\n",
            "        [0.1630],\n",
            "        [0.5227],\n",
            "        [2.4351],\n",
            "        [0.3797],\n",
            "        [0.5148],\n",
            "        [0.3082],\n",
            "        [0.9865],\n",
            "        [0.1441],\n",
            "        [0.7586],\n",
            "        [0.9229],\n",
            "        [0.2332],\n",
            "        [1.9051],\n",
            "        [1.7492],\n",
            "        [2.0824],\n",
            "        [0.2555],\n",
            "        [2.1249],\n",
            "        [1.2117],\n",
            "        [2.1467],\n",
            "        [1.6311],\n",
            "        [2.1950],\n",
            "        [2.0925],\n",
            "        [1.0745],\n",
            "        [0.8227],\n",
            "        [0.1528],\n",
            "        [0.4886],\n",
            "        [0.9284],\n",
            "        [0.8514],\n",
            "        [0.8435],\n",
            "        [0.2307],\n",
            "        [1.9632]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.4385, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.2293],\n",
            "        [ 1.9442],\n",
            "        [ 2.1920],\n",
            "        [ 0.5147],\n",
            "        [ 2.1116],\n",
            "        [ 1.3781],\n",
            "        [ 0.8149],\n",
            "        [ 2.0099],\n",
            "        [ 0.2370],\n",
            "        [ 2.0681],\n",
            "        [ 1.5900],\n",
            "        [ 0.8087],\n",
            "        [ 0.2264],\n",
            "        [ 0.8599],\n",
            "        [ 2.9501],\n",
            "        [ 0.3342],\n",
            "        [-0.0736],\n",
            "        [ 0.1620],\n",
            "        [ 2.3618],\n",
            "        [ 2.0720],\n",
            "        [ 2.2860],\n",
            "        [ 2.3042],\n",
            "        [ 1.5156],\n",
            "        [ 0.4136],\n",
            "        [ 1.5672],\n",
            "        [ 1.9518],\n",
            "        [ 2.6256],\n",
            "        [ 0.9240],\n",
            "        [ 0.0172],\n",
            "        [ 0.3252],\n",
            "        [ 0.1355],\n",
            "        [ 0.3893]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.0993, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[1.4919],\n",
            "        [2.3052],\n",
            "        [0.1100],\n",
            "        [0.8778],\n",
            "        [2.2205],\n",
            "        [2.3055],\n",
            "        [1.8480],\n",
            "        [0.3641],\n",
            "        [2.2561],\n",
            "        [0.9280],\n",
            "        [0.1454],\n",
            "        [0.1864],\n",
            "        [0.2793],\n",
            "        [0.8957],\n",
            "        [0.2058],\n",
            "        [2.1102],\n",
            "        [0.3777],\n",
            "        [1.2357],\n",
            "        [1.0618],\n",
            "        [0.5592],\n",
            "        [1.6012],\n",
            "        [0.1502],\n",
            "        [2.2452],\n",
            "        [0.1686],\n",
            "        [0.2738],\n",
            "        [0.6857],\n",
            "        [0.3615],\n",
            "        [0.7079],\n",
            "        [0.0918],\n",
            "        [1.7691],\n",
            "        [2.1044],\n",
            "        [1.6196]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.0876, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 2.3580],\n",
            "        [ 2.3064],\n",
            "        [ 0.6324],\n",
            "        [ 2.3545],\n",
            "        [ 0.4311],\n",
            "        [ 2.2969],\n",
            "        [ 1.7698],\n",
            "        [ 1.9753],\n",
            "        [ 0.1955],\n",
            "        [ 0.2022],\n",
            "        [ 1.1680],\n",
            "        [ 1.7632],\n",
            "        [ 0.4229],\n",
            "        [ 1.3084],\n",
            "        [ 1.8594],\n",
            "        [ 0.3550],\n",
            "        [ 2.0937],\n",
            "        [-0.1999],\n",
            "        [ 2.1707],\n",
            "        [ 0.1089],\n",
            "        [ 0.9848],\n",
            "        [ 0.6514],\n",
            "        [ 0.3820],\n",
            "        [ 0.5305],\n",
            "        [ 2.0857],\n",
            "        [ 2.4896],\n",
            "        [ 1.0225],\n",
            "        [ 1.7525],\n",
            "        [ 2.0332],\n",
            "        [ 0.9551],\n",
            "        [ 0.0864],\n",
            "        [ 1.9008]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1271, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.2413],\n",
            "        [ 1.0157],\n",
            "        [ 0.5032],\n",
            "        [ 1.8686],\n",
            "        [-0.0373],\n",
            "        [ 2.8354],\n",
            "        [ 2.1544],\n",
            "        [ 2.1076],\n",
            "        [ 2.2630],\n",
            "        [ 0.2475],\n",
            "        [-0.0076],\n",
            "        [ 0.8931],\n",
            "        [ 0.8261],\n",
            "        [ 0.4462],\n",
            "        [-0.0418],\n",
            "        [ 0.2607],\n",
            "        [-0.2315],\n",
            "        [ 0.6318],\n",
            "        [ 2.0507],\n",
            "        [ 2.2274],\n",
            "        [ 1.7587],\n",
            "        [ 0.2958],\n",
            "        [ 0.9410],\n",
            "        [ 1.3495],\n",
            "        [-0.1945],\n",
            "        [ 1.4643],\n",
            "        [ 0.1001],\n",
            "        [ 2.3966],\n",
            "        [ 0.0379],\n",
            "        [-0.0691],\n",
            "        [ 2.0344],\n",
            "        [ 0.1521]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1370, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.0668],\n",
            "        [ 0.9466],\n",
            "        [ 0.6543],\n",
            "        [ 0.3284],\n",
            "        [ 2.1811],\n",
            "        [ 2.1240],\n",
            "        [ 1.6861],\n",
            "        [ 1.1449],\n",
            "        [ 2.2277],\n",
            "        [ 2.2326],\n",
            "        [ 2.1094],\n",
            "        [ 0.1551],\n",
            "        [ 2.8727],\n",
            "        [ 1.8025],\n",
            "        [ 0.0155],\n",
            "        [-0.1667],\n",
            "        [ 1.7213],\n",
            "        [ 2.0549],\n",
            "        [ 0.0274],\n",
            "        [ 2.0073],\n",
            "        [ 1.9543],\n",
            "        [ 2.0675],\n",
            "        [ 0.1646],\n",
            "        [ 0.5552],\n",
            "        [ 1.8977],\n",
            "        [ 2.2325],\n",
            "        [ 0.1763],\n",
            "        [ 0.8146],\n",
            "        [ 0.3516],\n",
            "        [ 0.6614],\n",
            "        [ 0.1502],\n",
            "        [ 1.8792]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1904, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.1308],\n",
            "        [ 1.7868],\n",
            "        [ 0.9520],\n",
            "        [-0.1744],\n",
            "        [ 1.0436],\n",
            "        [ 0.6701],\n",
            "        [-0.0131],\n",
            "        [ 1.4477],\n",
            "        [ 1.0266],\n",
            "        [ 0.3627],\n",
            "        [ 0.9507],\n",
            "        [ 0.4535],\n",
            "        [ 0.9615],\n",
            "        [ 0.1906],\n",
            "        [ 0.0275],\n",
            "        [ 0.6234],\n",
            "        [ 0.0640],\n",
            "        [ 1.4305],\n",
            "        [ 2.0068],\n",
            "        [ 2.3266],\n",
            "        [ 0.8654],\n",
            "        [-0.0352],\n",
            "        [ 2.0666],\n",
            "        [ 0.7823],\n",
            "        [ 0.2073],\n",
            "        [ 1.9464],\n",
            "        [ 2.5483],\n",
            "        [ 0.5111],\n",
            "        [ 2.2722],\n",
            "        [ 0.0227],\n",
            "        [ 1.3103],\n",
            "        [ 2.0016]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3601, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[-0.0101],\n",
            "        [ 0.0801],\n",
            "        [ 2.1681],\n",
            "        [-0.0220],\n",
            "        [ 0.7284],\n",
            "        [ 0.1386],\n",
            "        [ 1.8792],\n",
            "        [ 0.9216],\n",
            "        [ 1.2217],\n",
            "        [ 0.0288],\n",
            "        [ 2.4094],\n",
            "        [ 0.7675],\n",
            "        [ 0.7163],\n",
            "        [ 0.9206],\n",
            "        [ 0.0724],\n",
            "        [ 2.0329],\n",
            "        [-0.2062],\n",
            "        [ 0.6883],\n",
            "        [ 1.0077],\n",
            "        [-0.1256],\n",
            "        [ 0.0988],\n",
            "        [ 1.9489],\n",
            "        [ 1.7335],\n",
            "        [ 1.1007],\n",
            "        [ 1.0188],\n",
            "        [ 1.1584],\n",
            "        [ 1.9377],\n",
            "        [ 0.1595],\n",
            "        [ 1.6515],\n",
            "        [ 2.0068],\n",
            "        [ 3.2940],\n",
            "        [-0.1918]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3148, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 2.5233],\n",
            "        [-0.1436],\n",
            "        [-0.1178],\n",
            "        [ 1.2328],\n",
            "        [ 1.7861],\n",
            "        [ 2.0218],\n",
            "        [ 3.4528],\n",
            "        [ 0.4383],\n",
            "        [ 2.2207],\n",
            "        [-0.2888],\n",
            "        [ 1.7838],\n",
            "        [-0.0292],\n",
            "        [ 0.7018],\n",
            "        [ 0.1278],\n",
            "        [-0.3186],\n",
            "        [ 2.1313],\n",
            "        [ 0.6844],\n",
            "        [ 0.8306],\n",
            "        [ 0.6784],\n",
            "        [ 2.3373],\n",
            "        [-0.0646],\n",
            "        [ 0.0844],\n",
            "        [ 1.9397],\n",
            "        [ 3.0376],\n",
            "        [ 0.1411],\n",
            "        [ 2.0548],\n",
            "        [-0.0991],\n",
            "        [ 0.8853],\n",
            "        [ 2.1944],\n",
            "        [ 0.2233],\n",
            "        [ 2.5962],\n",
            "        [ 0.7442]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2144, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[-0.0349],\n",
            "        [ 2.2694],\n",
            "        [ 1.9880],\n",
            "        [-0.0346],\n",
            "        [ 2.0481],\n",
            "        [-0.0093],\n",
            "        [ 0.2425],\n",
            "        [-0.3169],\n",
            "        [ 1.1695],\n",
            "        [ 1.5349],\n",
            "        [ 0.7032],\n",
            "        [ 1.4628],\n",
            "        [-0.1384],\n",
            "        [ 2.0571],\n",
            "        [ 1.9905],\n",
            "        [ 1.8467],\n",
            "        [-0.1466],\n",
            "        [ 2.0893],\n",
            "        [ 1.9850],\n",
            "        [ 0.2133],\n",
            "        [ 2.0086],\n",
            "        [ 0.3783],\n",
            "        [ 0.9382],\n",
            "        [ 1.8276],\n",
            "        [ 2.9120],\n",
            "        [ 0.0587],\n",
            "        [ 0.1292],\n",
            "        [ 0.9896],\n",
            "        [ 0.6570],\n",
            "        [-0.1276],\n",
            "        [ 0.6109],\n",
            "        [ 0.5281]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.0840, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.8684],\n",
            "        [ 0.7249],\n",
            "        [ 0.7969],\n",
            "        [ 1.7410],\n",
            "        [-0.1404],\n",
            "        [ 2.8390],\n",
            "        [ 0.8010],\n",
            "        [ 0.2398],\n",
            "        [ 1.7728],\n",
            "        [ 0.8993],\n",
            "        [ 2.4903],\n",
            "        [ 0.3328],\n",
            "        [ 1.9240],\n",
            "        [ 0.2459],\n",
            "        [ 2.0429],\n",
            "        [ 2.9219],\n",
            "        [ 1.7965],\n",
            "        [ 1.9875],\n",
            "        [ 0.3631],\n",
            "        [-0.0737],\n",
            "        [ 0.8962],\n",
            "        [ 2.0606],\n",
            "        [ 1.9795],\n",
            "        [ 0.2934],\n",
            "        [ 1.8723],\n",
            "        [ 0.6199],\n",
            "        [-0.1774],\n",
            "        [ 1.9065],\n",
            "        [ 2.1119],\n",
            "        [-0.0112],\n",
            "        [ 2.0986],\n",
            "        [ 1.3931]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.9283, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.0690],\n",
            "        [ 3.0143],\n",
            "        [ 3.2569],\n",
            "        [ 0.2021],\n",
            "        [ 2.1120],\n",
            "        [-0.1871],\n",
            "        [-0.2734],\n",
            "        [ 2.6525],\n",
            "        [ 0.8729],\n",
            "        [ 1.0455],\n",
            "        [-0.1270],\n",
            "        [ 2.0873],\n",
            "        [ 2.0515],\n",
            "        [ 1.8602],\n",
            "        [ 1.3981],\n",
            "        [ 3.1726],\n",
            "        [ 2.0478],\n",
            "        [ 2.6336],\n",
            "        [ 1.9581],\n",
            "        [ 2.1175],\n",
            "        [ 1.2826],\n",
            "        [-0.0742],\n",
            "        [ 2.5838],\n",
            "        [-0.2030],\n",
            "        [ 0.1205],\n",
            "        [-0.0385],\n",
            "        [ 0.6218],\n",
            "        [ 1.1461],\n",
            "        [ 0.0981],\n",
            "        [ 2.9196],\n",
            "        [ 1.7436],\n",
            "        [ 1.9971]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2400, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 2.1328],\n",
            "        [ 0.1954],\n",
            "        [ 1.0522],\n",
            "        [ 2.0891],\n",
            "        [-0.1112],\n",
            "        [-0.0857],\n",
            "        [ 2.2237],\n",
            "        [ 0.2706],\n",
            "        [ 2.1971],\n",
            "        [ 0.2476],\n",
            "        [ 0.5772],\n",
            "        [ 0.2276],\n",
            "        [-0.0507],\n",
            "        [ 1.9920],\n",
            "        [ 0.2007],\n",
            "        [ 2.2565],\n",
            "        [ 2.0617],\n",
            "        [ 0.1379],\n",
            "        [ 0.7256],\n",
            "        [ 0.1829],\n",
            "        [ 0.5442],\n",
            "        [ 0.8508],\n",
            "        [ 2.4257],\n",
            "        [ 1.9290],\n",
            "        [ 0.0967],\n",
            "        [ 0.9631],\n",
            "        [ 0.1422],\n",
            "        [ 0.9633],\n",
            "        [ 0.4363],\n",
            "        [ 0.9265],\n",
            "        [-0.1102],\n",
            "        [-0.1707]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1628, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.4568e-01],\n",
            "        [ 1.4373e-01],\n",
            "        [-8.5552e-02],\n",
            "        [ 1.2827e-01],\n",
            "        [ 1.0298e+00],\n",
            "        [ 2.3315e+00],\n",
            "        [ 9.7827e-02],\n",
            "        [ 3.3203e-01],\n",
            "        [-9.9768e-02],\n",
            "        [ 2.3281e+00],\n",
            "        [ 6.2471e-02],\n",
            "        [ 2.2765e+00],\n",
            "        [-7.0251e-03],\n",
            "        [ 2.0115e+00],\n",
            "        [-2.2746e-04],\n",
            "        [-2.4721e-03],\n",
            "        [ 1.1149e+00],\n",
            "        [ 3.6695e-01],\n",
            "        [-1.9470e-01],\n",
            "        [ 2.0981e+00],\n",
            "        [-1.0509e-02],\n",
            "        [ 3.2751e+00],\n",
            "        [-2.6265e-02],\n",
            "        [ 1.3821e+00],\n",
            "        [ 1.8344e+00],\n",
            "        [ 2.2712e+00],\n",
            "        [ 1.3598e-03],\n",
            "        [ 6.2249e-01],\n",
            "        [-1.2475e-01],\n",
            "        [ 1.8752e+00],\n",
            "        [ 1.7739e+00],\n",
            "        [ 2.1858e+00]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1499, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.4619],\n",
            "        [-0.0339],\n",
            "        [ 0.4175],\n",
            "        [ 3.7748],\n",
            "        [ 2.3929],\n",
            "        [ 2.0228],\n",
            "        [ 0.6808],\n",
            "        [ 1.9603],\n",
            "        [ 1.0160],\n",
            "        [ 1.0531],\n",
            "        [ 1.9433],\n",
            "        [ 0.3488],\n",
            "        [ 1.4950],\n",
            "        [ 2.2313],\n",
            "        [ 1.2240],\n",
            "        [-0.0771],\n",
            "        [ 2.3110],\n",
            "        [ 2.1057],\n",
            "        [ 0.9101],\n",
            "        [ 0.0743],\n",
            "        [-0.1084],\n",
            "        [ 2.1827],\n",
            "        [-0.1817],\n",
            "        [-0.1051],\n",
            "        [ 0.4174],\n",
            "        [ 1.1059],\n",
            "        [ 3.0203],\n",
            "        [-0.1114],\n",
            "        [ 0.1788],\n",
            "        [ 2.1160],\n",
            "        [ 1.1282],\n",
            "        [ 2.2772]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2358, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.9894],\n",
            "        [-0.0845],\n",
            "        [ 1.9973],\n",
            "        [ 2.1604],\n",
            "        [ 2.1196],\n",
            "        [ 2.3185],\n",
            "        [ 1.8871],\n",
            "        [ 0.1036],\n",
            "        [ 2.2918],\n",
            "        [ 1.0894],\n",
            "        [ 2.1770],\n",
            "        [ 1.8025],\n",
            "        [ 0.3297],\n",
            "        [ 0.9915],\n",
            "        [ 2.0574],\n",
            "        [-0.3330],\n",
            "        [ 2.0017],\n",
            "        [-0.0957],\n",
            "        [-0.0026],\n",
            "        [ 2.2433],\n",
            "        [-0.0480],\n",
            "        [ 0.2785],\n",
            "        [-0.2045],\n",
            "        [ 0.3184],\n",
            "        [ 1.7665],\n",
            "        [ 1.8662],\n",
            "        [ 1.8257],\n",
            "        [ 2.0743],\n",
            "        [-0.0557],\n",
            "        [-0.1584],\n",
            "        [ 0.5283],\n",
            "        [ 2.2111]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2243, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 2.1031],\n",
            "        [ 0.0630],\n",
            "        [ 1.8794],\n",
            "        [ 2.0723],\n",
            "        [-0.1607],\n",
            "        [ 1.5672],\n",
            "        [ 2.7594],\n",
            "        [ 0.0403],\n",
            "        [ 1.5440],\n",
            "        [ 0.4439],\n",
            "        [ 0.5311],\n",
            "        [ 2.0099],\n",
            "        [ 1.9945],\n",
            "        [ 3.5510],\n",
            "        [ 0.0207],\n",
            "        [ 0.2384],\n",
            "        [ 1.9631],\n",
            "        [ 1.8796],\n",
            "        [ 2.1620],\n",
            "        [ 2.4614],\n",
            "        [ 2.1597],\n",
            "        [ 1.2450],\n",
            "        [ 1.2231],\n",
            "        [ 2.0211],\n",
            "        [ 0.1642],\n",
            "        [ 2.2049],\n",
            "        [ 0.2148],\n",
            "        [ 0.1154],\n",
            "        [ 0.9684],\n",
            "        [ 0.7696],\n",
            "        [-0.2200],\n",
            "        [ 2.3349]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.5162, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 2.7369],\n",
            "        [ 1.9887],\n",
            "        [ 1.7627],\n",
            "        [ 1.0867],\n",
            "        [ 1.9749],\n",
            "        [ 2.2537],\n",
            "        [ 0.1999],\n",
            "        [ 1.9694],\n",
            "        [ 0.0526],\n",
            "        [ 1.2503],\n",
            "        [ 3.7288],\n",
            "        [ 3.3978],\n",
            "        [ 0.4357],\n",
            "        [ 1.8720],\n",
            "        [ 0.5940],\n",
            "        [ 0.6288],\n",
            "        [ 2.1291],\n",
            "        [-0.0516],\n",
            "        [ 1.7696],\n",
            "        [ 0.0543],\n",
            "        [ 1.5857],\n",
            "        [ 1.7776],\n",
            "        [-0.0814],\n",
            "        [ 0.0423],\n",
            "        [ 0.2100],\n",
            "        [ 0.0880],\n",
            "        [ 1.8063],\n",
            "        [ 2.1365],\n",
            "        [-0.1417],\n",
            "        [ 0.5925],\n",
            "        [-0.1990],\n",
            "        [-0.0597]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1700, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.4340],\n",
            "        [ 1.9720],\n",
            "        [ 1.9657],\n",
            "        [ 0.0307],\n",
            "        [ 1.7371],\n",
            "        [ 2.0280],\n",
            "        [ 2.0172],\n",
            "        [ 1.8580],\n",
            "        [ 1.8577],\n",
            "        [ 1.9359],\n",
            "        [ 1.1098],\n",
            "        [ 0.5887],\n",
            "        [ 1.7749],\n",
            "        [ 1.8751],\n",
            "        [-0.2595],\n",
            "        [ 0.0404],\n",
            "        [ 0.1203],\n",
            "        [ 0.0356],\n",
            "        [ 0.0284],\n",
            "        [ 0.0660],\n",
            "        [ 0.4034],\n",
            "        [ 2.4405],\n",
            "        [ 2.0339],\n",
            "        [ 0.0586],\n",
            "        [ 2.0000],\n",
            "        [ 0.2741],\n",
            "        [ 1.6506],\n",
            "        [ 2.2992],\n",
            "        [-0.0047],\n",
            "        [ 0.4338],\n",
            "        [-0.1152],\n",
            "        [ 2.2809]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.0991, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.0255],\n",
            "        [ 2.0783],\n",
            "        [ 0.0346],\n",
            "        [ 0.1310],\n",
            "        [ 0.8283],\n",
            "        [ 1.6822],\n",
            "        [-0.0143],\n",
            "        [-0.1443],\n",
            "        [ 0.1287],\n",
            "        [ 0.6086],\n",
            "        [-0.2109],\n",
            "        [ 1.9851],\n",
            "        [-0.0479],\n",
            "        [ 0.3001],\n",
            "        [ 0.7890],\n",
            "        [ 1.6179],\n",
            "        [ 0.1175],\n",
            "        [ 0.0381],\n",
            "        [-0.0514],\n",
            "        [ 0.2199],\n",
            "        [ 0.0162],\n",
            "        [-0.1101],\n",
            "        [ 0.0369],\n",
            "        [ 0.9292],\n",
            "        [ 0.7096],\n",
            "        [ 2.3954],\n",
            "        [ 0.8214],\n",
            "        [ 1.8458],\n",
            "        [ 1.8102],\n",
            "        [ 3.1408],\n",
            "        [-0.0742],\n",
            "        [-0.0972]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1570, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.7445],\n",
            "        [ 1.7908],\n",
            "        [ 2.2454],\n",
            "        [-0.1790],\n",
            "        [ 0.2146],\n",
            "        [ 0.0529],\n",
            "        [ 1.3297],\n",
            "        [-0.1445],\n",
            "        [ 1.4561],\n",
            "        [ 0.1147],\n",
            "        [ 1.6945],\n",
            "        [-0.2232],\n",
            "        [-0.2621],\n",
            "        [ 0.1593],\n",
            "        [ 0.2397],\n",
            "        [-0.1448],\n",
            "        [ 0.4517],\n",
            "        [ 0.9064],\n",
            "        [ 1.0256],\n",
            "        [ 1.9548],\n",
            "        [-0.3325],\n",
            "        [-0.0303],\n",
            "        [-0.1367],\n",
            "        [ 2.0939],\n",
            "        [ 0.0949],\n",
            "        [ 0.4979],\n",
            "        [ 1.4425],\n",
            "        [ 1.6527],\n",
            "        [ 2.0563],\n",
            "        [ 2.1182],\n",
            "        [ 1.0634],\n",
            "        [ 0.0736]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3917, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.4716],\n",
            "        [-0.1580],\n",
            "        [ 1.2451],\n",
            "        [ 1.9641],\n",
            "        [ 2.2728],\n",
            "        [ 1.1685],\n",
            "        [-0.1984],\n",
            "        [-0.0573],\n",
            "        [-0.1872],\n",
            "        [ 1.7920],\n",
            "        [ 1.2422],\n",
            "        [ 0.0607],\n",
            "        [ 1.8908],\n",
            "        [ 1.8432],\n",
            "        [ 0.6415],\n",
            "        [-0.1587],\n",
            "        [ 0.8799],\n",
            "        [-0.0583],\n",
            "        [ 0.0954],\n",
            "        [ 0.4492],\n",
            "        [ 1.7006],\n",
            "        [ 0.0991],\n",
            "        [ 0.7431],\n",
            "        [ 1.7622],\n",
            "        [ 1.7037],\n",
            "        [ 0.2832],\n",
            "        [ 2.1817],\n",
            "        [-0.2087],\n",
            "        [ 1.8212],\n",
            "        [-0.0340],\n",
            "        [ 2.6809],\n",
            "        [ 0.1092]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2197, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.8266e+00],\n",
            "        [ 1.5924e+00],\n",
            "        [ 2.0871e-01],\n",
            "        [ 3.0983e-01],\n",
            "        [-7.7700e-02],\n",
            "        [ 1.1337e+00],\n",
            "        [ 1.7649e-01],\n",
            "        [ 2.7336e+00],\n",
            "        [ 1.9834e+00],\n",
            "        [ 1.9627e+00],\n",
            "        [ 1.1820e+00],\n",
            "        [ 1.1720e+00],\n",
            "        [ 1.4867e+00],\n",
            "        [-1.9202e-01],\n",
            "        [ 2.8974e-01],\n",
            "        [ 2.0291e+00],\n",
            "        [ 1.8350e+00],\n",
            "        [ 2.0722e+00],\n",
            "        [ 1.9470e+00],\n",
            "        [ 1.8561e-03],\n",
            "        [ 1.9533e+00],\n",
            "        [-1.8184e-01],\n",
            "        [ 6.3309e-01],\n",
            "        [ 2.6716e+00],\n",
            "        [ 2.1263e+00],\n",
            "        [-2.2272e-01],\n",
            "        [ 3.0426e+00],\n",
            "        [ 1.9130e+00],\n",
            "        [ 1.9561e+00],\n",
            "        [-1.4943e-01],\n",
            "        [ 2.1290e+00],\n",
            "        [ 9.8099e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1655, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.9787],\n",
            "        [ 0.2923],\n",
            "        [ 0.1969],\n",
            "        [ 2.2197],\n",
            "        [ 2.2497],\n",
            "        [ 1.7293],\n",
            "        [ 1.5763],\n",
            "        [ 0.4092],\n",
            "        [ 1.4016],\n",
            "        [ 3.1785],\n",
            "        [ 1.0417],\n",
            "        [ 0.9259],\n",
            "        [ 1.0912],\n",
            "        [ 1.1894],\n",
            "        [ 2.2119],\n",
            "        [-0.1007],\n",
            "        [-0.0605],\n",
            "        [ 0.0954],\n",
            "        [-0.0612],\n",
            "        [ 2.0816],\n",
            "        [ 2.1119],\n",
            "        [-0.0244],\n",
            "        [-0.0796],\n",
            "        [ 1.0583],\n",
            "        [-0.1251],\n",
            "        [ 0.3712],\n",
            "        [ 1.2371],\n",
            "        [ 0.1260],\n",
            "        [ 1.9287],\n",
            "        [ 1.7385],\n",
            "        [ 1.0589],\n",
            "        [ 0.7770]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2879, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 2.5130],\n",
            "        [ 2.0887],\n",
            "        [ 0.2641],\n",
            "        [ 0.3475],\n",
            "        [ 2.2315],\n",
            "        [-0.0912],\n",
            "        [ 2.6872],\n",
            "        [ 2.3654],\n",
            "        [-0.2836],\n",
            "        [-0.0515],\n",
            "        [ 0.4267],\n",
            "        [-0.0942],\n",
            "        [ 2.1451],\n",
            "        [ 2.0851],\n",
            "        [ 0.4238],\n",
            "        [ 0.1657],\n",
            "        [ 1.2521],\n",
            "        [ 0.0322],\n",
            "        [ 0.0678],\n",
            "        [ 0.2471],\n",
            "        [ 2.3028],\n",
            "        [ 2.1416],\n",
            "        [ 2.0078],\n",
            "        [ 0.1789],\n",
            "        [ 1.8453],\n",
            "        [ 0.0101],\n",
            "        [ 0.1114],\n",
            "        [ 1.5886],\n",
            "        [ 1.8001],\n",
            "        [ 0.9214],\n",
            "        [ 2.4994],\n",
            "        [ 0.7946]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1500, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.8221],\n",
            "        [ 1.5052],\n",
            "        [ 3.3744],\n",
            "        [ 0.8317],\n",
            "        [ 2.5775],\n",
            "        [ 1.7996],\n",
            "        [ 2.0437],\n",
            "        [ 0.2658],\n",
            "        [ 1.3187],\n",
            "        [-0.2955],\n",
            "        [ 2.1244],\n",
            "        [ 1.9871],\n",
            "        [ 0.5720],\n",
            "        [ 0.1453],\n",
            "        [ 1.2287],\n",
            "        [ 0.1455],\n",
            "        [ 1.3044],\n",
            "        [ 1.8224],\n",
            "        [ 1.2397],\n",
            "        [ 2.9811],\n",
            "        [-0.0227],\n",
            "        [ 1.2279],\n",
            "        [ 1.3519],\n",
            "        [ 2.2591],\n",
            "        [ 2.1256],\n",
            "        [ 0.8296],\n",
            "        [-0.0648],\n",
            "        [ 1.4692],\n",
            "        [ 0.0655],\n",
            "        [ 1.5047],\n",
            "        [ 0.2259],\n",
            "        [ 2.7287]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2232, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.0175],\n",
            "        [ 0.4268],\n",
            "        [ 0.7718],\n",
            "        [ 2.2712],\n",
            "        [ 3.1064],\n",
            "        [ 1.4969],\n",
            "        [ 1.1350],\n",
            "        [ 0.5629],\n",
            "        [ 1.3338],\n",
            "        [-0.1632],\n",
            "        [ 0.1705],\n",
            "        [ 2.0641],\n",
            "        [ 2.2434],\n",
            "        [ 0.8814],\n",
            "        [ 2.1026],\n",
            "        [ 1.9267],\n",
            "        [ 1.2819],\n",
            "        [ 2.3033],\n",
            "        [ 2.0399],\n",
            "        [ 0.1350],\n",
            "        [ 2.1324],\n",
            "        [ 2.4411],\n",
            "        [ 1.9992],\n",
            "        [ 2.1211],\n",
            "        [ 0.2998],\n",
            "        [-0.0061],\n",
            "        [ 0.2282],\n",
            "        [ 0.1818],\n",
            "        [ 0.1944],\n",
            "        [ 2.2257],\n",
            "        [ 2.1054],\n",
            "        [ 1.3468]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1421, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.2632],\n",
            "        [ 1.8648],\n",
            "        [ 2.5489],\n",
            "        [ 1.6349],\n",
            "        [ 2.1538],\n",
            "        [ 0.9086],\n",
            "        [ 0.5138],\n",
            "        [ 0.0453],\n",
            "        [ 2.1118],\n",
            "        [ 0.5532],\n",
            "        [ 0.7865],\n",
            "        [ 2.0884],\n",
            "        [ 0.7236],\n",
            "        [ 1.3718],\n",
            "        [ 0.5412],\n",
            "        [ 0.0416],\n",
            "        [ 1.0401],\n",
            "        [-0.0831],\n",
            "        [ 2.2807],\n",
            "        [ 1.2154],\n",
            "        [ 0.5310],\n",
            "        [ 0.3298],\n",
            "        [ 1.8468],\n",
            "        [ 0.0468],\n",
            "        [ 1.8330],\n",
            "        [-0.0326],\n",
            "        [ 0.1373],\n",
            "        [ 2.3195],\n",
            "        [ 0.7393],\n",
            "        [-0.0126],\n",
            "        [ 1.6449],\n",
            "        [ 1.8309]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1040, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.7599],\n",
            "        [ 1.4284],\n",
            "        [-0.0913],\n",
            "        [ 2.0801],\n",
            "        [ 0.8983],\n",
            "        [ 0.0919],\n",
            "        [-0.0525],\n",
            "        [ 0.1397],\n",
            "        [ 0.8452],\n",
            "        [ 0.2667],\n",
            "        [ 0.4494],\n",
            "        [ 1.7062],\n",
            "        [ 0.4763],\n",
            "        [ 0.4800],\n",
            "        [ 0.1309],\n",
            "        [ 2.1711],\n",
            "        [-0.0363],\n",
            "        [ 0.8936],\n",
            "        [ 2.0315],\n",
            "        [ 1.7129],\n",
            "        [ 1.8990],\n",
            "        [ 0.2155],\n",
            "        [ 2.1541],\n",
            "        [ 1.7681],\n",
            "        [ 2.0357],\n",
            "        [ 1.6993],\n",
            "        [ 0.1316],\n",
            "        [ 1.5814],\n",
            "        [ 2.1588],\n",
            "        [ 2.2065],\n",
            "        [ 0.2215],\n",
            "        [ 1.6222]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2044, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.4951],\n",
            "        [ 0.0942],\n",
            "        [ 1.4075],\n",
            "        [ 3.3433],\n",
            "        [ 1.7829],\n",
            "        [ 0.4931],\n",
            "        [ 1.8890],\n",
            "        [-0.0754],\n",
            "        [ 1.9289],\n",
            "        [ 0.2376],\n",
            "        [ 2.1349],\n",
            "        [ 1.7029],\n",
            "        [ 1.9649],\n",
            "        [ 2.8358],\n",
            "        [ 2.1107],\n",
            "        [ 3.3447],\n",
            "        [ 2.3829],\n",
            "        [ 2.6393],\n",
            "        [ 2.1833],\n",
            "        [ 0.3540],\n",
            "        [ 2.1828],\n",
            "        [ 0.2270],\n",
            "        [ 1.8219],\n",
            "        [ 0.3626],\n",
            "        [ 0.1342],\n",
            "        [ 1.6231],\n",
            "        [ 2.0085],\n",
            "        [ 1.0274],\n",
            "        [ 0.7890],\n",
            "        [ 0.2767],\n",
            "        [ 1.7303],\n",
            "        [ 0.2149]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1430, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.2717],\n",
            "        [ 2.0134],\n",
            "        [ 0.0046],\n",
            "        [ 2.1581],\n",
            "        [ 0.5820],\n",
            "        [-0.0896],\n",
            "        [ 0.0654],\n",
            "        [ 1.5434],\n",
            "        [ 0.2650],\n",
            "        [ 1.1444],\n",
            "        [-0.0190],\n",
            "        [ 0.2075],\n",
            "        [ 1.6686],\n",
            "        [ 0.7637],\n",
            "        [ 1.8673],\n",
            "        [ 0.6305],\n",
            "        [ 0.0855],\n",
            "        [ 1.8833],\n",
            "        [-0.0216],\n",
            "        [ 0.2039],\n",
            "        [ 1.7252],\n",
            "        [ 0.1518],\n",
            "        [ 0.0946],\n",
            "        [ 1.8577],\n",
            "        [ 1.1185],\n",
            "        [ 0.3051],\n",
            "        [-0.0024],\n",
            "        [ 1.8605],\n",
            "        [ 0.1242],\n",
            "        [ 1.9190],\n",
            "        [ 1.1259],\n",
            "        [ 0.1197]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2432, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.0559],\n",
            "        [ 0.4178],\n",
            "        [ 1.9367],\n",
            "        [ 0.1803],\n",
            "        [ 1.9879],\n",
            "        [ 2.7098],\n",
            "        [ 2.0577],\n",
            "        [ 0.0094],\n",
            "        [ 0.0056],\n",
            "        [ 0.9706],\n",
            "        [ 2.2663],\n",
            "        [ 0.8245],\n",
            "        [ 0.4890],\n",
            "        [ 0.0608],\n",
            "        [ 0.7284],\n",
            "        [ 1.8200],\n",
            "        [ 1.6207],\n",
            "        [-0.1499],\n",
            "        [ 1.7326],\n",
            "        [ 0.2979],\n",
            "        [ 0.7209],\n",
            "        [ 1.8351],\n",
            "        [ 1.6067],\n",
            "        [ 1.7033],\n",
            "        [ 0.8973],\n",
            "        [ 0.4062],\n",
            "        [ 1.3278],\n",
            "        [ 0.2131],\n",
            "        [ 1.2390],\n",
            "        [ 0.0976],\n",
            "        [ 1.8005],\n",
            "        [-0.0490]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.6123, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.6738],\n",
            "        [ 1.9925],\n",
            "        [ 1.8430],\n",
            "        [ 1.5939],\n",
            "        [-0.0137],\n",
            "        [ 0.8937],\n",
            "        [ 1.6616],\n",
            "        [ 2.3125],\n",
            "        [ 0.1844],\n",
            "        [-0.1774],\n",
            "        [ 1.1634],\n",
            "        [ 0.2192],\n",
            "        [ 0.0801],\n",
            "        [-0.2077],\n",
            "        [ 0.6723],\n",
            "        [ 1.6454],\n",
            "        [ 0.6238],\n",
            "        [ 0.0051],\n",
            "        [-0.0463],\n",
            "        [-0.2185],\n",
            "        [ 0.0794],\n",
            "        [ 0.3112],\n",
            "        [ 0.0176],\n",
            "        [ 0.2471],\n",
            "        [-0.0802],\n",
            "        [ 0.0595],\n",
            "        [ 2.1127],\n",
            "        [ 2.0181],\n",
            "        [ 2.9028],\n",
            "        [ 2.2610],\n",
            "        [ 0.1287],\n",
            "        [-0.0870]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.0836, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.6836],\n",
            "        [-0.0061],\n",
            "        [ 0.0562],\n",
            "        [ 1.6944],\n",
            "        [ 1.0834],\n",
            "        [ 0.0508],\n",
            "        [ 0.3040],\n",
            "        [-0.0559],\n",
            "        [ 0.7801],\n",
            "        [ 1.7183],\n",
            "        [ 0.1911],\n",
            "        [ 1.9574],\n",
            "        [ 0.6052],\n",
            "        [-0.1410],\n",
            "        [-0.1677],\n",
            "        [ 0.0905],\n",
            "        [-0.0877],\n",
            "        [ 1.8961],\n",
            "        [-0.0822],\n",
            "        [ 0.0049],\n",
            "        [ 1.8432],\n",
            "        [-0.1700],\n",
            "        [ 0.3482],\n",
            "        [ 0.4154],\n",
            "        [-0.0525],\n",
            "        [ 2.6603],\n",
            "        [ 1.8560],\n",
            "        [ 1.9216],\n",
            "        [ 0.8559],\n",
            "        [ 1.8865],\n",
            "        [ 0.8161],\n",
            "        [ 2.0257]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1613, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.0972],\n",
            "        [ 1.4834],\n",
            "        [-0.2808],\n",
            "        [ 0.0403],\n",
            "        [ 2.6950],\n",
            "        [ 0.2088],\n",
            "        [ 1.9168],\n",
            "        [ 1.0826],\n",
            "        [-0.1046],\n",
            "        [ 2.0835],\n",
            "        [ 0.1373],\n",
            "        [ 2.1688],\n",
            "        [-0.1535],\n",
            "        [ 1.1017],\n",
            "        [ 0.4202],\n",
            "        [ 2.1470],\n",
            "        [ 0.0759],\n",
            "        [ 0.6329],\n",
            "        [ 1.9629],\n",
            "        [ 2.4316],\n",
            "        [ 0.6985],\n",
            "        [-0.1240],\n",
            "        [ 2.0584],\n",
            "        [ 0.1745],\n",
            "        [ 1.9683],\n",
            "        [ 0.2077],\n",
            "        [ 2.4557],\n",
            "        [ 1.2607],\n",
            "        [-0.0962],\n",
            "        [ 2.1863],\n",
            "        [ 1.1341],\n",
            "        [ 1.0330]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "  Batch    40  of     66.    Elapsed: 19.043158.\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1067, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[-0.0912],\n",
            "        [ 0.9206],\n",
            "        [ 1.6038],\n",
            "        [ 1.0141],\n",
            "        [ 2.2980],\n",
            "        [ 0.1179],\n",
            "        [ 2.4325],\n",
            "        [ 1.8392],\n",
            "        [ 0.8802],\n",
            "        [-0.1135],\n",
            "        [ 0.3749],\n",
            "        [ 0.7133],\n",
            "        [ 0.0707],\n",
            "        [ 2.1615],\n",
            "        [ 0.9881],\n",
            "        [-0.0600],\n",
            "        [ 2.1396],\n",
            "        [ 2.0985],\n",
            "        [ 1.4449],\n",
            "        [ 0.1567],\n",
            "        [-0.2715],\n",
            "        [ 1.1756],\n",
            "        [ 1.9893],\n",
            "        [ 1.5425],\n",
            "        [ 1.8839],\n",
            "        [-0.0157],\n",
            "        [ 1.8843],\n",
            "        [ 2.3450],\n",
            "        [ 2.0685],\n",
            "        [ 1.6773],\n",
            "        [ 1.7864],\n",
            "        [ 2.2294]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2047, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.9710],\n",
            "        [ 2.1497],\n",
            "        [ 2.0394],\n",
            "        [ 2.5483],\n",
            "        [ 0.8405],\n",
            "        [ 1.1242],\n",
            "        [-0.3232],\n",
            "        [ 1.5575],\n",
            "        [ 0.7475],\n",
            "        [ 0.1876],\n",
            "        [ 0.0477],\n",
            "        [ 0.2397],\n",
            "        [ 0.0184],\n",
            "        [ 0.2515],\n",
            "        [ 0.0103],\n",
            "        [ 0.1171],\n",
            "        [ 1.9719],\n",
            "        [ 1.7139],\n",
            "        [ 1.7168],\n",
            "        [ 1.7923],\n",
            "        [ 1.9951],\n",
            "        [ 0.5041],\n",
            "        [ 0.1488],\n",
            "        [ 0.0684],\n",
            "        [ 1.2002],\n",
            "        [ 2.0402],\n",
            "        [ 0.1222],\n",
            "        [ 2.0538],\n",
            "        [ 1.9918],\n",
            "        [ 2.1663],\n",
            "        [ 2.0175],\n",
            "        [ 0.8038]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1885, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.8723e+00],\n",
            "        [ 7.2418e-02],\n",
            "        [ 9.1456e-01],\n",
            "        [ 3.7862e-01],\n",
            "        [ 2.0271e+00],\n",
            "        [ 8.1386e-02],\n",
            "        [-2.9073e-02],\n",
            "        [ 6.9667e-02],\n",
            "        [ 1.0609e+00],\n",
            "        [ 9.5517e-01],\n",
            "        [ 2.2609e+00],\n",
            "        [ 2.8877e-01],\n",
            "        [ 2.7973e+00],\n",
            "        [ 1.1764e+00],\n",
            "        [ 2.8552e+00],\n",
            "        [ 2.0550e+00],\n",
            "        [ 2.0155e-01],\n",
            "        [ 1.7907e-01],\n",
            "        [ 9.1909e-01],\n",
            "        [ 1.6535e+00],\n",
            "        [ 1.6085e-01],\n",
            "        [ 2.1692e+00],\n",
            "        [ 2.3224e+00],\n",
            "        [ 2.0706e+00],\n",
            "        [ 8.5584e-02],\n",
            "        [-1.2945e-01],\n",
            "        [ 2.7550e-02],\n",
            "        [-2.0334e-02],\n",
            "        [ 1.7010e+00],\n",
            "        [ 1.3027e-03],\n",
            "        [ 1.8701e+00],\n",
            "        [ 1.8900e+00]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1605, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.9067],\n",
            "        [ 0.0625],\n",
            "        [ 0.1585],\n",
            "        [ 1.9221],\n",
            "        [ 2.2831],\n",
            "        [ 1.9698],\n",
            "        [ 2.3973],\n",
            "        [ 2.5010],\n",
            "        [ 1.7144],\n",
            "        [ 0.1892],\n",
            "        [ 2.1474],\n",
            "        [ 1.9108],\n",
            "        [ 2.3349],\n",
            "        [ 0.1367],\n",
            "        [ 2.1892],\n",
            "        [ 2.9819],\n",
            "        [-0.0820],\n",
            "        [ 0.2092],\n",
            "        [ 0.2226],\n",
            "        [ 0.1233],\n",
            "        [ 1.7607],\n",
            "        [ 1.2970],\n",
            "        [-0.0776],\n",
            "        [ 2.0874],\n",
            "        [ 2.4675],\n",
            "        [ 2.6708],\n",
            "        [-0.0862],\n",
            "        [-0.0223],\n",
            "        [ 0.0802],\n",
            "        [ 0.0458],\n",
            "        [ 0.1173],\n",
            "        [ 2.1175]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3653, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.6861],\n",
            "        [ 1.1393],\n",
            "        [ 1.1498],\n",
            "        [ 2.1547],\n",
            "        [ 2.4316],\n",
            "        [ 1.4558],\n",
            "        [ 0.1776],\n",
            "        [ 0.1944],\n",
            "        [ 0.2777],\n",
            "        [ 2.3114],\n",
            "        [ 2.3335],\n",
            "        [ 0.1498],\n",
            "        [ 0.3310],\n",
            "        [ 1.7219],\n",
            "        [ 0.1888],\n",
            "        [-0.0869],\n",
            "        [ 2.1131],\n",
            "        [ 2.3426],\n",
            "        [ 2.3443],\n",
            "        [ 0.3539],\n",
            "        [ 2.3805],\n",
            "        [-0.2871],\n",
            "        [ 2.0617],\n",
            "        [ 1.7462],\n",
            "        [ 1.0627],\n",
            "        [ 0.9741],\n",
            "        [ 0.2633],\n",
            "        [ 1.9714],\n",
            "        [-0.2388],\n",
            "        [ 0.1623],\n",
            "        [ 2.4681],\n",
            "        [ 2.2782]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2579, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.4642],\n",
            "        [ 0.2552],\n",
            "        [ 0.3137],\n",
            "        [ 0.8248],\n",
            "        [ 0.9359],\n",
            "        [ 0.1170],\n",
            "        [ 2.0971],\n",
            "        [ 0.4160],\n",
            "        [ 1.8643],\n",
            "        [ 3.2049],\n",
            "        [ 0.5678],\n",
            "        [-0.0877],\n",
            "        [ 1.3671],\n",
            "        [ 1.6341],\n",
            "        [ 1.2183],\n",
            "        [-0.2642],\n",
            "        [ 2.0887],\n",
            "        [ 2.8282],\n",
            "        [ 0.3570],\n",
            "        [ 2.2466],\n",
            "        [ 0.0481],\n",
            "        [ 1.7889],\n",
            "        [ 0.3254],\n",
            "        [ 0.3003],\n",
            "        [ 0.0548],\n",
            "        [ 2.1339],\n",
            "        [ 2.6842],\n",
            "        [ 3.0012],\n",
            "        [ 2.6240],\n",
            "        [-0.1691],\n",
            "        [ 2.0218],\n",
            "        [ 0.8648]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.4291, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.5149],\n",
            "        [ 2.0083],\n",
            "        [ 0.0080],\n",
            "        [ 1.9964],\n",
            "        [ 2.2760],\n",
            "        [ 0.2541],\n",
            "        [ 2.1380],\n",
            "        [ 0.3663],\n",
            "        [ 0.1625],\n",
            "        [-0.1301],\n",
            "        [ 1.0743],\n",
            "        [ 1.9490],\n",
            "        [ 1.6910],\n",
            "        [ 2.4976],\n",
            "        [ 2.0348],\n",
            "        [ 1.1406],\n",
            "        [ 2.1393],\n",
            "        [ 2.8534],\n",
            "        [-0.2677],\n",
            "        [ 2.1401],\n",
            "        [ 0.0398],\n",
            "        [ 1.4454],\n",
            "        [-0.0828],\n",
            "        [ 2.7513],\n",
            "        [-0.1553],\n",
            "        [ 2.2281],\n",
            "        [ 0.0241],\n",
            "        [-0.0776],\n",
            "        [ 2.1112],\n",
            "        [ 0.4788],\n",
            "        [ 1.1083],\n",
            "        [ 1.7447]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2380, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.3791],\n",
            "        [ 0.0695],\n",
            "        [ 1.9353],\n",
            "        [ 0.7306],\n",
            "        [ 2.0243],\n",
            "        [-0.1228],\n",
            "        [ 0.1145],\n",
            "        [ 1.9737],\n",
            "        [ 1.9308],\n",
            "        [ 0.6861],\n",
            "        [ 0.8033],\n",
            "        [ 1.7147],\n",
            "        [-0.0319],\n",
            "        [ 1.9959],\n",
            "        [ 2.1318],\n",
            "        [ 0.3095],\n",
            "        [ 1.5867],\n",
            "        [ 2.4158],\n",
            "        [ 1.2755],\n",
            "        [-0.2919],\n",
            "        [-0.0091],\n",
            "        [ 1.8169],\n",
            "        [ 1.1722],\n",
            "        [ 0.1608],\n",
            "        [ 0.7014],\n",
            "        [ 1.1243],\n",
            "        [-0.2672],\n",
            "        [ 1.2476],\n",
            "        [ 0.0552],\n",
            "        [ 0.4410],\n",
            "        [-0.0823],\n",
            "        [ 1.7568]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2331, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.4417],\n",
            "        [-0.3251],\n",
            "        [ 0.2004],\n",
            "        [ 0.6067],\n",
            "        [ 1.9316],\n",
            "        [ 0.9373],\n",
            "        [ 0.2245],\n",
            "        [ 1.9486],\n",
            "        [-0.1146],\n",
            "        [-0.3034],\n",
            "        [ 1.6313],\n",
            "        [ 1.4811],\n",
            "        [ 0.1503],\n",
            "        [ 1.1262],\n",
            "        [ 2.7621],\n",
            "        [ 1.8562],\n",
            "        [ 0.1857],\n",
            "        [ 0.8660],\n",
            "        [ 2.2521],\n",
            "        [ 0.8190],\n",
            "        [ 0.1591],\n",
            "        [ 0.3573],\n",
            "        [ 1.7897],\n",
            "        [-0.1637],\n",
            "        [ 1.3829],\n",
            "        [ 1.6548],\n",
            "        [ 1.6902],\n",
            "        [ 0.1535],\n",
            "        [-0.2396],\n",
            "        [ 1.1356],\n",
            "        [ 0.8515],\n",
            "        [-0.0567]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.0705, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[-4.5422e-02],\n",
            "        [ 2.0228e+00],\n",
            "        [-9.8142e-02],\n",
            "        [ 1.7730e+00],\n",
            "        [-9.8270e-02],\n",
            "        [-2.9749e-02],\n",
            "        [ 6.3523e-01],\n",
            "        [ 9.7532e-02],\n",
            "        [ 1.8849e-03],\n",
            "        [ 6.5365e-01],\n",
            "        [ 8.1251e-01],\n",
            "        [-6.2609e-02],\n",
            "        [ 1.7994e+00],\n",
            "        [ 3.1271e-01],\n",
            "        [ 2.1300e-02],\n",
            "        [ 1.5416e+00],\n",
            "        [ 1.6765e+00],\n",
            "        [ 2.1020e-01],\n",
            "        [ 1.1055e-01],\n",
            "        [ 1.1851e-02],\n",
            "        [ 2.3450e+00],\n",
            "        [-6.8489e-02],\n",
            "        [ 2.0097e-01],\n",
            "        [-5.0361e-03],\n",
            "        [ 5.7704e-02],\n",
            "        [ 1.8987e+00],\n",
            "        [ 1.5654e-01],\n",
            "        [ 2.0723e+00],\n",
            "        [ 1.6236e+00],\n",
            "        [ 2.1178e-01],\n",
            "        [ 1.9253e+00],\n",
            "        [ 1.1636e-01]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3736, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.5130],\n",
            "        [-0.1947],\n",
            "        [ 0.7149],\n",
            "        [ 0.2337],\n",
            "        [ 0.0105],\n",
            "        [ 1.8705],\n",
            "        [ 2.0002],\n",
            "        [-0.0775],\n",
            "        [ 0.2251],\n",
            "        [ 1.7605],\n",
            "        [ 1.6861],\n",
            "        [ 0.1688],\n",
            "        [ 0.1774],\n",
            "        [ 1.8342],\n",
            "        [ 1.6429],\n",
            "        [-0.2028],\n",
            "        [ 1.8313],\n",
            "        [ 1.9434],\n",
            "        [ 1.8910],\n",
            "        [ 1.9466],\n",
            "        [-0.3007],\n",
            "        [ 1.3153],\n",
            "        [ 1.3412],\n",
            "        [-0.0283],\n",
            "        [ 2.5680],\n",
            "        [ 0.1469],\n",
            "        [ 1.7795],\n",
            "        [ 2.1889],\n",
            "        [ 1.8283],\n",
            "        [ 1.9762],\n",
            "        [ 0.9554],\n",
            "        [ 1.6049]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1520, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[-0.2156],\n",
            "        [-0.1665],\n",
            "        [ 1.4744],\n",
            "        [ 0.8075],\n",
            "        [ 0.4911],\n",
            "        [ 0.4280],\n",
            "        [ 1.9460],\n",
            "        [ 1.1536],\n",
            "        [ 2.0584],\n",
            "        [ 0.3193],\n",
            "        [-0.0921],\n",
            "        [-0.1172],\n",
            "        [ 1.6083],\n",
            "        [-0.3573],\n",
            "        [-0.1938],\n",
            "        [ 0.7026],\n",
            "        [ 3.0078],\n",
            "        [ 1.8982],\n",
            "        [ 1.8516],\n",
            "        [ 2.0811],\n",
            "        [ 1.7673],\n",
            "        [ 1.8296],\n",
            "        [-0.2947],\n",
            "        [ 1.3639],\n",
            "        [-0.0143],\n",
            "        [ 1.9612],\n",
            "        [ 1.8432],\n",
            "        [ 0.0067],\n",
            "        [ 1.7118],\n",
            "        [ 0.1166],\n",
            "        [-0.0051],\n",
            "        [ 1.7152]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.8822, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[-0.2844],\n",
            "        [ 0.1936],\n",
            "        [ 0.1110],\n",
            "        [ 1.7227],\n",
            "        [ 3.3419],\n",
            "        [ 0.1898],\n",
            "        [ 1.7535],\n",
            "        [ 0.0504],\n",
            "        [ 1.7548],\n",
            "        [ 0.7113],\n",
            "        [ 0.6589],\n",
            "        [ 1.8160],\n",
            "        [ 0.6315],\n",
            "        [ 0.0725],\n",
            "        [ 1.1593],\n",
            "        [ 2.2748],\n",
            "        [ 1.9610],\n",
            "        [ 1.9043],\n",
            "        [ 0.0859],\n",
            "        [ 1.7145],\n",
            "        [ 3.3117],\n",
            "        [ 1.7133],\n",
            "        [ 1.9500],\n",
            "        [-0.0094],\n",
            "        [ 0.1421],\n",
            "        [ 1.7164],\n",
            "        [ 3.1953],\n",
            "        [ 0.0323],\n",
            "        [ 0.0042],\n",
            "        [ 0.9753],\n",
            "        [ 0.3176],\n",
            "        [ 1.4628]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.0828, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.9870],\n",
            "        [ 1.8032],\n",
            "        [ 0.1606],\n",
            "        [ 2.2170],\n",
            "        [ 0.5351],\n",
            "        [ 0.7994],\n",
            "        [ 1.6292],\n",
            "        [ 2.0486],\n",
            "        [ 0.1230],\n",
            "        [ 0.2406],\n",
            "        [ 1.7743],\n",
            "        [-0.0575],\n",
            "        [ 1.8667],\n",
            "        [ 0.3219],\n",
            "        [ 1.5184],\n",
            "        [ 0.4140],\n",
            "        [-0.0119],\n",
            "        [ 2.0513],\n",
            "        [ 1.6713],\n",
            "        [-0.0206],\n",
            "        [-0.1667],\n",
            "        [-0.0246],\n",
            "        [ 0.2885],\n",
            "        [ 1.9884],\n",
            "        [ 0.0628],\n",
            "        [-0.1866],\n",
            "        [ 1.8652],\n",
            "        [-0.4058],\n",
            "        [ 0.0429],\n",
            "        [ 1.6665],\n",
            "        [ 0.5876],\n",
            "        [-0.0081]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2548, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.8722],\n",
            "        [ 0.9511],\n",
            "        [ 1.8503],\n",
            "        [ 0.1540],\n",
            "        [ 0.1710],\n",
            "        [-0.1018],\n",
            "        [ 1.9099],\n",
            "        [-0.0448],\n",
            "        [-0.3111],\n",
            "        [ 1.0026],\n",
            "        [ 3.6423],\n",
            "        [ 2.1653],\n",
            "        [ 0.9366],\n",
            "        [ 0.2147],\n",
            "        [-0.1935],\n",
            "        [-0.0792],\n",
            "        [ 2.0232],\n",
            "        [ 1.9071],\n",
            "        [ 2.5316],\n",
            "        [ 0.2521],\n",
            "        [ 0.2403],\n",
            "        [ 1.3073],\n",
            "        [ 0.0151],\n",
            "        [-0.0693],\n",
            "        [ 0.2297],\n",
            "        [ 1.3968],\n",
            "        [ 1.8319],\n",
            "        [ 1.9387],\n",
            "        [ 2.3067],\n",
            "        [ 2.0325],\n",
            "        [ 1.7948],\n",
            "        [-0.2022]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1330, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.8197],\n",
            "        [ 1.1584],\n",
            "        [ 2.0488],\n",
            "        [ 0.2274],\n",
            "        [ 2.1941],\n",
            "        [-0.0393],\n",
            "        [ 0.2205],\n",
            "        [ 0.2227],\n",
            "        [ 1.4657],\n",
            "        [ 2.6058],\n",
            "        [ 2.1679],\n",
            "        [ 2.4210],\n",
            "        [ 2.0666],\n",
            "        [-0.0216],\n",
            "        [-0.0673],\n",
            "        [-0.1974],\n",
            "        [ 1.9161],\n",
            "        [ 0.7381],\n",
            "        [ 2.3776],\n",
            "        [ 0.1816],\n",
            "        [ 0.7049],\n",
            "        [ 1.9200],\n",
            "        [ 1.9388],\n",
            "        [ 2.1610],\n",
            "        [ 2.2823],\n",
            "        [ 1.7598],\n",
            "        [ 0.0535],\n",
            "        [-0.1377],\n",
            "        [ 0.1154],\n",
            "        [ 1.9397],\n",
            "        [ 0.0517],\n",
            "        [ 2.0348]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1230, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.0272],\n",
            "        [-0.1215],\n",
            "        [ 3.1149],\n",
            "        [ 2.1428],\n",
            "        [ 1.8808],\n",
            "        [ 0.6895],\n",
            "        [-0.0133],\n",
            "        [ 0.0253],\n",
            "        [-0.0097],\n",
            "        [ 2.5778],\n",
            "        [ 2.0579],\n",
            "        [ 1.0846],\n",
            "        [-0.0863],\n",
            "        [ 2.2295],\n",
            "        [ 1.9565],\n",
            "        [ 2.1026],\n",
            "        [ 2.2469],\n",
            "        [ 0.8731],\n",
            "        [ 0.3088],\n",
            "        [-0.0152],\n",
            "        [ 2.3733],\n",
            "        [ 2.3791],\n",
            "        [ 0.3366],\n",
            "        [ 0.8019],\n",
            "        [ 1.3333],\n",
            "        [ 2.5477],\n",
            "        [ 2.0925],\n",
            "        [ 2.1369],\n",
            "        [ 0.5674],\n",
            "        [ 1.8100],\n",
            "        [ 2.1388],\n",
            "        [ 0.0131]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.0551, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.4134],\n",
            "        [ 1.9784],\n",
            "        [ 0.2905],\n",
            "        [ 2.3077],\n",
            "        [ 1.9489],\n",
            "        [-0.0148],\n",
            "        [-0.1601],\n",
            "        [ 0.2039],\n",
            "        [ 2.0190],\n",
            "        [ 0.0305],\n",
            "        [ 0.5978],\n",
            "        [ 0.0619],\n",
            "        [ 2.1220],\n",
            "        [ 1.1137],\n",
            "        [ 2.0175],\n",
            "        [-0.1218],\n",
            "        [ 2.1615],\n",
            "        [ 1.5381],\n",
            "        [-0.0504],\n",
            "        [ 2.0704],\n",
            "        [ 0.1591],\n",
            "        [ 0.0897],\n",
            "        [ 0.4325],\n",
            "        [ 1.9182],\n",
            "        [ 2.2983],\n",
            "        [ 0.0933],\n",
            "        [ 2.1355],\n",
            "        [ 0.8921],\n",
            "        [ 2.1637],\n",
            "        [ 1.2697],\n",
            "        [ 1.3677],\n",
            "        [ 0.8459]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.4000, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 2.0952],\n",
            "        [ 0.1137],\n",
            "        [ 2.2847],\n",
            "        [ 0.0547],\n",
            "        [ 0.1134],\n",
            "        [ 0.2835],\n",
            "        [ 2.5939],\n",
            "        [ 0.1135],\n",
            "        [ 1.8807],\n",
            "        [ 2.3791],\n",
            "        [ 1.0354],\n",
            "        [-0.0247],\n",
            "        [ 2.2356],\n",
            "        [ 2.4167],\n",
            "        [ 0.1091],\n",
            "        [ 2.5618],\n",
            "        [ 1.1245],\n",
            "        [ 3.8673],\n",
            "        [ 2.8998],\n",
            "        [ 1.5922],\n",
            "        [ 2.4732],\n",
            "        [ 2.3800],\n",
            "        [ 1.9248],\n",
            "        [ 0.1496],\n",
            "        [ 1.6911],\n",
            "        [ 0.1339],\n",
            "        [ 1.9087],\n",
            "        [ 0.1653],\n",
            "        [ 0.0606],\n",
            "        [ 2.0847],\n",
            "        [ 0.7473],\n",
            "        [ 0.7191]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.0789, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.1161],\n",
            "        [ 0.1777],\n",
            "        [ 1.8526],\n",
            "        [ 0.9728],\n",
            "        [ 0.6278],\n",
            "        [ 0.2690],\n",
            "        [ 2.0562],\n",
            "        [ 0.1513],\n",
            "        [ 2.2580],\n",
            "        [ 1.7620],\n",
            "        [ 0.2308],\n",
            "        [ 2.0742],\n",
            "        [ 0.1186],\n",
            "        [ 0.3057],\n",
            "        [ 1.6628],\n",
            "        [ 0.2878],\n",
            "        [ 2.2207],\n",
            "        [-0.0478],\n",
            "        [-0.1975],\n",
            "        [ 2.3958],\n",
            "        [ 1.2178],\n",
            "        [ 0.3546],\n",
            "        [ 2.2398],\n",
            "        [ 2.1182],\n",
            "        [ 0.1361],\n",
            "        [ 2.1526],\n",
            "        [ 1.7503],\n",
            "        [ 0.0410],\n",
            "        [ 2.2614],\n",
            "        [ 2.7608],\n",
            "        [ 1.5966],\n",
            "        [ 2.1898]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.4598, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[-0.0759],\n",
            "        [-0.0648],\n",
            "        [ 1.7162],\n",
            "        [ 1.9413],\n",
            "        [ 3.1692],\n",
            "        [-0.1172],\n",
            "        [ 0.0506],\n",
            "        [ 1.8807],\n",
            "        [ 0.2982],\n",
            "        [ 0.7087],\n",
            "        [ 2.5034],\n",
            "        [ 0.0382],\n",
            "        [ 1.6230],\n",
            "        [ 0.0986],\n",
            "        [ 2.1301],\n",
            "        [ 1.7598],\n",
            "        [ 2.0250],\n",
            "        [ 1.3186],\n",
            "        [ 2.1104],\n",
            "        [ 1.8826],\n",
            "        [ 1.1644],\n",
            "        [ 2.1103],\n",
            "        [ 0.1098],\n",
            "        [ 0.0653],\n",
            "        [ 0.0431],\n",
            "        [-0.0943],\n",
            "        [ 0.0967],\n",
            "        [ 0.3511],\n",
            "        [ 0.3922],\n",
            "        [ 0.2298],\n",
            "        [ 0.1475],\n",
            "        [ 1.0717]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.1182, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 2.4020],\n",
            "        [ 2.1962],\n",
            "        [ 0.6773],\n",
            "        [ 0.1117],\n",
            "        [ 0.0816],\n",
            "        [ 0.1924],\n",
            "        [ 1.9252],\n",
            "        [ 1.8089],\n",
            "        [ 2.6457],\n",
            "        [-0.0446],\n",
            "        [ 1.7423],\n",
            "        [ 2.0928],\n",
            "        [ 1.9399],\n",
            "        [ 2.2799],\n",
            "        [ 2.1878],\n",
            "        [ 2.1833],\n",
            "        [-0.1625],\n",
            "        [-0.1091],\n",
            "        [ 0.3246],\n",
            "        [ 1.6740],\n",
            "        [ 1.1581],\n",
            "        [ 0.7803],\n",
            "        [ 0.0489],\n",
            "        [ 0.4748],\n",
            "        [ 1.9236],\n",
            "        [ 2.9156],\n",
            "        [ 1.6445],\n",
            "        [ 1.6430],\n",
            "        [ 0.2488],\n",
            "        [ 0.6732],\n",
            "        [ 0.1468],\n",
            "        [ 2.1242]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2828, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 1.9602],\n",
            "        [ 0.1270],\n",
            "        [ 0.4555],\n",
            "        [ 0.1260],\n",
            "        [ 1.8414],\n",
            "        [-0.0642],\n",
            "        [ 1.9487],\n",
            "        [-0.1166],\n",
            "        [ 1.7963],\n",
            "        [ 1.7072],\n",
            "        [-0.0152],\n",
            "        [ 1.6217],\n",
            "        [ 0.7140],\n",
            "        [ 1.7144],\n",
            "        [ 2.0081],\n",
            "        [ 0.1628],\n",
            "        [ 0.5332],\n",
            "        [ 0.5915],\n",
            "        [ 1.9633],\n",
            "        [-0.0595],\n",
            "        [ 0.2268],\n",
            "        [-0.0376],\n",
            "        [ 2.8122],\n",
            "        [ 1.2382],\n",
            "        [ 2.3640],\n",
            "        [ 2.0643],\n",
            "        [ 0.8625],\n",
            "        [-0.0753],\n",
            "        [ 0.5878],\n",
            "        [ 0.9327],\n",
            "        [ 0.8945],\n",
            "        [ 3.3967]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.2561, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 0.0243],\n",
            "        [ 0.7847],\n",
            "        [ 0.5569],\n",
            "        [ 2.8787],\n",
            "        [-0.0375],\n",
            "        [ 1.9176],\n",
            "        [ 2.0315],\n",
            "        [-0.1451],\n",
            "        [ 0.0325],\n",
            "        [ 1.7309],\n",
            "        [-0.1849],\n",
            "        [ 2.0771],\n",
            "        [ 0.1276],\n",
            "        [-0.1291],\n",
            "        [ 1.9035],\n",
            "        [ 1.9455],\n",
            "        [ 1.9250],\n",
            "        [ 0.0594],\n",
            "        [ 0.2698],\n",
            "        [ 3.0635],\n",
            "        [ 0.5476],\n",
            "        [ 2.0265],\n",
            "        [ 0.0951],\n",
            "        [-0.0373],\n",
            "        [ 0.1963],\n",
            "        [-0.0378],\n",
            "        [-0.0580],\n",
            "        [ 1.8493],\n",
            "        [ 0.1483],\n",
            "        [ 1.9426],\n",
            "        [ 1.9259],\n",
            "        [ 0.0985]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.3163, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[ 3.0498],\n",
            "        [ 1.6526],\n",
            "        [ 2.0582],\n",
            "        [ 0.2157],\n",
            "        [ 0.0353],\n",
            "        [ 1.7512],\n",
            "        [ 2.5486],\n",
            "        [ 1.7503],\n",
            "        [ 1.1263],\n",
            "        [-0.0906],\n",
            "        [ 0.2506],\n",
            "        [ 1.8323],\n",
            "        [ 1.8914],\n",
            "        [ 0.1837],\n",
            "        [ 2.8369],\n",
            "        [ 0.2194],\n",
            "        [ 0.2794],\n",
            "        [ 0.1872],\n",
            "        [ 1.5405],\n",
            "        [ 1.9054],\n",
            "        [ 2.1736],\n",
            "        [ 1.7323],\n",
            "        [ 0.7395],\n",
            "        [ 1.9982],\n",
            "        [ 0.7881],\n",
            "        [ 1.6646],\n",
            "        [ 1.9358],\n",
            "        [ 1.5589],\n",
            "        [-0.0745],\n",
            "        [ 1.8970],\n",
            "        [ 1.9420],\n",
            "        [ 1.8004]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "train_dataloader:  SequenceClassifierOutput(loss=tensor(0.0243, device='cuda:0', grad_fn=<MseLossBackward0>), logits=tensor([[1.8442]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "\n",
            "  Average training loss: 0.24\n",
            "  Training epoch took:  31.090996\n",
            "\n",
            "Running Validation...\n",
            "b_input_ids:  tensor([[  101,  2330,  2689,  ...,     0,     0,     0],\n",
            "        [  101, 12210,  4471,  ...,     0,     0,     0],\n",
            "        [  101, 11562,  5587,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  4653,  2035,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  2291,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  2291,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [ 0.02250143  1.1734946   2.1433291   1.8474182   1.892763    1.9284197\n",
            "  0.01644543  0.19831079  1.7226746   0.10254214  0.0146845  -0.0310279\n",
            "  2.1433291   2.0690525   1.9679154   1.7824186   2.1290758   0.02099905\n",
            "  0.01252724  1.9395229   0.9877138   1.9076566   0.0469758   0.00629903\n",
            "  0.01002192  0.8870845   0.8005499   0.01051133  1.0296446   1.845511\n",
            "  0.9182391   0.40438733]\n",
            "label_ids [0. 1. 2. 2. 2. 2. 0. 4. 2. 0. 0. 0. 1. 2. 2. 1. 2. 0. 0. 2. 1. 2. 0. 0.\n",
            " 0. 1. 1. 0. 1. 2. 1. 1.]\n",
            "Mean absolute error : 0.2661472878535278\n",
            "b_input_ids:  tensor([[  101,  4607,  7514,  ...,     0,     0,     0],\n",
            "        [  101, 11562,  7929,  ...,     0,     0,     0],\n",
            "        [  101,  3116,  2038,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2291,  4618,  ...,     0,     0,     0],\n",
            "        [  101,  5310,  2064,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  8619,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [ 2.5231969e-03  8.8708448e-01  1.9063755e+00  8.1230942e-03\n",
            "  2.0690651e+00  1.2109805e+00  2.0688159e+00 -1.4970308e-02\n",
            "  2.0039263e+00  4.8759561e-03 -1.2411492e-02 -1.0137098e-02\n",
            " -5.4027941e-03  6.2314611e-02  1.8757038e+00  2.0690525e+00\n",
            "  1.1734946e+00  1.7673172e+00  1.6805451e+00  1.1170493e+00\n",
            "  7.9666787e-01  2.0284867e+00  1.8694667e+00  1.1734946e+00\n",
            "  1.8587402e+00  3.1440969e+00  2.3701785e+00  1.2234720e+00\n",
            "  9.2943490e-01  1.8783250e+00  1.5073048e-02  6.1570551e-02]\n",
            "label_ids [0. 1. 2. 0. 2. 0. 3. 0. 0. 0. 0. 0. 0. 0. 2. 2. 1. 2. 2. 2. 1. 2. 2. 1.\n",
            " 2. 6. 2. 0. 1. 2. 0. 0.]\n",
            "Mean absolute error : 0.36687281262129545\n",
            "b_input_ids:  tensor([[  101, 11562,  3972,  ...,     0,     0,     0],\n",
            "        [  101, 12210,  4471,  ...,     0,     0,     0],\n",
            "        [  101,  3967,  2052,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101, 11562,  7929,  ...,     0,     0,     0],\n",
            "        [  101,  2492,  3942,  ...,     0,     0,     0],\n",
            "        [  101,  2291,  4618,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [ 1.0431558   1.1734946   0.30986792  1.9922149   1.4188768   0.90481865\n",
            "  0.00858854  1.0802737   1.1734946   0.00608593  2.1109192   0.01507305\n",
            "  0.11185155 -0.00385329 -0.00370085  0.02561384  1.8893412   1.9086456\n",
            "  1.7058235   0.03432215  1.6990205   0.15220179  1.0058806   0.17397226\n",
            " -0.00951138  2.395148    1.8514336  -0.0123745   1.6411889   0.8870845\n",
            "  2.0253367   1.791691  ]\n",
            "label_ids [1. 1. 0. 2. 0. 0. 0. 1. 0. 0. 2. 0. 0. 0. 0. 0. 2. 2. 2. 0. 0. 2. 0. 0.\n",
            " 0. 2. 2. 0. 2. 1. 2. 2.]\n",
            "Mean absolute error : 0.3414252958027646\n",
            "b_input_ids:  tensor([[  101,  8013,  4978,  ...,     0,     0,     0],\n",
            "        [  101,  8619, 11562,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  2291,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101, 11562,  3828,  ...,     0,     0,     0],\n",
            "        [  101,  2291, 11204,  ...,     0,     0,     0],\n",
            "        [  101,  8619,  2064,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [ 1.5203908e-01  2.1602616e+00  1.5336864e+00  1.3515255e-01\n",
            "  3.0222038e-02  1.7656484e+00  2.0690525e+00  1.6665566e+00\n",
            "  2.1433291e+00  1.8343171e+00  1.2597104e+00  1.1734946e+00\n",
            "  2.2306935e-01  9.5049031e-02  8.5141170e-01  3.3592308e-01\n",
            "  1.0455698e+00 -6.1039794e-03  1.7053362e+00  1.9141118e-01\n",
            "  1.8776242e+00  1.8680383e+00  1.2975784e-02 -1.2903790e-03\n",
            " -1.2289638e-03  1.7367820e+00  1.6088561e+00  2.6185820e-02\n",
            "  6.7758694e-02  2.0690525e+00  1.0566723e+00 -3.1604469e-02]\n",
            "label_ids [1. 2. 3. 0. 0. 2. 2. 2. 2. 2. 2. 1. 0. 0. 1. 1. 2. 2. 2. 0. 2. 2. 0. 0.\n",
            " 0. 2. 1. 0. 0. 2. 1. 0.]\n",
            "Mean absolute error : 0.3271914949218626\n",
            "b_input_ids:  tensor([[  101,  2291, 19818,  ...,     0,     0,     0],\n",
            "        [  101,  2048, 12020,  ...,     0,     0,     0],\n",
            "        [  101, 19413,  2497,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  5310,  2064,  ...,     0,     0,     0],\n",
            "        [  101,  2291,  4604,  ...,     0,     0,     0],\n",
            "        [  101,  5310, 29225,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [ 1.6513994e+00  3.6698854e-01  5.0812799e-02  3.0731270e+00\n",
            "  2.5849702e-02 -3.0794274e-03  1.0686003e+00  2.0962159e-01\n",
            "  9.1560000e-01  2.0750709e+00  1.9139062e+00  5.4833573e-01\n",
            "  2.0690525e+00  1.8180792e+00  2.5510855e+00  2.7158764e-01\n",
            "  1.2527237e-02  8.5980433e-01  1.9141526e+00  6.7921376e-01\n",
            " -4.9349722e-03  1.6334197e-01  1.8387436e+00 -9.3343016e-03\n",
            "  1.9131130e+00  1.9005727e+00  1.8488730e+00  1.9684966e-01\n",
            "  2.0690525e+00  2.0886604e-03  2.6512465e+00  2.3422377e-01]\n",
            "label_ids [2. 2. 2. 3. 0. 0. 2. 0. 1. 2. 2. 0. 2. 2. 3. 0. 0. 1. 2. 0. 0. 0. 2. 2.\n",
            " 2. 2. 2. 0. 2. 0. 4. 2.]\n",
            "Mean absolute error : 0.434870813798625\n",
            "b_input_ids:  tensor([[  101,  5310,  2064,  ...,     0,     0,     0],\n",
            "        [  101,  5310, 29225,  ...,     0,     0,     0],\n",
            "        [  101,  2291, 19818,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101, 11562,  5587,  ...,     0,     0,     0],\n",
            "        [  101,  4607,  8094,  ...,     0,     0,     0],\n",
            "        [  101,  5310,  2064,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [ 0.8795735   0.23814888  1.7235698   0.02618582  1.0430832   0.00775976\n",
            "  1.5668124   1.7370353   1.3007284   0.80057645  1.7108265   0.00985799\n",
            " -0.02277398  0.49830222  0.382843    1.8914315   1.8926866   0.0544166\n",
            "  2.0864902   0.07432106  0.02037807  1.828527    1.8509855   0.8870845\n",
            "  0.02557498  1.2203411   1.6400539  -0.0100984   2.0690525   2.1433291\n",
            "  0.00430112 -0.01058116]\n",
            "label_ids [0. 0. 2. 0. 1. 0. 1. 2. 1. 1. 2. 0. 0. 0. 0. 2. 2. 0. 2. 0. 0. 2. 2. 1.\n",
            " 0. 0. 2. 0. 3. 2. 1. 0.]\n",
            "Mean absolute error : 0.26829591765999794\n",
            "b_input_ids:  tensor([[  101,  4653,  2035,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  2291,  ...,     0,     0,     0],\n",
            "        [  101,  5310, 29225,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  8013,  8039,  ...,     0,     0,     0],\n",
            "        [  101,  2291, 19818,  ...,     0,     0,     0],\n",
            "        [  101,  2291, 15210,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [ 1.8969564   1.9851578   0.29189187  0.41194674  0.19831079  0.04840681\n",
            "  0.4111384   0.1617574   0.06777228  0.8870845   0.20420523  1.7899631\n",
            "  0.95955086  1.8725532   1.1575139   1.0076987   1.9569006  -0.01692594\n",
            "  1.9021995   0.890583    0.02937531  2.1093833   0.8870845   0.01136589\n",
            " -0.02087985  0.934966    0.19831079  1.8541883   1.9249922  -0.00615986\n",
            "  1.8544728   1.6400539 ]\n",
            "label_ids [2. 2. 1. 0. 0. 0. 0. 1. 0. 1. 1. 2. 0. 2. 0. 1. 2. 0. 2. 0. 0. 2. 1. 0.\n",
            " 0. 0. 0. 2. 2. 0. 2. 2.]\n",
            "Mean absolute error : 0.2928383440594189\n",
            "b_input_ids:  tensor([[  101,  5310, 29225,  ...,     0,     0,     0],\n",
            "        [  101,  8619,  6039,  ...,     0,     0,     0],\n",
            "        [  101, 11562,  3828,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2291, 15210,  ...,     0,     0,     0],\n",
            "        [  101,  5310, 12453,  ...,     0,     0,     0],\n",
            "        [  101,  5402,  2038,  ...,     0,     0,     0]], device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')\n",
            "logits:  [ 2.55036056e-01  2.36306060e-02  2.06905246e+00  2.06905246e+00\n",
            "  1.13011695e-01  8.11500661e-03  4.42320369e-02  9.41222191e-01\n",
            "  1.70966927e-02  1.88996065e+00  5.43297052e-01  1.38271798e-03\n",
            "  2.09375429e+00  1.82378983e+00  2.04041019e-01  1.28112817e+00\n",
            " -2.39290483e-03  1.14618223e-02  8.08582544e-01 -1.74569543e-02\n",
            "  5.26504815e-01  4.91950095e-01  3.18977737e+00  8.92694294e-01\n",
            "  2.99223995e+00  2.06700161e-01  5.42759709e-02  9.56227332e-02\n",
            "  9.12191272e-02  1.64005387e+00  9.16205049e-01  1.98000288e+00]\n",
            "label_ids [0. 0. 2. 2. 0. 0. 0. 3. 0. 2. 0. 0. 3. 3. 0. 0. 0. 0. 1. 1. 0. 1. 3. 1.\n",
            " 3. 0. 0. 0. 0. 2. 0. 2.]\n",
            "Mean absolute error : 0.3495763229475415\n",
            "b_input_ids:  tensor([[  101,  8619,  6039,  1996,  4746,  2224,  2005,  3784,  2492,  2005,\n",
            "          1996,  9459,  3639,  1012,   102,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  1996,  5310, 15867,  1996,  1000, 12210,  1000,  5724,  1012,\n",
            "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101, 11562, 10086,  6462,  2005,  3563, 11412,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101, 11562,  7929,  6462,   102,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  101,  2291,  4618, 18584,  3076,  2592,  2013,  7809,   102,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]],\n",
            "       device='cuda:0')\n",
            "b_input_ids:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]], device='cuda:0')\n",
            "logits:  [0.04745423 0.04123082 0.8560617  0.8870845  1.8488729 ]\n",
            "label_ids [0. 1. 0. 1. 2.]\n",
            "Mean absolute error : 0.42526554986834525\n",
            "  Accuracy: 0.34\n",
            "  Validation Loss: 0.00\n",
            "  Validation took:  1.443466\n",
            "Total training took:  127.760771\n"
          ]
        }
      ],
      "source": [
        "seed_val = 20\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "t_start = datetime.now()\n",
        "d = {}\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "\n",
        "    t_epoch_trng = datetime.now()\n",
        "\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = (datetime.now() - t_epoch_trng).total_seconds()\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "        trng_output = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "                \n",
        "        total_train_loss += trng_output.loss.item()\n",
        "\n",
        "        trng_output.loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        print(\"train_dataloader: \",trng_output)\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "\n",
        "    training_time = datetime.now() - t_epoch_trng\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: \", training_time.total_seconds())\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t_epoch_validation = datetime.now()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        print(\"b_input_ids: \", b_input_ids)\n",
        "        print(\"b_input_ids: \", b_input_mask)\n",
        "\n",
        "\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            validation_output = model(b_input_ids, \n",
        "                                   attention_mask=b_input_mask)\n",
        "\n",
        "        logits = validation_output.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        print(\"logits: \",logits.flatten())\n",
        "        print(\"label_ids\",label_ids.flatten())\n",
        "        total_eval_accuracy += get_mae(logits, label_ids)\n",
        "\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "    validation_time = datetime.now() - t_epoch_validation\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: \", validation_time.total_seconds())\n",
        "    d[epoch_i] = model.cuda()\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "print(\"Total training took: \", (datetime.now() - t_start).total_seconds())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Nrrd30eRXQe",
        "outputId": "398f6bab-95e1-485f-b831-f05f998096e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid_accuracy:  [0.45778720764081093, 0.37920105682711036, 0.3519629239104688, 0.34138709328148653]\n",
            "min index:  3\n",
            "test_dataset:  tensor([  101,  8619, 12040,  2015,  2010,  1013,  2014,  8833,  2378,  2592,\n",
            "         1012,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
            "Mean absolute error : 0.3528059867094271\n",
            "Pred:  [1.9779304, 1.8831135, 1.1456013, 1.2404028, 1.8666158, 1.2399454, 0.7373822, 0.012527237, 0.0810019, 0.06635677, 2.0690525, 2.6512465, 1.9141526, 2.116219, 2.1433291, 2.0690525, 2.1433291, 0.026252257, 0.044230327, 1.7775608, 1.7945064, 1.7663201, 1.9099649, -0.020350719, 1.8587402, 0.8744859, 1.8723763, 0.6992508, 1.1734946, 0.034801416, 1.5917767, 2.125819]\n",
            "Actual:  [2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 3.0, 4.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n",
            "Mean absolute error : 0.2546784458681941\n",
            "Pred:  [1.9779304, 1.8831135, 1.1456013, 1.2404028, 1.8666158, 1.2399454, 0.7373822, 0.012527237, 0.0810019, 0.06635677, 2.0690525, 2.6512465, 1.9141526, 2.116219, 2.1433291, 2.0690525, 2.1433291, 0.026252257, 0.044230327, 1.7775608, 1.7945064, 1.7663201, 1.9099649, -0.020350719, 1.8587402, 0.8744859, 1.8723763, 0.6992508, 1.1734946, 0.034801416, 1.5917767, 2.125819, 0.06391847, -0.01459956, 0.9294349, 0.5754093, 1.9413345, 1.7655487, 1.7186928, 1.9065403, -0.014487015, 0.08392377, 1.8893412, 2.1433291, 0.8917234, 0.971875, 0.22804844, -0.007043386, 0.037121747, 0.4042501, 2.0690525, 0.9314897, 0.71773803, 0.044383932, 1.8619365, 1.1332499, 0.24665628, 0.015620781, 0.0388848, 1.848873, -0.00527619, 0.0031094868, 1.2210089, 0.25141248]\n",
            "Actual:  [2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 3.0, 4.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 0.0, 0.0, 2.0, 2.0, 1.0, 1.0, 2.0, 0.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0]\n",
            "Mean absolute error : 0.4875997625058517\n",
            "Pred:  [1.9779304, 1.8831135, 1.1456013, 1.2404028, 1.8666158, 1.2399454, 0.7373822, 0.012527237, 0.0810019, 0.06635677, 2.0690525, 2.6512465, 1.9141526, 2.116219, 2.1433291, 2.0690525, 2.1433291, 0.026252257, 0.044230327, 1.7775608, 1.7945064, 1.7663201, 1.9099649, -0.020350719, 1.8587402, 0.8744859, 1.8723763, 0.6992508, 1.1734946, 0.034801416, 1.5917767, 2.125819, 0.06391847, -0.01459956, 0.9294349, 0.5754093, 1.9413345, 1.7655487, 1.7186928, 1.9065403, -0.014487015, 0.08392377, 1.8893412, 2.1433291, 0.8917234, 0.971875, 0.22804844, -0.007043386, 0.037121747, 0.4042501, 2.0690525, 0.9314897, 0.71773803, 0.044383932, 1.8619365, 1.1332499, 0.24665628, 0.015620781, 0.0388848, 1.848873, -0.00527619, 0.0031094868, 1.2210089, 0.25141248, 2.1001103, 1.7819601, 0.065078914, 1.8218799, 0.021792294, 0.17408182, 0.42319646, 1.8090521, 1.0821667, 0.06764042, 1.874289, 2.0690525, 1.8637782, 1.8769059, 0.18000518, 0.9294349, 0.18996783, -0.020425877, 2.6441288, 0.8259071, 0.02618582, -0.008238805, 0.24849902, 1.8422202, -0.006419262, 2.146595, 0.18383665, 1.8153514, 0.9294349, 2.048503, 0.15647247, 2.197843]\n",
            "Actual:  [2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 3.0, 4.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 0.0, 0.0, 2.0, 2.0, 1.0, 1.0, 2.0, 0.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 3.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 2.0, 1.0, 3.0, 0.0, 2.0]\n",
            "Mean absolute error : 0.553175279230345\n",
            "Pred:  [1.9779304, 1.8831135, 1.1456013, 1.2404028, 1.8666158, 1.2399454, 0.7373822, 0.012527237, 0.0810019, 0.06635677, 2.0690525, 2.6512465, 1.9141526, 2.116219, 2.1433291, 2.0690525, 2.1433291, 0.026252257, 0.044230327, 1.7775608, 1.7945064, 1.7663201, 1.9099649, -0.020350719, 1.8587402, 0.8744859, 1.8723763, 0.6992508, 1.1734946, 0.034801416, 1.5917767, 2.125819, 0.06391847, -0.01459956, 0.9294349, 0.5754093, 1.9413345, 1.7655487, 1.7186928, 1.9065403, -0.014487015, 0.08392377, 1.8893412, 2.1433291, 0.8917234, 0.971875, 0.22804844, -0.007043386, 0.037121747, 0.4042501, 2.0690525, 0.9314897, 0.71773803, 0.044383932, 1.8619365, 1.1332499, 0.24665628, 0.015620781, 0.0388848, 1.848873, -0.00527619, 0.0031094868, 1.2210089, 0.25141248, 2.1001103, 1.7819601, 0.065078914, 1.8218799, 0.021792294, 0.17408182, 0.42319646, 1.8090521, 1.0821667, 0.06764042, 1.874289, 2.0690525, 1.8637782, 1.8769059, 0.18000518, 0.9294349, 0.18996783, -0.020425877, 2.6441288, 0.8259071, 0.02618582, -0.008238805, 0.24849902, 1.8422202, -0.006419262, 2.146595, 0.18383665, 1.8153514, 0.9294349, 2.048503, 0.15647247, 2.197843, 1.8323358, 0.12416007, 0.8738594, 1.9171954, 0.93222916, 0.26388597, 2.404441, 2.1290758, 2.1433291, 0.3313716, 0.42672485, 1.8348492, 1.878325, 3.1846998, 1.9320377, 1.6488756, -0.01829292, 0.008053573, 1.9660207, 0.002691226, 2.0209088, 1.0176462, -0.025486851, 1.8190292, 2.1433291, 0.015073048, 1.9302727, 0.07373709, 1.4046458, 0.99972045, 1.8538815, 0.409198]\n",
            "Actual:  [2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 3.0, 4.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 0.0, 0.0, 2.0, 2.0, 1.0, 1.0, 2.0, 0.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 3.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 2.0, 1.0, 3.0, 0.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 2.0, 4.0, 2.0, 6.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 1.0, 1.0, 2.0, 2.0, 3.0]\n",
            "Mean absolute error : 0.3436215711408295\n",
            "Pred:  [1.9779304, 1.8831135, 1.1456013, 1.2404028, 1.8666158, 1.2399454, 0.7373822, 0.012527237, 0.0810019, 0.06635677, 2.0690525, 2.6512465, 1.9141526, 2.116219, 2.1433291, 2.0690525, 2.1433291, 0.026252257, 0.044230327, 1.7775608, 1.7945064, 1.7663201, 1.9099649, -0.020350719, 1.8587402, 0.8744859, 1.8723763, 0.6992508, 1.1734946, 0.034801416, 1.5917767, 2.125819, 0.06391847, -0.01459956, 0.9294349, 0.5754093, 1.9413345, 1.7655487, 1.7186928, 1.9065403, -0.014487015, 0.08392377, 1.8893412, 2.1433291, 0.8917234, 0.971875, 0.22804844, -0.007043386, 0.037121747, 0.4042501, 2.0690525, 0.9314897, 0.71773803, 0.044383932, 1.8619365, 1.1332499, 0.24665628, 0.015620781, 0.0388848, 1.848873, -0.00527619, 0.0031094868, 1.2210089, 0.25141248, 2.1001103, 1.7819601, 0.065078914, 1.8218799, 0.021792294, 0.17408182, 0.42319646, 1.8090521, 1.0821667, 0.06764042, 1.874289, 2.0690525, 1.8637782, 1.8769059, 0.18000518, 0.9294349, 0.18996783, -0.020425877, 2.6441288, 0.8259071, 0.02618582, -0.008238805, 0.24849902, 1.8422202, -0.006419262, 2.146595, 0.18383665, 1.8153514, 0.9294349, 2.048503, 0.15647247, 2.197843, 1.8323358, 0.12416007, 0.8738594, 1.9171954, 0.93222916, 0.26388597, 2.404441, 2.1290758, 2.1433291, 0.3313716, 0.42672485, 1.8348492, 1.878325, 3.1846998, 1.9320377, 1.6488756, -0.01829292, 0.008053573, 1.9660207, 0.002691226, 2.0209088, 1.0176462, -0.025486851, 1.8190292, 2.1433291, 0.015073048, 1.9302727, 0.07373709, 1.4046458, 0.99972045, 1.8538815, 0.409198, 0.004611807, 2.4340672, 0.052521583, 2.6734092, 1.8210957, 1.2228899, 0.015073048, 1.6834829, 1.8426267, 0.39886254, 1.8625051, 0.0058705118, 1.9114422, 0.12063136, 0.459842, 0.004669519, 0.4380929, 0.01793663, 0.0064572636, 1.8739543, 0.13214199, 2.4849615, 1.973481, 0.04290237, 1.0468194, 0.9589579, 1.5851058, 0.9147417, 0.017991105, 1.9086456, 2.1009245, 1.7586709]\n",
            "Actual:  [2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 3.0, 4.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 0.0, 0.0, 2.0, 2.0, 1.0, 1.0, 2.0, 0.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 3.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 2.0, 1.0, 3.0, 0.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 2.0, 4.0, 2.0, 6.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 1.0, 1.0, 2.0, 2.0, 3.0, 0.0, 3.0, 1.0, 4.0, 2.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 3.0, 3.0, 0.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 2.0, 2.0]\n",
            "Mean absolute error : 0.354547937928146\n",
            "Pred:  [1.9779304, 1.8831135, 1.1456013, 1.2404028, 1.8666158, 1.2399454, 0.7373822, 0.012527237, 0.0810019, 0.06635677, 2.0690525, 2.6512465, 1.9141526, 2.116219, 2.1433291, 2.0690525, 2.1433291, 0.026252257, 0.044230327, 1.7775608, 1.7945064, 1.7663201, 1.9099649, -0.020350719, 1.8587402, 0.8744859, 1.8723763, 0.6992508, 1.1734946, 0.034801416, 1.5917767, 2.125819, 0.06391847, -0.01459956, 0.9294349, 0.5754093, 1.9413345, 1.7655487, 1.7186928, 1.9065403, -0.014487015, 0.08392377, 1.8893412, 2.1433291, 0.8917234, 0.971875, 0.22804844, -0.007043386, 0.037121747, 0.4042501, 2.0690525, 0.9314897, 0.71773803, 0.044383932, 1.8619365, 1.1332499, 0.24665628, 0.015620781, 0.0388848, 1.848873, -0.00527619, 0.0031094868, 1.2210089, 0.25141248, 2.1001103, 1.7819601, 0.065078914, 1.8218799, 0.021792294, 0.17408182, 0.42319646, 1.8090521, 1.0821667, 0.06764042, 1.874289, 2.0690525, 1.8637782, 1.8769059, 0.18000518, 0.9294349, 0.18996783, -0.020425877, 2.6441288, 0.8259071, 0.02618582, -0.008238805, 0.24849902, 1.8422202, -0.006419262, 2.146595, 0.18383665, 1.8153514, 0.9294349, 2.048503, 0.15647247, 2.197843, 1.8323358, 0.12416007, 0.8738594, 1.9171954, 0.93222916, 0.26388597, 2.404441, 2.1290758, 2.1433291, 0.3313716, 0.42672485, 1.8348492, 1.878325, 3.1846998, 1.9320377, 1.6488756, -0.01829292, 0.008053573, 1.9660207, 0.002691226, 2.0209088, 1.0176462, -0.025486851, 1.8190292, 2.1433291, 0.015073048, 1.9302727, 0.07373709, 1.4046458, 0.99972045, 1.8538815, 0.409198, 0.004611807, 2.4340672, 0.052521583, 2.6734092, 1.8210957, 1.2228899, 0.015073048, 1.6834829, 1.8426267, 0.39886254, 1.8625051, 0.0058705118, 1.9114422, 0.12063136, 0.459842, 0.004669519, 0.4380929, 0.01793663, 0.0064572636, 1.8739543, 0.13214199, 2.4849615, 1.973481, 0.04290237, 1.0468194, 0.9589579, 1.5851058, 0.9147417, 0.017991105, 1.9086456, 2.1009245, 1.7586709, -0.00527619, 0.8870845, 1.9660207, 0.015073048, 1.784409, 0.5110162, 0.06834243, 0.0010613223, 1.608766, 0.11782785, 0.8870845, 0.14573096, 1.9284197, 0.023350252, 0.020700773, 1.9249922, 0.63423884, 0.009479651, 1.934214, 0.15794301, 2.5556529, 0.02042977, 2.0690525, -0.0120663885, 1.8571391, 1.8380035, 1.8612247, 2.125819, 3.0685701, -0.018823603, 0.86022526, 0.7769511]\n",
            "Actual:  [2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 3.0, 4.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 0.0, 0.0, 2.0, 2.0, 1.0, 1.0, 2.0, 0.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 3.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 2.0, 1.0, 3.0, 0.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 2.0, 4.0, 2.0, 6.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 1.0, 1.0, 2.0, 2.0, 3.0, 0.0, 3.0, 1.0, 4.0, 2.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 3.0, 3.0, 0.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 2.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 3.0, 1.0, 2.0, 0.0, 1.0, 1.0]\n",
            "Mean absolute error : 0.36517018115046085\n",
            "Pred:  [1.9779304, 1.8831135, 1.1456013, 1.2404028, 1.8666158, 1.2399454, 0.7373822, 0.012527237, 0.0810019, 0.06635677, 2.0690525, 2.6512465, 1.9141526, 2.116219, 2.1433291, 2.0690525, 2.1433291, 0.026252257, 0.044230327, 1.7775608, 1.7945064, 1.7663201, 1.9099649, -0.020350719, 1.8587402, 0.8744859, 1.8723763, 0.6992508, 1.1734946, 0.034801416, 1.5917767, 2.125819, 0.06391847, -0.01459956, 0.9294349, 0.5754093, 1.9413345, 1.7655487, 1.7186928, 1.9065403, -0.014487015, 0.08392377, 1.8893412, 2.1433291, 0.8917234, 0.971875, 0.22804844, -0.007043386, 0.037121747, 0.4042501, 2.0690525, 0.9314897, 0.71773803, 0.044383932, 1.8619365, 1.1332499, 0.24665628, 0.015620781, 0.0388848, 1.848873, -0.00527619, 0.0031094868, 1.2210089, 0.25141248, 2.1001103, 1.7819601, 0.065078914, 1.8218799, 0.021792294, 0.17408182, 0.42319646, 1.8090521, 1.0821667, 0.06764042, 1.874289, 2.0690525, 1.8637782, 1.8769059, 0.18000518, 0.9294349, 0.18996783, -0.020425877, 2.6441288, 0.8259071, 0.02618582, -0.008238805, 0.24849902, 1.8422202, -0.006419262, 2.146595, 0.18383665, 1.8153514, 0.9294349, 2.048503, 0.15647247, 2.197843, 1.8323358, 0.12416007, 0.8738594, 1.9171954, 0.93222916, 0.26388597, 2.404441, 2.1290758, 2.1433291, 0.3313716, 0.42672485, 1.8348492, 1.878325, 3.1846998, 1.9320377, 1.6488756, -0.01829292, 0.008053573, 1.9660207, 0.002691226, 2.0209088, 1.0176462, -0.025486851, 1.8190292, 2.1433291, 0.015073048, 1.9302727, 0.07373709, 1.4046458, 0.99972045, 1.8538815, 0.409198, 0.004611807, 2.4340672, 0.052521583, 2.6734092, 1.8210957, 1.2228899, 0.015073048, 1.6834829, 1.8426267, 0.39886254, 1.8625051, 0.0058705118, 1.9114422, 0.12063136, 0.459842, 0.004669519, 0.4380929, 0.01793663, 0.0064572636, 1.8739543, 0.13214199, 2.4849615, 1.973481, 0.04290237, 1.0468194, 0.9589579, 1.5851058, 0.9147417, 0.017991105, 1.9086456, 2.1009245, 1.7586709, -0.00527619, 0.8870845, 1.9660207, 0.015073048, 1.784409, 0.5110162, 0.06834243, 0.0010613223, 1.608766, 0.11782785, 0.8870845, 0.14573096, 1.9284197, 0.023350252, 0.020700773, 1.9249922, 0.63423884, 0.009479651, 1.934214, 0.15794301, 2.5556529, 0.02042977, 2.0690525, -0.0120663885, 1.8571391, 1.8380035, 1.8612247, 2.125819, 3.0685701, -0.018823603, 0.86022526, 0.7769511, 1.8198689, 0.2692385, 2.467908, 0.010203322, 1.0419155, -0.017620144, 0.02618582, 1.8469913, 0.00014503498, 0.32940677, 1.4278055, 2.5574307, 1.8467879, 1.0226079, 0.015073048, 0.03653941, 2.0690525, 2.604508, 1.8535367, 1.0248967, 0.36698854, 1.9531397, 2.0808914, 2.0690525, 1.8321974, -0.020879848, 2.1343596, 1.8723763, -0.011356713, 2.1433291, 2.1433291, -0.010581156]\n",
            "Actual:  [2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 3.0, 4.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 0.0, 0.0, 2.0, 2.0, 1.0, 1.0, 2.0, 0.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 3.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 2.0, 1.0, 3.0, 0.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 2.0, 4.0, 2.0, 6.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 1.0, 1.0, 2.0, 2.0, 3.0, 0.0, 3.0, 1.0, 4.0, 2.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 3.0, 3.0, 0.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 2.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 3.0, 1.0, 2.0, 0.0, 1.0, 1.0, 2.0, 1.0, 4.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 0.0, 4.0, 2.0, 1.0, 0.0, 0.0, 2.0, 3.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0]\n",
            "Mean absolute error : 0.3612854374223389\n",
            "Pred:  [1.9779304, 1.8831135, 1.1456013, 1.2404028, 1.8666158, 1.2399454, 0.7373822, 0.012527237, 0.0810019, 0.06635677, 2.0690525, 2.6512465, 1.9141526, 2.116219, 2.1433291, 2.0690525, 2.1433291, 0.026252257, 0.044230327, 1.7775608, 1.7945064, 1.7663201, 1.9099649, -0.020350719, 1.8587402, 0.8744859, 1.8723763, 0.6992508, 1.1734946, 0.034801416, 1.5917767, 2.125819, 0.06391847, -0.01459956, 0.9294349, 0.5754093, 1.9413345, 1.7655487, 1.7186928, 1.9065403, -0.014487015, 0.08392377, 1.8893412, 2.1433291, 0.8917234, 0.971875, 0.22804844, -0.007043386, 0.037121747, 0.4042501, 2.0690525, 0.9314897, 0.71773803, 0.044383932, 1.8619365, 1.1332499, 0.24665628, 0.015620781, 0.0388848, 1.848873, -0.00527619, 0.0031094868, 1.2210089, 0.25141248, 2.1001103, 1.7819601, 0.065078914, 1.8218799, 0.021792294, 0.17408182, 0.42319646, 1.8090521, 1.0821667, 0.06764042, 1.874289, 2.0690525, 1.8637782, 1.8769059, 0.18000518, 0.9294349, 0.18996783, -0.020425877, 2.6441288, 0.8259071, 0.02618582, -0.008238805, 0.24849902, 1.8422202, -0.006419262, 2.146595, 0.18383665, 1.8153514, 0.9294349, 2.048503, 0.15647247, 2.197843, 1.8323358, 0.12416007, 0.8738594, 1.9171954, 0.93222916, 0.26388597, 2.404441, 2.1290758, 2.1433291, 0.3313716, 0.42672485, 1.8348492, 1.878325, 3.1846998, 1.9320377, 1.6488756, -0.01829292, 0.008053573, 1.9660207, 0.002691226, 2.0209088, 1.0176462, -0.025486851, 1.8190292, 2.1433291, 0.015073048, 1.9302727, 0.07373709, 1.4046458, 0.99972045, 1.8538815, 0.409198, 0.004611807, 2.4340672, 0.052521583, 2.6734092, 1.8210957, 1.2228899, 0.015073048, 1.6834829, 1.8426267, 0.39886254, 1.8625051, 0.0058705118, 1.9114422, 0.12063136, 0.459842, 0.004669519, 0.4380929, 0.01793663, 0.0064572636, 1.8739543, 0.13214199, 2.4849615, 1.973481, 0.04290237, 1.0468194, 0.9589579, 1.5851058, 0.9147417, 0.017991105, 1.9086456, 2.1009245, 1.7586709, -0.00527619, 0.8870845, 1.9660207, 0.015073048, 1.784409, 0.5110162, 0.06834243, 0.0010613223, 1.608766, 0.11782785, 0.8870845, 0.14573096, 1.9284197, 0.023350252, 0.020700773, 1.9249922, 0.63423884, 0.009479651, 1.934214, 0.15794301, 2.5556529, 0.02042977, 2.0690525, -0.0120663885, 1.8571391, 1.8380035, 1.8612247, 2.125819, 3.0685701, -0.018823603, 0.86022526, 0.7769511, 1.8198689, 0.2692385, 2.467908, 0.010203322, 1.0419155, -0.017620144, 0.02618582, 1.8469913, 0.00014503498, 0.32940677, 1.4278055, 2.5574307, 1.8467879, 1.0226079, 0.015073048, 0.03653941, 2.0690525, 2.604508, 1.8535367, 1.0248967, 0.36698854, 1.9531397, 2.0808914, 2.0690525, 1.8321974, -0.020879848, 2.1343596, 1.8723763, -0.011356713, 2.1433291, 2.1433291, -0.010581156, 1.4702117, 0.9000598, 0.009723002, 2.4849615, 0.03575223, 1.8864422, 0.022705147, 1.8010701, 1.7656484, -0.017768791, 0.1555249, 0.18025137, 1.8084029, 1.9669763, 1.570249, 0.014246417, 0.952126, -0.012594366, 1.3193442, 0.63350415, 1.8904636, 1.8914315, 1.1779621, 0.020378066, 1.8760066, 1.848873, 0.012026811, 1.82736, 0.16388097, 0.8595556, 2.1433291, 1.7687129]\n",
            "Actual:  [2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 3.0, 4.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 0.0, 0.0, 2.0, 2.0, 1.0, 1.0, 2.0, 0.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 3.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 2.0, 1.0, 3.0, 0.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 2.0, 4.0, 2.0, 6.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 1.0, 1.0, 2.0, 2.0, 3.0, 0.0, 3.0, 1.0, 4.0, 2.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 3.0, 3.0, 0.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 2.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 3.0, 1.0, 2.0, 0.0, 1.0, 1.0, 2.0, 1.0, 4.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 0.0, 4.0, 2.0, 1.0, 0.0, 0.0, 2.0, 3.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 2.0, 1.0, 0.0, 3.0, 0.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 1.0, 2.0, 1.0, 4.0, 0.0, 0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0, 2.0, 0.0, 2.0, 2.0, 2.0]\n",
            "Mean absolute error : 0.713798972312361\n",
            "Pred:  [1.9779304, 1.8831135, 1.1456013, 1.2404028, 1.8666158, 1.2399454, 0.7373822, 0.012527237, 0.0810019, 0.06635677, 2.0690525, 2.6512465, 1.9141526, 2.116219, 2.1433291, 2.0690525, 2.1433291, 0.026252257, 0.044230327, 1.7775608, 1.7945064, 1.7663201, 1.9099649, -0.020350719, 1.8587402, 0.8744859, 1.8723763, 0.6992508, 1.1734946, 0.034801416, 1.5917767, 2.125819, 0.06391847, -0.01459956, 0.9294349, 0.5754093, 1.9413345, 1.7655487, 1.7186928, 1.9065403, -0.014487015, 0.08392377, 1.8893412, 2.1433291, 0.8917234, 0.971875, 0.22804844, -0.007043386, 0.037121747, 0.4042501, 2.0690525, 0.9314897, 0.71773803, 0.044383932, 1.8619365, 1.1332499, 0.24665628, 0.015620781, 0.0388848, 1.848873, -0.00527619, 0.0031094868, 1.2210089, 0.25141248, 2.1001103, 1.7819601, 0.065078914, 1.8218799, 0.021792294, 0.17408182, 0.42319646, 1.8090521, 1.0821667, 0.06764042, 1.874289, 2.0690525, 1.8637782, 1.8769059, 0.18000518, 0.9294349, 0.18996783, -0.020425877, 2.6441288, 0.8259071, 0.02618582, -0.008238805, 0.24849902, 1.8422202, -0.006419262, 2.146595, 0.18383665, 1.8153514, 0.9294349, 2.048503, 0.15647247, 2.197843, 1.8323358, 0.12416007, 0.8738594, 1.9171954, 0.93222916, 0.26388597, 2.404441, 2.1290758, 2.1433291, 0.3313716, 0.42672485, 1.8348492, 1.878325, 3.1846998, 1.9320377, 1.6488756, -0.01829292, 0.008053573, 1.9660207, 0.002691226, 2.0209088, 1.0176462, -0.025486851, 1.8190292, 2.1433291, 0.015073048, 1.9302727, 0.07373709, 1.4046458, 0.99972045, 1.8538815, 0.409198, 0.004611807, 2.4340672, 0.052521583, 2.6734092, 1.8210957, 1.2228899, 0.015073048, 1.6834829, 1.8426267, 0.39886254, 1.8625051, 0.0058705118, 1.9114422, 0.12063136, 0.459842, 0.004669519, 0.4380929, 0.01793663, 0.0064572636, 1.8739543, 0.13214199, 2.4849615, 1.973481, 0.04290237, 1.0468194, 0.9589579, 1.5851058, 0.9147417, 0.017991105, 1.9086456, 2.1009245, 1.7586709, -0.00527619, 0.8870845, 1.9660207, 0.015073048, 1.784409, 0.5110162, 0.06834243, 0.0010613223, 1.608766, 0.11782785, 0.8870845, 0.14573096, 1.9284197, 0.023350252, 0.020700773, 1.9249922, 0.63423884, 0.009479651, 1.934214, 0.15794301, 2.5556529, 0.02042977, 2.0690525, -0.0120663885, 1.8571391, 1.8380035, 1.8612247, 2.125819, 3.0685701, -0.018823603, 0.86022526, 0.7769511, 1.8198689, 0.2692385, 2.467908, 0.010203322, 1.0419155, -0.017620144, 0.02618582, 1.8469913, 0.00014503498, 0.32940677, 1.4278055, 2.5574307, 1.8467879, 1.0226079, 0.015073048, 0.03653941, 2.0690525, 2.604508, 1.8535367, 1.0248967, 0.36698854, 1.9531397, 2.0808914, 2.0690525, 1.8321974, -0.020879848, 2.1343596, 1.8723763, -0.011356713, 2.1433291, 2.1433291, -0.010581156, 1.4702117, 0.9000598, 0.009723002, 2.4849615, 0.03575223, 1.8864422, 0.022705147, 1.8010701, 1.7656484, -0.017768791, 0.1555249, 0.18025137, 1.8084029, 1.9669763, 1.570249, 0.014246417, 0.952126, -0.012594366, 1.3193442, 0.63350415, 1.8904636, 1.8914315, 1.1779621, 0.020378066, 1.8760066, 1.848873, 0.012026811, 1.82736, 0.16388097, 0.8595556, 2.1433291, 1.7687129, 0.020261237, 0.815794, 0.44938102, 0.89989024]\n",
            "Actual:  [2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 3.0, 4.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 0.0, 0.0, 2.0, 2.0, 1.0, 1.0, 2.0, 0.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 2.0, 1.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 3.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 2.0, 1.0, 3.0, 0.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 2.0, 4.0, 2.0, 6.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 1.0, 1.0, 2.0, 2.0, 3.0, 0.0, 3.0, 1.0, 4.0, 2.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 3.0, 3.0, 0.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 2.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 3.0, 1.0, 2.0, 0.0, 1.0, 1.0, 2.0, 1.0, 4.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 0.0, 4.0, 2.0, 1.0, 0.0, 0.0, 2.0, 3.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 2.0, 1.0, 0.0, 3.0, 0.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 1.0, 2.0, 1.0, 4.0, 0.0, 0.0, 0.0, 1.0, 1.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0, 2.0, 0.0, 2.0, 2.0, 2.0, 0.0, 1.0, 3.0, 1.0]\n",
            "Test Accuracy: 0.421\n",
            "c:  1\n",
            "c:  2\n",
            "c:  3\n",
            "c:  4\n",
            "c:  5\n",
            "c:  6\n",
            "c:  7\n",
            "c:  8\n",
            "c:  9\n",
            "c:  10\n",
            "c:  11\n",
            "c:  12\n",
            "c:  13\n",
            "c:  14\n",
            "c:  15\n",
            "c:  16\n",
            "c:  17\n",
            "c:  18\n",
            "c:  19\n",
            "c:  20\n",
            "c:  21\n",
            "c:  22\n",
            "c:  23\n",
            "c:  24\n",
            "c:  25\n",
            "c:  26\n",
            "c:  27\n",
            "c:  28\n",
            "c:  29\n",
            "c:  30\n",
            "c:  31\n",
            "c:  32\n",
            "c:  33\n",
            "c:  34\n",
            "c:  35\n",
            "c:  36\n",
            "c:  37\n",
            "c:  38\n",
            "c:  39\n",
            "c:  40\n",
            "c:  41\n",
            "c:  42\n",
            "c:  43\n",
            "c:  44\n",
            "c:  45\n",
            "c:  46\n",
            "c:  47\n",
            "c:  48\n",
            "c:  49\n",
            "c:  50\n",
            "c:  51\n",
            "c:  52\n",
            "c:  53\n",
            "c:  54\n",
            "c:  55\n",
            "c:  56\n",
            "c:  57\n",
            "c:  58\n",
            "c:  59\n",
            "c:  60\n",
            "c:  61\n",
            "c:  62\n",
            "c:  63\n",
            "c:  64\n",
            "c:  65\n",
            "c:  66\n",
            "c:  67\n",
            "c:  68\n",
            "c:  69\n",
            "c:  70\n",
            "c:  71\n",
            "c:  72\n",
            "c:  73\n",
            "c:  74\n",
            "c:  75\n",
            "c:  76\n",
            "c:  77\n",
            "c:  78\n",
            "c:  79\n",
            "c:  80\n",
            "c:  81\n",
            "c:  82\n",
            "c:  83\n",
            "c:  84\n",
            "c:  85\n",
            "c:  86\n",
            "c:  87\n",
            "c:  88\n",
            "c:  89\n",
            "c:  90\n",
            "c:  91\n",
            "c:  92\n",
            "c:  93\n",
            "c:  94\n",
            "c:  95\n",
            "c:  96\n",
            "c:  97\n",
            "c:  98\n",
            "c:  99\n",
            "c:  100\n",
            "c:  101\n",
            "c:  102\n",
            "c:  103\n",
            "c:  104\n",
            "c:  105\n",
            "c:  106\n",
            "c:  107\n",
            "c:  108\n",
            "c:  109\n",
            "c:  110\n",
            "c:  111\n",
            "c:  112\n",
            "c:  113\n",
            "c:  114\n",
            "c:  115\n",
            "c:  116\n",
            "c:  117\n",
            "c:  118\n",
            "c:  119\n",
            "c:  120\n",
            "c:  121\n",
            "c:  122\n",
            "c:  123\n",
            "c:  124\n",
            "c:  125\n",
            "c:  126\n",
            "c:  127\n",
            "c:  128\n",
            "c:  129\n",
            "c:  130\n",
            "c:  131\n",
            "c:  132\n",
            "c:  133\n",
            "c:  134\n",
            "c:  135\n",
            "c:  136\n",
            "c:  137\n",
            "c:  138\n",
            "c:  139\n",
            "c:  140\n",
            "c:  141\n",
            "c:  142\n",
            "c:  143\n",
            "c:  144\n",
            "c:  145\n",
            "c:  146\n",
            "c:  147\n",
            "c:  148\n",
            "c:  149\n",
            "c:  150\n",
            "c:  151\n",
            "c:  152\n",
            "c:  153\n",
            "c:  154\n",
            "c:  155\n",
            "c:  156\n",
            "c:  157\n",
            "c:  158\n",
            "c:  159\n",
            "c:  160\n",
            "c:  161\n",
            "c:  162\n",
            "MRE value:  39.61580242216587\n",
            "MMRE value:  0.24454199026028314\n",
            "p:  0.011034787\n",
            "c:  1\n",
            "p:  0.8831135\n",
            "c:  2\n",
            "p:  0.14560127\n",
            "c:  3\n",
            "p:  0.24040282\n",
            "c:  4\n",
            "p:  0.066692114\n",
            "c:  5\n",
            "p:  0.23994541\n",
            "c:  6\n",
            "p:  0.9189981\n",
            "c:  7\n",
            "p:  0.31031585\n",
            "c:  8\n",
            "p:  0.33718836\n",
            "c:  9\n",
            "p:  0.04292369\n",
            "c:  10\n",
            "p:  0.058109522\n",
            "c:  11\n",
            "p:  0.07166457\n",
            "c:  12\n",
            "p:  0.03452623\n",
            "c:  13\n",
            "p:  0.07166457\n",
            "c:  14\n",
            "p:  0.111219585\n",
            "c:  15\n",
            "p:  0.116839945\n",
            "c:  16\n",
            "p:  0.04501754\n",
            "c:  17\n",
            "p:  0.070629895\n",
            "c:  18\n",
            "p:  0.12551409\n",
            "c:  19\n",
            "p:  0.06381184\n",
            "c:  20\n",
            "p:  0.30074918\n",
            "c:  21\n",
            "p:  0.17349458\n",
            "c:  22\n",
            "p:  0.5917767\n",
            "c:  23\n",
            "p:  1.125819\n",
            "c:  24\n",
            "p:  0.070565104\n",
            "c:  25\n",
            "p:  0.029332757\n",
            "c:  26\n",
            "p:  0.11722565\n",
            "c:  27\n",
            "p:  0.046729863\n",
            "c:  28\n",
            "p:  0.055329382\n",
            "c:  29\n",
            "p:  0.07166457\n",
            "c:  30\n",
            "p:  0.108276606\n",
            "c:  31\n",
            "p:  0.028124988\n",
            "c:  32\n",
            "p:  0.8859758\n",
            "c:  33\n",
            "p:  0.03452623\n",
            "c:  34\n",
            "p:  0.068510294\n",
            "c:  35\n",
            "p:  0.069031775\n",
            "c:  36\n",
            "p:  0.13324988\n",
            "c:  37\n",
            "p:  0.7533437\n",
            "c:  38\n",
            "p:  0.07556349\n",
            "c:  39\n",
            "p:  0.2210089\n",
            "c:  40\n",
            "p:  0.29996324\n",
            "c:  41\n",
            "p:  0.109019935\n",
            "c:  42\n",
            "p:  0.08906007\n",
            "c:  43\n",
            "p:  0.98910385\n",
            "c:  44\n",
            "p:  0.06285548\n",
            "c:  45\n",
            "p:  0.03452623\n",
            "c:  46\n",
            "p:  0.06811088\n",
            "c:  47\n",
            "p:  0.06154704\n",
            "c:  48\n",
            "p:  0.9099974\n",
            "c:  49\n",
            "p:  0.070565104\n",
            "c:  50\n",
            "p:  0.3389678\n",
            "c:  51\n",
            "p:  0.07888991\n",
            "c:  52\n",
            "p:  0.0732975\n",
            "c:  53\n",
            "p:  0.90808165\n",
            "c:  54\n",
            "p:  0.09232432\n",
            "c:  55\n",
            "p:  0.070565104\n",
            "c:  56\n",
            "p:  0.3171657\n",
            "c:  57\n",
            "p:  0.09892154\n",
            "c:  58\n",
            "p:  0.083832085\n",
            "c:  59\n",
            "p:  0.5630703\n",
            "c:  60\n",
            "p:  0.04140228\n",
            "c:  61\n",
            "p:  0.20222056\n",
            "c:  62\n",
            "p:  0.06453788\n",
            "c:  63\n",
            "p:  0.07166457\n",
            "c:  64\n",
            "p:  0.78663754\n",
            "c:  65\n",
            "p:  0.54128766\n",
            "c:  66\n",
            "p:  0.060837507\n",
            "c:  67\n",
            "p:  0.4692167\n",
            "c:  68\n",
            "p:  0.033981144\n",
            "c:  69\n",
            "p:  0.016989648\n",
            "c:  70\n",
            "p:  0.010454416\n",
            "c:  71\n",
            "p:  0.017646194\n",
            "c:  72\n",
            "p:  0.090485394\n",
            "c:  73\n",
            "p:  0.07166457\n",
            "c:  74\n",
            "p:  0.03486365\n",
            "c:  75\n",
            "p:  0.9262629\n",
            "c:  76\n",
            "p:  0.4046458\n",
            "c:  77\n",
            "p:  0.5001398\n",
            "c:  78\n",
            "p:  0.07305926\n",
            "c:  79\n",
            "p:  0.8636007\n",
            "c:  80\n",
            "p:  0.18864425\n",
            "c:  81\n",
            "p:  0.9474784\n",
            "c:  82\n",
            "p:  0.3316477\n",
            "c:  83\n",
            "p:  0.08945215\n",
            "c:  84\n",
            "p:  0.2228899\n",
            "c:  85\n",
            "p:  0.15825856\n",
            "c:  86\n",
            "p:  0.078686655\n",
            "c:  87\n",
            "p:  0.06874746\n",
            "c:  88\n",
            "p:  0.04427892\n",
            "c:  89\n",
            "p:  0.87936866\n",
            "c:  90\n",
            "p:  0.5619071\n",
            "c:  91\n",
            "p:  0.06302285\n",
            "c:  92\n",
            "p:  0.1716795\n",
            "c:  93\n",
            "p:  0.34217298\n",
            "c:  94\n",
            "p:  0.04681945\n",
            "c:  95\n",
            "p:  0.04104209\n",
            "c:  96\n",
            "p:  0.20744711\n",
            "c:  97\n",
            "p:  0.085258305\n",
            "c:  98\n",
            "p:  0.050462246\n",
            "c:  99\n",
            "p:  0.12066454\n",
            "c:  100\n",
            "p:  0.112915516\n",
            "c:  101\n",
            "p:  0.016989648\n",
            "c:  102\n",
            "p:  0.10779548\n",
            "c:  103\n",
            "p:  0.4889838\n",
            "c:  104\n",
            "p:  0.60876596\n",
            "c:  105\n",
            "p:  0.112915516\n",
            "c:  106\n",
            "p:  0.035790145\n",
            "c:  107\n",
            "p:  0.0375039\n",
            "c:  108\n",
            "p:  0.032893002\n",
            "c:  109\n",
            "p:  0.9210285\n",
            "c:  110\n",
            "p:  0.03452623\n",
            "c:  111\n",
            "p:  0.071430445\n",
            "c:  112\n",
            "p:  0.08099824\n",
            "c:  113\n",
            "p:  0.3795918\n",
            "c:  114\n",
            "p:  1.125819\n",
            "c:  115\n",
            "p:  0.53428507\n",
            "c:  116\n",
            "p:  0.13977474\n",
            "c:  117\n",
            "p:  0.22304893\n",
            "c:  118\n",
            "p:  0.09006554\n",
            "c:  119\n",
            "p:  0.7307615\n",
            "c:  120\n",
            "p:  0.38302302\n",
            "c:  121\n",
            "p:  0.07650435\n",
            "c:  122\n",
            "p:  0.83529663\n",
            "c:  123\n",
            "p:  0.3606423\n",
            "c:  124\n",
            "p:  0.076606035\n",
            "c:  125\n",
            "p:  0.022607923\n",
            "c:  126\n",
            "p:  0.03452623\n",
            "c:  127\n",
            "p:  0.13183069\n",
            "c:  128\n",
            "p:  0.07323164\n",
            "c:  129\n",
            "p:  0.02489674\n",
            "c:  130\n",
            "p:  0.81650573\n",
            "c:  131\n",
            "p:  0.023430169\n",
            "c:  132\n",
            "p:  0.040445685\n",
            "c:  133\n",
            "p:  0.03452623\n",
            "c:  134\n",
            "p:  0.083901286\n",
            "c:  135\n",
            "p:  0.0671798\n",
            "c:  136\n",
            "p:  0.06381184\n",
            "c:  137\n",
            "p:  0.07166457\n",
            "c:  138\n",
            "p:  0.07166457\n",
            "c:  139\n",
            "p:  0.26489413\n",
            "c:  140\n",
            "p:  0.09994018\n",
            "c:  141\n",
            "p:  0.1716795\n",
            "c:  142\n",
            "p:  0.056778908\n",
            "c:  143\n",
            "p:  0.09946495\n",
            "c:  144\n",
            "p:  0.11717582\n",
            "c:  145\n",
            "p:  0.81974864\n",
            "c:  146\n",
            "p:  0.09579855\n",
            "c:  147\n",
            "p:  0.9669763\n",
            "c:  148\n",
            "p:  0.60743773\n",
            "c:  149\n",
            "p:  0.31934416\n",
            "c:  150\n",
            "p:  0.36649585\n",
            "c:  151\n",
            "p:  0.054768205\n",
            "c:  152\n",
            "p:  0.054284275\n",
            "c:  153\n",
            "p:  0.0619967\n",
            "c:  154\n",
            "p:  0.07556349\n",
            "c:  155\n",
            "p:  0.08631998\n",
            "c:  156\n",
            "p:  0.5702222\n",
            "c:  157\n",
            "p:  0.07166457\n",
            "c:  158\n",
            "p:  0.11564356\n",
            "c:  159\n",
            "p:  0.18420601\n",
            "c:  160\n",
            "p:  0.8502063\n",
            "c:  161\n",
            "p:  0.100109756\n",
            "c:  162\n",
            "Total Number for PRED30:  117\n",
            "PRED30:  0.7222222222222222\n"
          ]
        }
      ],
      "source": [
        "def evaluate(model, test_dataset, device):\n",
        "\n",
        "    test = test_dataset\n",
        "    print(\"test_dataset: \",test_dataset[0][0])\n",
        "    \n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "\n",
        "        model = model.cuda()\n",
        "\n",
        "        total_acc_test = 0\n",
        "        logits_arr = []\n",
        "        label_ids_arr = []\n",
        "        logits_ = []\n",
        "        labels_ = []\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for batch in test_dataloader:\n",
        "        \n",
        "               b_input_ids = batch[0].to(device)\n",
        "               b_input_mask = batch[1].to(device)\n",
        "               b_labels = batch[2].to(device)      \n",
        "\n",
        "               test_output = model(b_input_ids, \n",
        "                                   attention_mask=b_input_mask)\n",
        "    \n",
        "               logits = test_output.logits.detach().cpu().numpy()\n",
        "               label_ids = b_labels.to('cpu').numpy()\n",
        "               total_acc_test += get_mae(logits, label_ids)\n",
        "               \n",
        "               logits_arr = logits.flatten()\n",
        "               label_ids_arr = label_ids.flatten()\n",
        "               #print(\"Pred: \", logits_arr)\n",
        "               #print(\"Actual: \",label_ids_arr)\n",
        "               for l in logits_arr:\n",
        "                 logits_.append(l)\n",
        "               for la in label_ids_arr:\n",
        "                 labels_.append(la)\n",
        "               print(\"Pred: \", logits_)\n",
        "               print(\"Actual: \",labels_)  \n",
        "\n",
        "\n",
        "    avg_test_accuracy = total_acc_test / len(test_dataloader)\n",
        "    print(\"Test Accuracy: {0:.3f}\".format(avg_test_accuracy))\n",
        "    total = 0\n",
        "    x = 0\n",
        "    c = 0\n",
        "    for j in logits_:\n",
        "        if(labels_[x] != 0.0):\n",
        "          total += abs(j-labels_[x]) / abs(labels_[x])\n",
        "          c += 1\n",
        "          x = x + 1\n",
        "          #print(\"Total: \", total)\n",
        "          print(\"c: \", c)\n",
        "        else:\n",
        "          x = x + 1\n",
        "          continue\n",
        "          \n",
        "    average = total / c   \n",
        "    print(\"MRE value: \", total) \n",
        "    print(\"MMRE value: \", average) \n",
        "\n",
        "    p = 0\n",
        "    c1 = 0\n",
        "    c2 = 0\n",
        "    j = 0\n",
        "    x = 0\n",
        "    for j in logits_:\n",
        "        if(labels_[x] != 0.0):\n",
        "          p = abs(j-labels_[x]) / abs(labels_[x])\n",
        "          print(\"p: \",p)\n",
        "          c1 += 1\n",
        "          if (p < 0.3):\n",
        "            c2 += 1\n",
        "            \n",
        "          x = x + 1\n",
        "          p = 0\n",
        "          #print(\"Total: \", total)\n",
        "          print(\"c: \", c1)\n",
        "        else:\n",
        "          x = x + 1\n",
        "          continue\n",
        "    print(\"Total Number for PRED30: \", c2)\n",
        "    print(\"PRED30: \", (c2/c1))   \n",
        "\n",
        "valid_accuracy = [x['Valid. Accur.'] for x in training_stats]\n",
        "print(\"Valid_accuracy: \", valid_accuracy)\n",
        "print(\"min index: \",np.argmin(valid_accuracy))\n",
        "min = np.argmin(valid_accuracy)\n",
        "\n",
        "model = d[min]\n",
        "data_p = \"/content/drive/My Drive/Dataset/record.pt\"\n",
        "torch.save(model, data_p)\n",
        "evaluate(model, test_dataset, device)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}